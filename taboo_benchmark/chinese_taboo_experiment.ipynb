{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# ä¸­æ–‡Tabooå®éªŒ - åŸºäºOpenHowNetçš„æ•°æ®é›†æ„å»ºä¸æµ‹è¯•\n",
    "# ä»¿ç…§base_test.ipynbç»“æ„ï¼Œä¸“é—¨é’ˆå¯¹ä¸­æ–‡è¯æ±‡å’Œè¯­è¨€æ¨¡å‹\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: OpenHowNet in /opt/homebrew/lib/python3.11/site-packages (2.0)\n",
      "Requirement already satisfied: jieba in /opt/homebrew/lib/python3.11/site-packages (0.42.1)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (2.3.0)\n",
      "Requirement already satisfied: anytree in /opt/homebrew/lib/python3.11/site-packages (from OpenHowNet) (2.13.0)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/lib/python3.11/site-packages (from OpenHowNet) (78.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (from OpenHowNet) (4.66.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install OpenHowNet jieba requests pandas numpy\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "!pip3 install OpenHowNet\n",
    "!pip3 install jieba\n",
    "!pip3 install requests\n",
    "!pip3 install pandas\n",
    "!pip3 install numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OpenHowNetå·²å¯¼å…¥\n",
      "ğŸš€ ä¸­æ–‡Tabooå®éªŒç¯å¢ƒåˆå§‹åŒ–å®Œæˆ\n",
      "ğŸ“‹ å®éªŒç›®æ ‡: ä½¿ç”¨OpenHowNetæ„å»º100ä¸ªä¸­æ–‡è¯æ±‡çš„Tabooæ•°æ®é›†\n",
      "ğŸ¯ è¯æ€§åˆ†å¸ƒ: åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ã€å‰¯è¯å„25ä¸ª\n"
     ]
    }
   ],
   "source": [
    "# 1. å¯¼å…¥ä¾èµ–å’Œè®¾ç½®ç¯å¢ƒ\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "import jieba\n",
    "import re\n",
    "from typing import Dict, List, Any, Tuple\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# å®‰è£…å’Œå¯¼å…¥OpenHowNet\n",
    "try:\n",
    "    import OpenHowNet\n",
    "    print(\"âœ… OpenHowNetå·²å¯¼å…¥\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ æ­£åœ¨å®‰è£…OpenHowNet...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"OpenHowNet\"])\n",
    "    import OpenHowNet\n",
    "    print(\"âœ… OpenHowNetå®‰è£…å¹¶å¯¼å…¥æˆåŠŸ\")\n",
    "\n",
    "print(\"ğŸš€ ä¸­æ–‡Tabooå®éªŒç¯å¢ƒåˆå§‹åŒ–å®Œæˆ\")\n",
    "print(\"ğŸ“‹ å®éªŒç›®æ ‡: ä½¿ç”¨OpenHowNetæ„å»º100ä¸ªä¸­æ–‡è¯æ±‡çš„Tabooæ•°æ®é›†\")\n",
    "print(\"ğŸ¯ è¯æ€§åˆ†å¸ƒ: åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ã€å‰¯è¯å„25ä¸ª\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” æ·±å…¥æµ‹è¯•OpenHowNet APIç»“æ„...\n",
      "\n",
      "ğŸ“ æµ‹è¯•è¯æ±‡: è®¡ç®—æœº\n",
      "   ä¹‰é¡¹æ•°é‡: 1\n",
      "   ç¬¬ä¸€ä¸ªä¹‰é¡¹ç±»å‹: <class 'OpenHowNet.Sense.Sense'>\n",
      "   æ‰€æœ‰å±æ€§: ['No', 'en_word', 'en_grammar', 'zh_word', 'zh_grammar', 'Def', 'sememes']\n",
      "   ä¸­æ–‡è¯: è®¡ç®—æœº\n",
      "   ä¸­æ–‡è¯æ€§(zh_grammar): noun\n",
      "\n",
      "ğŸ“ æµ‹è¯•è¯æ±‡: å­¦ä¹ \n",
      "   ä¹‰é¡¹æ•°é‡: 5\n",
      "   ç¬¬ä¸€ä¸ªä¹‰é¡¹ç±»å‹: <class 'OpenHowNet.Sense.Sense'>\n",
      "   æ‰€æœ‰å±æ€§: ['No', 'en_word', 'en_grammar', 'zh_word', 'zh_grammar', 'Def', 'sememes']\n",
      "   ä¸­æ–‡è¯: å­¦ä¹ \n",
      "   ä¸­æ–‡è¯æ€§(zh_grammar): verb\n",
      "\n",
      "ğŸ“ æµ‹è¯•è¯æ±‡: ç¾ä¸½\n",
      "   ä¹‰é¡¹æ•°é‡: 3\n",
      "   ç¬¬ä¸€ä¸ªä¹‰é¡¹ç±»å‹: <class 'OpenHowNet.Sense.Sense'>\n",
      "   æ‰€æœ‰å±æ€§: ['No', 'en_word', 'en_grammar', 'zh_word', 'zh_grammar', 'Def', 'sememes']\n",
      "   ä¸­æ–‡è¯: ç¾ä¸½\n",
      "   ä¸­æ–‡è¯æ€§(zh_grammar): adj\n",
      "\n",
      "ğŸ“ æµ‹è¯•è¯æ±‡: å¿«é€Ÿ\n",
      "   ä¹‰é¡¹æ•°é‡: 4\n",
      "   ç¬¬ä¸€ä¸ªä¹‰é¡¹ç±»å‹: <class 'OpenHowNet.Sense.Sense'>\n",
      "   æ‰€æœ‰å±æ€§: ['No', 'en_word', 'en_grammar', 'zh_word', 'zh_grammar', 'Def', 'sememes']\n",
      "   ä¸­æ–‡è¯: å¿«é€Ÿ\n",
      "   ä¸­æ–‡è¯æ€§(zh_grammar): adj\n",
      "\n",
      "ğŸ“Š è¯æ±‡è·å–æ–¹æ³•æµ‹è¯•:\n",
      "   get_zh_words() è¿”å›ç±»å‹: <class 'list'>\n",
      "   è¯æ±‡æ•°é‡: 135009\n",
      "   å‰10ä¸ªè¯æ±‡: ['', 'æ·±åœ³ä¹å®¶ç²¾å“æœåŠ¡å…¬å¯“', 'ä¸´åºŠè¡¨ç°ä¸º', 'ä¼‘æ¯', 'æ‰“å°ç®—ç›˜', 'æ¹–åº•', 'èµæ ¼', 'è¶…çŸ­è£™', 'è¥‘', 'å²']\n"
     ]
    }
   ],
   "source": [
    "# æ·±å…¥æµ‹è¯•OpenHowNet API\n",
    "if hownet_dict:\n",
    "    print(\"ğŸ” æ·±å…¥æµ‹è¯•OpenHowNet APIç»“æ„...\")\n",
    "    \n",
    "    # æµ‹è¯•è¯æ±‡\n",
    "    test_words = [\"è®¡ç®—æœº\", \"å­¦ä¹ \", \"ç¾ä¸½\", \"å¿«é€Ÿ\"]\n",
    "    \n",
    "    for test_word in test_words:\n",
    "        print(f\"\\nğŸ“ æµ‹è¯•è¯æ±‡: {test_word}\")\n",
    "        try:\n",
    "            senses = hownet_dict.get_sense(test_word)\n",
    "            if senses:\n",
    "                print(f\"   ä¹‰é¡¹æ•°é‡: {len(senses)}\")\n",
    "                sense = senses[0]\n",
    "                print(f\"   ç¬¬ä¸€ä¸ªä¹‰é¡¹ç±»å‹: {type(sense)}\")\n",
    "                \n",
    "                # æ£€æŸ¥æ‰€æœ‰å¯èƒ½çš„å±æ€§\n",
    "                if hasattr(sense, '__dict__'):\n",
    "                    attrs = list(sense.__dict__.keys())\n",
    "                    print(f\"   æ‰€æœ‰å±æ€§: {attrs}\")\n",
    "                else:\n",
    "                    # å°è¯•å¸¸è§å±æ€§\n",
    "                    common_attrs = ['zh_word', 'en_word', 'pos', 'zh_grammar', 'definition', 'Def', 'def']\n",
    "                    available_attrs = []\n",
    "                    for attr in common_attrs:\n",
    "                        if hasattr(sense, attr):\n",
    "                            value = getattr(sense, attr)\n",
    "                            available_attrs.append(f\"{attr}={value}\")\n",
    "                    print(f\"   å¯ç”¨å±æ€§: {available_attrs}\")\n",
    "                \n",
    "                # å°è¯•è°ƒç”¨ä¸€äº›æ–¹æ³•\n",
    "                try:\n",
    "                    if hasattr(sense, 'zh_word'):\n",
    "                        print(f\"   ä¸­æ–‡è¯: {sense.zh_word}\")\n",
    "                    if hasattr(sense, 'pos'):\n",
    "                        print(f\"   è¯æ€§(pos): {sense.pos}\")\n",
    "                    if hasattr(sense, 'zh_grammar'):\n",
    "                        print(f\"   ä¸­æ–‡è¯æ€§(zh_grammar): {sense.zh_grammar}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   å±æ€§è®¿é—®é”™è¯¯: {e}\")\n",
    "            else:\n",
    "                print(f\"   æœªæ‰¾åˆ°ä¹‰é¡¹\")\n",
    "        except Exception as e:\n",
    "            print(f\"   æµ‹è¯•é”™è¯¯: {e}\")\n",
    "    \n",
    "    # æµ‹è¯•è¯æ±‡åˆ—è¡¨æ–¹æ³•\n",
    "    print(f\"\\nğŸ“Š è¯æ±‡è·å–æ–¹æ³•æµ‹è¯•:\")\n",
    "    try:\n",
    "        zh_words = hownet_dict.get_zh_words()\n",
    "        print(f\"   get_zh_words() è¿”å›ç±»å‹: {type(zh_words)}\")\n",
    "        print(f\"   è¯æ±‡æ•°é‡: {len(zh_words)}\")\n",
    "        print(f\"   å‰10ä¸ªè¯æ±‡: {list(zh_words)[:10]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   get_zh_words() é”™è¯¯: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ hownet_dict ä¸º Noneï¼Œè·³è¿‡æµ‹è¯•\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ æ­£åœ¨åˆå§‹åŒ–OpenHowNetå’Œä¸­æ–‡å¤„ç†å·¥å…·...\n",
      "ğŸ“¥ æ­£åœ¨ä¸‹è½½OpenHowNetæ•°æ®...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resources/resources.zip: 72948KB [00:06, 10870.59KB/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OpenHowNetæ•°æ®ä¸‹è½½å®Œæˆ\n",
      "Initializing OpenHowNet succeeded!\n",
      "âœ… OpenHowNetè¯å…¸åŠ è½½æˆåŠŸ\n",
      "ğŸ“š è¯å…¸åŒ…å«ä¸­æ–‡è¯æ±‡æ•°é‡: 135009 ä¸ª\n",
      "âœ… APIæµ‹è¯•æˆåŠŸï¼Œ'è®¡ç®—æœº'æœ‰ 1 ä¸ªä¹‰é¡¹\n",
      "ğŸ“‹ ç¬¬ä¸€ä¸ªä¹‰é¡¹æ•°æ®ç»“æ„:\n",
      "   ç±»å‹: <class 'OpenHowNet.Sense.Sense'>\n",
      "   å†…å®¹: No.255809|computer|è®¡ç®—æœº\n",
      "   å±æ€§: ['No', 'en_word', 'en_grammar', 'zh_word', 'zh_grammar', 'Def', 'sememes']\n",
      "âœ… jiebaåˆ†è¯å·¥å…·å·²é…ç½®\n",
      "ğŸ² éšæœºç§å­å·²è®¾ç½®ä¸º42ï¼Œç¡®ä¿å®éªŒå¯å¤ç°\n"
     ]
    }
   ],
   "source": [
    "# 2. åˆå§‹åŒ–OpenHowNetå’Œä¸­æ–‡å¤„ç†å·¥å…·\n",
    "print(\"ğŸ”§ æ­£åœ¨åˆå§‹åŒ–OpenHowNetå’Œä¸­æ–‡å¤„ç†å·¥å…·...\")\n",
    "\n",
    "# åˆå§‹åŒ–OpenHowNetå®ä¾‹\n",
    "try:\n",
    "    # é¦–å…ˆå°è¯•ä¸‹è½½æ•°æ®\n",
    "    print(\"ğŸ“¥ æ­£åœ¨ä¸‹è½½OpenHowNetæ•°æ®...\")\n",
    "    OpenHowNet.download()\n",
    "    print(\"âœ… OpenHowNetæ•°æ®ä¸‹è½½å®Œæˆ\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ ä¸‹è½½è¿‡ç¨‹ä¸­å‡ºç°è­¦å‘Š: {e}\")\n",
    "\n",
    "try:\n",
    "    hownet_dict = OpenHowNet.HowNetDict()\n",
    "    print(\"âœ… OpenHowNetè¯å…¸åŠ è½½æˆåŠŸ\")\n",
    "    \n",
    "    # æµ‹è¯•APIæ–¹æ³•\n",
    "    zh_words = hownet_dict.get_zh_words()\n",
    "    print(f\"ğŸ“š è¯å…¸åŒ…å«ä¸­æ–‡è¯æ±‡æ•°é‡: {len(zh_words)} ä¸ª\")\n",
    "    \n",
    "    # æµ‹è¯•get_senseæ–¹æ³•\n",
    "    test_sense = hownet_dict.get_sense(\"è®¡ç®—æœº\")\n",
    "    if test_sense:\n",
    "        print(f\"âœ… APIæµ‹è¯•æˆåŠŸï¼Œ'è®¡ç®—æœº'æœ‰ {len(test_sense)} ä¸ªä¹‰é¡¹\")\n",
    "        print(f\"ğŸ“‹ ç¬¬ä¸€ä¸ªä¹‰é¡¹æ•°æ®ç»“æ„:\")\n",
    "        print(f\"   ç±»å‹: {type(test_sense[0])}\")\n",
    "        print(f\"   å†…å®¹: {test_sense[0]}\")\n",
    "        if hasattr(test_sense[0], '__dict__'):\n",
    "            print(f\"   å±æ€§: {list(test_sense[0].__dict__.keys())}\")\n",
    "    else:\n",
    "        print(\"âš ï¸ æµ‹è¯•è¯æ±‡'è®¡ç®—æœº'æœªæ‰¾åˆ°ä¹‰é¡¹\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ OpenHowNetåˆå§‹åŒ–å¤±è´¥: {e}\")\n",
    "    print(\"ğŸ”„ å°è¯•é‡æ–°åˆå§‹åŒ–...\")\n",
    "    hownet_dict = None\n",
    "\n",
    "# è®¾ç½®jiebaåˆ†è¯\n",
    "jieba.setLogLevel(20)  # å‡å°‘jiebaçš„æ—¥å¿—è¾“å‡º\n",
    "print(\"âœ… jiebaåˆ†è¯å·¥å…·å·²é…ç½®\")\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "random.seed(42)\n",
    "print(\"ğŸ² éšæœºç§å­å·²è®¾ç½®ä¸º42ï¼Œç¡®ä¿å®éªŒå¯å¤ç°\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” æ£€æŸ¥ HowNet è¯å…¸çŠ¶æ€...\n",
      "âœ… hownet_dict å·²å­˜åœ¨ä¸”æœ‰æ•ˆ\n",
      "ğŸ¯ æœ€ç»ˆçŠ¶æ€: hownet_dict = <class 'OpenHowNet.HowNetDict.HowNetDict'>\n"
     ]
    }
   ],
   "source": [
    "# æ£€æŸ¥å¹¶é‡æ–°åˆå§‹åŒ– HowNetï¼ˆå¦‚æœéœ€è¦ï¼‰\n",
    "print(\"ğŸ” æ£€æŸ¥ HowNet è¯å…¸çŠ¶æ€...\")\n",
    "\n",
    "try:\n",
    "    # æ£€æŸ¥ hownet_dict æ˜¯å¦å·²å®šä¹‰ä¸”æœ‰æ•ˆ\n",
    "    if 'hownet_dict' not in globals():\n",
    "        print(\"âš ï¸ hownet_dict æœªå®šä¹‰ï¼Œæ­£åœ¨åˆå§‹åŒ–...\")\n",
    "        hownet_dict = None\n",
    "    elif hownet_dict is None:\n",
    "        print(\"âš ï¸ hownet_dict ä¸º Noneï¼Œæ­£åœ¨é‡æ–°åˆå§‹åŒ–...\")\n",
    "    else:\n",
    "        print(\"âœ… hownet_dict å·²å­˜åœ¨ä¸”æœ‰æ•ˆ\")\n",
    "        \n",
    "    # å¦‚æœéœ€è¦ï¼Œé‡æ–°åˆå§‹åŒ–\n",
    "    if hownet_dict is None:\n",
    "        try:\n",
    "            import OpenHowNet\n",
    "            print(\"ğŸ“¥ æ­£åœ¨åˆå§‹åŒ– OpenHowNet...\")\n",
    "            hownet_dict = OpenHowNet.HowNetDict()\n",
    "            print(\"âœ… OpenHowNet è¯å…¸é‡æ–°åˆå§‹åŒ–æˆåŠŸ\")\n",
    "            \n",
    "            # æµ‹è¯•åŠŸèƒ½\n",
    "            test_words = hownet_dict.get_zh_words()\n",
    "            print(f\"ğŸ“š è¯å…¸åŒ…å« {len(test_words)} ä¸ªä¸­æ–‡è¯æ±‡\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ OpenHowNet åˆå§‹åŒ–å¤±è´¥: {e}\")\n",
    "            hownet_dict = None\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"âŒ æ£€æŸ¥è¿‡ç¨‹å‡ºé”™: {e}\")\n",
    "    hownet_dict = None\n",
    "\n",
    "print(f\"ğŸ¯ æœ€ç»ˆçŠ¶æ€: hownet_dict = {type(hownet_dict) if hownet_dict else 'None'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ä¸­æ–‡è¯æ±‡å¤„ç†å·¥å…·å‡½æ•°å·²å®šä¹‰\n"
     ]
    }
   ],
   "source": [
    "# 3. ä¸­æ–‡è¯æ±‡æ•°æ®é›†æ„å»ºå·¥å…·å‡½æ•°\n",
    "\n",
    "def get_pos_mapping():\n",
    "    \"\"\"HowNetè¯æ€§åˆ°æ ‡å‡†è¯æ€§çš„æ˜ å°„\"\"\"\n",
    "    return {\n",
    "        # åè¯ç±»\n",
    "        'N': 'noun', 'noun': 'noun',\n",
    "        # åŠ¨è¯ç±»  \n",
    "        'V': 'verb', 'verb': 'verb',\n",
    "        # å½¢å®¹è¯ç±»\n",
    "        'A': 'adj', 'adj': 'adj', 'a': 'adj',\n",
    "        # å‰¯è¯ç±»\n",
    "        'D': 'adv', 'adv': 'adv', 'd': 'adv'\n",
    "    }\n",
    "\n",
    "def is_valid_chinese_word(word: str) -> bool:\n",
    "    \"\"\"æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ä¸­æ–‡è¯æ±‡\"\"\"\n",
    "    if not word or len(word) < 1:\n",
    "        return False\n",
    "    \n",
    "    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦\n",
    "    chinese_pattern = re.compile(r'[\\u4e00-\\u9fff]+')\n",
    "    if not chinese_pattern.search(word):\n",
    "        return False\n",
    "    \n",
    "    # è¿‡æ»¤è¿‡é•¿æˆ–è¿‡çŸ­çš„è¯\n",
    "    if len(word) > 6 or len(word) < 1:\n",
    "        return False\n",
    "    \n",
    "    # è¿‡æ»¤åŒ…å«ç‰¹æ®Šå­—ç¬¦çš„è¯\n",
    "    special_chars = ['Â·', 'â€”', 'â€¦', 'ã€ˆ', 'ã€‰', 'ã€Š', 'ã€‹', 'ã€Œ', 'ã€']\n",
    "    if any(char in word for char in special_chars):\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def extract_similar_words_from_hownet(target_word: str, target_pos: str, hownet_dict, max_count: int = 10) -> List[str]:\n",
    "    \"\"\"ä»HowNetä¸­æå–ä¸ç›®æ ‡è¯ç›¸ä¼¼çš„è¯æ±‡ä½œä¸ºç¦ç”¨è¯å€™é€‰\"\"\"\n",
    "    similar_words = set()\n",
    "    \n",
    "    try:\n",
    "        # è·å–ç›®æ ‡è¯çš„ä¹‰é¡¹\n",
    "        word_senses = hownet_dict.get_sense(target_word)\n",
    "        if not word_senses:\n",
    "            return []\n",
    "        \n",
    "        # ä»ç¬¬ä¸€ä¸ªä¹‰é¡¹å¼€å§‹æå–ç›¸ä¼¼è¯\n",
    "        primary_sense = word_senses[0]\n",
    "        \n",
    "        # æ–¹æ³•1: ä»ä¹‰é¡¹ä¸­æå–ç›¸å…³è¯æ±‡\n",
    "        try:\n",
    "            # è·å–æ‰€æœ‰åŒ…å«è¯¥è¯çš„ç›¸å…³ä¹‰é¡¹ä¿¡æ¯\n",
    "            for sense in word_senses[:3]:  # å–å‰3ä¸ªä¹‰é¡¹\n",
    "                # æå–ä¸­æ–‡è¯ - ä½¿ç”¨å±æ€§è€Œä¸æ˜¯å­—å…¸è®¿é—®\n",
    "                if hasattr(sense, 'zh_word') and sense.zh_word and is_valid_chinese_word(sense.zh_word) and sense.zh_word != target_word:\n",
    "                    similar_words.add(sense.zh_word)\n",
    "                    \n",
    "                # å°è¯•è·å–åŒä¹‰è¯ - ä¸åŒçš„å±æ€§åç§°\n",
    "                syn_attrs = ['syn', 'synonyms', 'similar_words']\n",
    "                for syn_attr in syn_attrs:\n",
    "                    if hasattr(sense, syn_attr):\n",
    "                        syn_data = getattr(sense, syn_attr)\n",
    "                        if syn_data:\n",
    "                            # å¤„ç†ä¸åŒçš„åŒä¹‰è¯æ•°æ®æ ¼å¼\n",
    "                            if isinstance(syn_data, list):\n",
    "                                for syn_item in syn_data[:5]:\n",
    "                                    if isinstance(syn_item, str):\n",
    "                                        syn_word = syn_item\n",
    "                                    elif hasattr(syn_item, 'text'):\n",
    "                                        syn_word = syn_item.text\n",
    "                                    elif hasattr(syn_item, 'word'):\n",
    "                                        syn_word = syn_item.word\n",
    "                                    else:\n",
    "                                        continue\n",
    "                                    \n",
    "                                    if is_valid_chinese_word(syn_word) and syn_word != target_word:\n",
    "                                        similar_words.add(syn_word)\n",
    "                        break\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ æå–åŒä¹‰è¯æ—¶å‡ºé”™: {e}\")\n",
    "            pass\n",
    "        \n",
    "        # æ–¹æ³•2: ä»ä¹‰åŸå®šä¹‰ä¸­æå–å…³é”®è¯\n",
    "        try:\n",
    "            for sense in word_senses[:2]:  # å–å‰2ä¸ªä¹‰é¡¹\n",
    "                # å°è¯•ä¸åŒçš„å®šä¹‰å±æ€§åç§°\n",
    "                definition = \"\"\n",
    "                def_attrs = ['Def', 'definition', 'def', 'meaning']\n",
    "                for def_attr in def_attrs:\n",
    "                    if hasattr(sense, def_attr):\n",
    "                        definition = getattr(sense, def_attr)\n",
    "                        if definition:\n",
    "                            break\n",
    "                \n",
    "                if definition:\n",
    "                    # ä½¿ç”¨jiebaåˆ†è¯æå–å®šä¹‰ä¸­çš„ä¸­æ–‡è¯æ±‡\n",
    "                    words_in_def = jieba.lcut(definition)\n",
    "                    for word in words_in_def:\n",
    "                        if is_valid_chinese_word(word) and word != target_word and len(word) >= 2:\n",
    "                            similar_words.add(word)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ æå–å®šä¹‰å…³é”®è¯æ—¶å‡ºé”™: {e}\")\n",
    "            pass\n",
    "        \n",
    "        # æ–¹æ³•3: ä»ç›¸å…³è¯æ±‡ä¸­æå–ï¼ˆåŸºäºè¯æ€§ï¼‰\n",
    "        try:\n",
    "            # è·å–ç›¸åŒè¯æ€§çš„ç›¸å…³è¯æ±‡\n",
    "            target_pos = \"\"\n",
    "            pos_attrs = ['zh_grammar', 'pos', 'part_of_speech']\n",
    "            for pos_attr in pos_attrs:\n",
    "                if hasattr(primary_sense, pos_attr):\n",
    "                    target_pos = getattr(primary_sense, pos_attr)\n",
    "                    if target_pos:\n",
    "                        break\n",
    "            \n",
    "            if target_pos:\n",
    "                # ä»å·²è·å¾—çš„ç›¸å…³è¯æ±‡ä¸­è¿›ä¸€æ­¥ç­›é€‰ï¼ˆé™åˆ¶æ•°é‡é¿å…è¿‡åº¦æœç´¢ï¼‰\n",
    "                similar_words_list = list(similar_words)[:10]  # å‡å°‘æœç´¢èŒƒå›´\n",
    "                for word in similar_words_list:\n",
    "                    try:\n",
    "                        related_senses = hownet_dict.get_sense(word)\n",
    "                        if related_senses:\n",
    "                            for related_sense in related_senses[:1]:  # åªæ£€æŸ¥ç¬¬ä¸€ä¸ªä¹‰é¡¹\n",
    "                                related_pos = \"\"\n",
    "                                for pos_attr in pos_attrs:\n",
    "                                    if hasattr(related_sense, pos_attr):\n",
    "                                        related_pos = getattr(related_sense, pos_attr)\n",
    "                                        if related_pos:\n",
    "                                            break\n",
    "                                \n",
    "                                if related_pos == target_pos:\n",
    "                                    # å°è¯•è·å–ç›¸å…³è¯æ±‡\n",
    "                                    if hasattr(related_sense, 'zh_word') and related_sense.zh_word:\n",
    "                                        related_word = related_sense.zh_word\n",
    "                                        if is_valid_chinese_word(related_word) and related_word != target_word:\n",
    "                                            similar_words.add(related_word)\n",
    "                    except:\n",
    "                        continue\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ åŸºäºè¯æ€§æå–ç›¸å…³è¯æ—¶å‡ºé”™: {e}\")\n",
    "            pass\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ æå– {target_word} çš„ç›¸ä¼¼è¯æ—¶å‡ºé”™: {e}\")\n",
    "    \n",
    "    # è¿‡æ»¤å¹¶è¿”å›ç»“æœ\n",
    "    result = [word for word in similar_words if is_valid_chinese_word(word)][:max_count]\n",
    "    return result\n",
    "\n",
    "print(\"âœ… ä¸­æ–‡è¯æ±‡å¤„ç†å·¥å…·å‡½æ•°å·²å®šä¹‰\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—ï¸ å¼€å§‹æ„å»ºä¸­æ–‡Tabooæ•°æ®é›†...\n",
      "ğŸ“Š ç›®æ ‡: æ¯ä¸ªè¯æ€§ 25 ä¸ªè¯ï¼Œæ€»è®¡ 100 ä¸ªè¯\n",
      "ğŸ” æ­£åœ¨è·å–HowNetä¸­æ–‡è¯æ±‡...\n",
      "ğŸ“š HowNetä¸­æ–‡è¯æ±‡æ€»æ•°: 130347 ä¸ª\n",
      "ğŸ” æ­£åœ¨åˆ†æè¯æ±‡è¯æ€§...\n",
      "   å·²å¤„ç† 500/5000 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 1000/5000 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 1500/5000 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 2000/5000 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 2500/5000 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 3000/5000 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 3500/5000 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 4000/5000 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 4500/5000 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 5000/5000 ä¸ªè¯æ±‡\n",
      "\n",
      "ğŸ“ˆ è¯æ€§åˆ†å¸ƒç»Ÿè®¡:\n",
      "   noun: 2528 ä¸ªå€™é€‰è¯\n",
      "   verb: 1257 ä¸ªå€™é€‰è¯\n",
      "   adj: 490 ä¸ªå€™é€‰è¯\n",
      "   adv: 77 ä¸ªå€™é€‰è¯\n",
      "\n",
      "ğŸ¯ å¼€å§‹é€‰æ‹©ç›®æ ‡è¯æ±‡å¹¶ç”Ÿæˆç¦ç”¨è¯...\n",
      "\n",
      "ğŸ”„ æ­£åœ¨å¤„ç† noun ç±»è¯æ±‡ (25 ä¸ª)...\n",
      "   å¤„ç† 1/25: äººå·¥å²›\n",
      "   å¤„ç† 2/25: é³\n",
      "   å¤„ç† 3/25: è¿‘å‡ ä¸ªæœˆæ¥\n",
      "   å¤„ç† 4/25: è™šæ•°\n",
      "   å¤„ç† 5/25: ä¾›éœ€çŸ›ç›¾\n",
      "   å¤„ç† 6/25: è‹¦ç«¹\n",
      "   å¤„ç† 7/25: é•¿å‘\n",
      "   å¤„ç† 8/25: å¤šç±³è¯ºéª¨ç‰Œ\n",
      "   å¤„ç† 9/25: å•ä½\n",
      "   å¤„ç† 10/25: åå¥æ›²\n",
      "   å¤„ç† 11/25: ç›æµ“åº¦\n",
      "   å¤„ç† 12/25: å†¤å‡é”™æ¡ˆ\n",
      "   å¤„ç† 13/25: é¥§\n",
      "   å¤„ç† 14/25: å†›åš\n",
      "   å¤„ç† 15/25: çƒæœ\n",
      "   å¤„ç† 16/25: æ—¥å…‰èŠ‚çº¦æ—¶é—´\n",
      "   å¤„ç† 17/25: æµ·ä¼¦å¨œ\n",
      "   å¤„ç† 18/25: åº·é©¬å¿\n",
      "   å¤„ç† 19/25: å…¥åœºåˆ¸\n",
      "   å¤„ç† 20/25: å’¬ç¿¼ç‰‡\n",
      "   å¤„ç† 21/25: åŒè½¨\n",
      "   å¤„ç† 22/25: å†°é•©\n",
      "   å¤„ç† 23/25: é€ä¿¡äºº\n",
      "   å¤„ç† 24/25: æµ·å•†æ³•\n",
      "   å¤„ç† 25/25: ç”Ÿå‘½è¿¹è±¡\n",
      "\n",
      "ğŸ”„ æ­£åœ¨å¤„ç† verb ç±»è¯æ±‡ (25 ä¸ª)...\n",
      "   å¤„ç† 1/25: è™šæ·\n",
      "   å¤„ç† 2/25: è§£é™¤\n",
      "   å¤„ç† 3/25: çŸ\n",
      "   å¤„ç† 4/25: æ‰“å…«æŠ˜\n",
      "   å¤„ç† 5/25: çŒ›æ¶¨\n",
      "   å¤„ç† 6/25: è€ç¾æˆæ€’\n",
      "   å¤„ç† 7/25: èº«å¿ƒäº¤ç˜\n",
      "   å¤„ç† 8/25: è¨€å½’æ­£ä¼ \n",
      "   å¤„ç† 9/25: è‹¦ä¹ä¸å…±\n",
      "   å¤„ç† 10/25: æ¸å¼±\n",
      "   å¤„ç† 11/25: å¼•ç”³\n",
      "   å¤„ç† 12/25: å°†ä½äº\n",
      "   å¤„ç† 13/25: å›¾ä¸ªå‰åˆ©\n",
      "   å¤„ç† 14/25: æº¶è§£\n",
      "   å¤„ç† 15/25: ä»£è´­\n",
      "   å¤„ç† 16/25: å¦‚åé’ˆæ¯¡\n",
      "   å¤„ç† 17/25: ç©ä¸è½¬\n",
      "   å¤„ç† 18/25: è‡´ç”µ\n",
      "   å¤„ç† 19/25: å´‡æ´‹è¿·å¤–\n",
      "   å¤„ç† 20/25: è¸¹å¼€\n",
      "   å¤„ç† 21/25: é“­è®°\n",
      "   å¤„ç† 22/25: å†³ä¸€èƒœè´Ÿ\n",
      "   å¤„ç† 23/25: å‹è´§\n",
      "   å¤„ç† 24/25: é½å”±\n",
      "   å¤„ç† 25/25: éšé€€\n",
      "\n",
      "ğŸ”„ æ­£åœ¨å¤„ç† adj ç±»è¯æ±‡ (25 ä¸ª)...\n",
      "   å¤„ç† 1/25: åº”å—è°´è´£\n",
      "   å¤„ç† 2/25: çŠ¹è±«ä¸å‰\n",
      "   å¤„ç† 3/25: å¯é„™\n",
      "   å¤„ç† 4/25: å¯æœ›ä¸å¯å³\n",
      "   å¤„ç† 5/25: ä½ç«¯\n",
      "   å¤„ç† 6/25: åºæ‚\n",
      "   å¤„ç† 7/25: é½å£°\n",
      "   å¤„ç† 8/25: ç¼›\n",
      "   å¤„ç† 9/25: ä¸è™”è¯š\n",
      "   å¤„ç† 10/25: æ˜Ÿé™…\n",
      "   å¤„ç† 11/25: ä¼—\n",
      "   å¤„ç† 12/25: ç›´è§‚\n",
      "   å¤„ç† 13/25: å¿¸å¿¸æ€©æ€©\n",
      "   å¤„ç† 14/25: å‡€\n",
      "   å¤„ç† 15/25: å°ä»¶\n",
      "   å¤„ç† 16/25: æ²¡æ„æ€\n",
      "   å¤„ç† 17/25: ä¸çŸ¥æ­»æ´»\n",
      "   å¤„ç† 18/25: ä¸å¯ä¼°é‡\n",
      "   å¤„ç† 19/25: è°ˆåæ–‡é›…\n",
      "   å¤„ç† 20/25: é»ç³Šç³Š\n",
      "   å¤„ç† 21/25: ç°ä¸æºœç§‹\n",
      "   å¤„ç† 22/25: ä½äº§\n",
      "   å¤„ç† 23/25: æµ“åš\n",
      "   å¤„ç† 24/25: æ›´é«˜\n",
      "   å¤„ç† 25/25: å¤–æŒ‚\n",
      "\n",
      "ğŸ”„ æ­£åœ¨å¤„ç† adv ç±»è¯æ±‡ (25 ä¸ª)...\n",
      "   å¤„ç† 1/25: æ°åˆ°å¥½å¤„\n",
      "   å¤„ç† 2/25: ç…§ä¾‹\n",
      "   å¤„ç† 3/25: çœ‹æ ·å­\n",
      "   å¤„ç† 4/25: ä¸å¯é¿å…åœ°\n",
      "   å¤„ç† 5/25: æ—¶æœ‰\n",
      "   å¤„ç† 6/25: å…¨éƒ½\n",
      "   å¤„ç† 7/25: å—£å\n",
      "   å¤„ç† 8/25: æ®‹æš´åœ°\n",
      "   å¤„ç† 9/25: å¶å°”\n",
      "   å¤„ç† 10/25: å¾€å‰\n",
      "   å¤„ç† 11/25: æ•´ä½“ä¸Š\n",
      "   å¤„ç† 12/25: åœ¨æ­¤ä¹‹å‰\n",
      "   å¤„ç† 13/25: æ€»å½’\n",
      "   å¤„ç† 14/25: ä»é‚£æ—¶èµ·\n",
      "   å¤„ç† 15/25: ç”±ä¸œè€Œè¥¿\n",
      "   å¤„ç† 16/25: ä¸€ç‚¹ä¸€æ»´åœ°\n",
      "   å¤„ç† 17/25: æ€ªä¸å¾—\n",
      "   å¤„ç† 18/25: ä¸äº‰åœ°\n",
      "   å¤„ç† 19/25: è‚ƒç„¶\n",
      "   å¤„ç† 20/25: ä»å§‹è‡³ç»ˆ\n",
      "   å¤„ç† 21/25: æœ‰é¡·\n",
      "   å¤„ç† 22/25: æ—©æ—©æ™šæ™š\n",
      "   å¤„ç† 23/25: è¦ä¸\n",
      "   å¤„ç† 24/25: æœ€å¤§é™åº¦åœ°\n",
      "   å¤„ç† 25/25: è¯šç„¶\n",
      "\n",
      "âœ… ä¸­æ–‡Tabooæ•°æ®é›†æ„å»ºå®Œæˆï¼\n",
      "ğŸ“Š æ€»è¯æ±‡æ•°: 100 ä¸ª\n"
     ]
    }
   ],
   "source": [
    "# ä¿®å¤çš„ä¸­æ–‡Tabooæ•°æ®é›†æ„å»ºï¼ˆåŸºäºOpenHowNetï¼‰\n",
    "print(\"ğŸ—ï¸ å¼€å§‹æ„å»ºä¸­æ–‡Tabooæ•°æ®é›†...\")\n",
    "\n",
    "def build_chinese_taboo_dataset_corrected(hownet_dict, target_count_per_pos: int = 25):\n",
    "    \"\"\"æ„å»ºä¸­æ–‡Tabooæ•°æ®é›† - ä¿®å¤ç‰ˆ\"\"\"\n",
    "    \n",
    "    if hownet_dict is None:\n",
    "        print(\"âŒ HowNetè¯å…¸æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ„å»ºæ•°æ®é›†\")\n",
    "        return []\n",
    "    \n",
    "    pos_mapping = get_pos_mapping()\n",
    "    target_pos_list = ['noun', 'verb', 'adj', 'adv']\n",
    "    dataset = []\n",
    "    \n",
    "    print(f\"ğŸ“Š ç›®æ ‡: æ¯ä¸ªè¯æ€§ {target_count_per_pos} ä¸ªè¯ï¼Œæ€»è®¡ {target_count_per_pos * 4} ä¸ªè¯\")\n",
    "    \n",
    "    # è·å–HowNetä¸­æ–‡è¯æ±‡è¡¨ - ä½¿ç”¨æ­£ç¡®çš„APIæ–¹æ³•\n",
    "    try:\n",
    "        print(\"ğŸ” æ­£åœ¨è·å–HowNetä¸­æ–‡è¯æ±‡...\")\n",
    "        zh_words = hownet_dict.get_zh_words()\n",
    "        chinese_vocab = [word for word in zh_words if is_valid_chinese_word(word)]\n",
    "        print(f\"ğŸ“š HowNetä¸­æ–‡è¯æ±‡æ€»æ•°: {len(chinese_vocab)} ä¸ª\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è·å–ä¸­æ–‡è¯æ±‡å¤±è´¥: {e}\")\n",
    "        print(\"ğŸ’¡ æç¤º: ç¡®ä¿OpenHowNetæ•°æ®å·²æ­£ç¡®ä¸‹è½½\")\n",
    "        return []\n",
    "    \n",
    "    # æŒ‰è¯æ€§åˆ†ç»„æ”¶é›†è¯æ±‡\n",
    "    words_by_pos = {pos: [] for pos in target_pos_list}\n",
    "    \n",
    "    print(\"ğŸ” æ­£åœ¨åˆ†æè¯æ±‡è¯æ€§...\")\n",
    "    progress_count = 0\n",
    "    \n",
    "    for word in chinese_vocab[:5000]:  # é™åˆ¶å¤„ç†å‰5000ä¸ªè¯æ±‡ä»¥èŠ‚çœæ—¶é—´\n",
    "        progress_count += 1\n",
    "        if progress_count % 500 == 0:\n",
    "            print(f\"   å·²å¤„ç† {progress_count}/{min(5000, len(chinese_vocab))} ä¸ªè¯æ±‡\")\n",
    "        \n",
    "        try:\n",
    "            # è·å–è¯æ±‡çš„ä¹‰é¡¹ä¿¡æ¯\n",
    "            senses = hownet_dict.get_sense(word)\n",
    "            if not senses:\n",
    "                continue\n",
    "            \n",
    "            # è·å–ä¸»è¦è¯æ€§\n",
    "            primary_sense = senses[0]\n",
    "            \n",
    "            # å°è¯•ä¸åŒçš„è¯æ€§è·å–æ–¹æ³•\n",
    "            pos_info = None\n",
    "            for attr in ['zh_grammar', 'pos', 'part_of_speech']:\n",
    "                if hasattr(primary_sense, attr):\n",
    "                    pos_info = getattr(primary_sense, attr)\n",
    "                    if pos_info:\n",
    "                        break\n",
    "            \n",
    "            if not pos_info:\n",
    "                continue\n",
    "                \n",
    "            # æ˜ å°„åˆ°æ ‡å‡†è¯æ€§\n",
    "            standard_pos = pos_mapping.get(pos_info, None)\n",
    "            if standard_pos and standard_pos in target_pos_list:\n",
    "                words_by_pos[standard_pos].append({\n",
    "                    'word': word,\n",
    "                    'senses': senses,\n",
    "                    'primary_pos': standard_pos\n",
    "                })\n",
    "        \n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ è¯æ€§åˆ†å¸ƒç»Ÿè®¡:\")\n",
    "    for pos, words in words_by_pos.items():\n",
    "        print(f\"   {pos}: {len(words)} ä¸ªå€™é€‰è¯\")\n",
    "    \n",
    "    # ä¸ºæ¯ä¸ªè¯æ€§éšæœºé€‰æ‹©æŒ‡å®šæ•°é‡çš„è¯æ±‡\n",
    "    print(f\"\\nğŸ¯ å¼€å§‹é€‰æ‹©ç›®æ ‡è¯æ±‡å¹¶ç”Ÿæˆç¦ç”¨è¯...\")\n",
    "    \n",
    "    for pos in target_pos_list:\n",
    "        available_words = words_by_pos[pos]\n",
    "        if len(available_words) == 0:\n",
    "            print(f\"âš ï¸ {pos} è¯æ€§æ— å¯ç”¨è¯æ±‡ï¼Œè·³è¿‡\")\n",
    "            continue\n",
    "            \n",
    "        selected_count = min(target_count_per_pos, len(available_words))\n",
    "        \n",
    "        # éšæœºé€‰æ‹©è¯æ±‡\n",
    "        selected_words = random.sample(available_words, selected_count)\n",
    "        print(f\"\\nğŸ”„ æ­£åœ¨å¤„ç† {pos} ç±»è¯æ±‡ ({selected_count} ä¸ª)...\")\n",
    "        \n",
    "        for i, word_info in enumerate(selected_words):\n",
    "            target_word = word_info['word']\n",
    "            senses = word_info['senses']\n",
    "            \n",
    "            print(f\"   å¤„ç† {i+1}/{selected_count}: {target_word}\")\n",
    "            \n",
    "            # ç”Ÿæˆç¦ç”¨è¯\n",
    "            taboo_words = extract_similar_words_from_hownet(\n",
    "                target_word, pos, hownet_dict, max_count=8\n",
    "            )\n",
    "            \n",
    "            # ç¡®ä¿è‡³å°‘æœ‰5ä¸ªç¦ç”¨è¯\n",
    "            if len(taboo_words) < 5:\n",
    "                # æ ¹æ®è¯æ€§æ·»åŠ é€šç”¨ç¦ç”¨è¯\n",
    "                generic_mapping = {\n",
    "                    'noun': ['ä¸œè¥¿', 'ç‰©å“', 'äº‹ç‰©', 'å¯¹è±¡', 'åè¯'],\n",
    "                    'verb': ['åŠ¨ä½œ', 'è¡Œä¸º', 'åš', 'è¿›è¡Œ', 'æ´»åŠ¨'],\n",
    "                    'adj': ['ç‰¹å¾', 'æ€§è´¨', 'çŠ¶æ€', 'å½¢å®¹', 'æè¿°'],\n",
    "                    'adv': ['æ–¹å¼', 'ç¨‹åº¦', 'å¦‚ä½•', 'çŠ¶å†µ', 'ä¿®é¥°']\n",
    "                }\n",
    "                \n",
    "                generic_taboos = generic_mapping.get(pos, ['ç›¸å…³', 'æ¦‚å¿µ', 'è¯æ±‡', 'å†…å®¹', 'æ„æ€'])\n",
    "                for generic in generic_taboos:\n",
    "                    if generic not in taboo_words and generic != target_word:\n",
    "                        taboo_words.append(generic)\n",
    "                        if len(taboo_words) >= 5:\n",
    "                            break\n",
    "            \n",
    "            # å°† Sense å¯¹è±¡è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸æ ¼å¼\n",
    "            serializable_senses = []\n",
    "            for sense in senses:\n",
    "                sense_dict = {\n",
    "                    'zh_word': getattr(sense, 'zh_word', ''),\n",
    "                    'en_word': getattr(sense, 'en_word', ''),\n",
    "                    'zh_grammar': getattr(sense, 'zh_grammar', ''),\n",
    "                    'en_grammar': getattr(sense, 'en_grammar', ''),\n",
    "                    'Def': getattr(sense, 'Def', ''),\n",
    "                    'No': getattr(sense, 'No', ''),\n",
    "                    'sememes': str(getattr(sense, 'sememes', []))  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²\n",
    "                }\n",
    "                serializable_senses.append(sense_dict)\n",
    "            \n",
    "            # æ„å»ºæ•°æ®é›†æ¡ç›®\n",
    "            entry = {\n",
    "                'target': target_word,\n",
    "                'part_of_speech': pos,\n",
    "                'taboo': taboo_words[:5],  # ç¡®ä¿æ­£å¥½5ä¸ªç¦ç”¨è¯\n",
    "                'category': 'chinese_hownet',\n",
    "                'senses': serializable_senses,  # ä½¿ç”¨å¯åºåˆ—åŒ–çš„ç‰ˆæœ¬\n",
    "                'metadata': {\n",
    "                    'sense_count': len(senses),\n",
    "                    'taboo_count': len(taboo_words[:5]),\n",
    "                    'source': 'openhownet_corrected'\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            dataset.append(entry)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# æ„å»ºæ•°æ®é›†\n",
    "if hownet_dict:\n",
    "    chinese_dataset = build_chinese_taboo_dataset_corrected(hownet_dict, target_count_per_pos=25)\n",
    "    print(f\"\\nâœ… ä¸­æ–‡Tabooæ•°æ®é›†æ„å»ºå®Œæˆï¼\")\n",
    "    print(f\"ğŸ“Š æ€»è¯æ±‡æ•°: {len(chinese_dataset)} ä¸ª\")\n",
    "else:\n",
    "    print(\"âŒ OpenHowNetæœªæ­£ç¡®åˆå§‹åŒ–ï¼Œæ— æ³•æ„å»ºæ•°æ®é›†\")\n",
    "    chinese_dataset = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—ï¸ å¼€å§‹æ„å»ºä¸­æ–‡Tabooæ•°æ®é›†...\n",
      "ğŸ“Š ç›®æ ‡: æ¯ä¸ªè¯æ€§ 25 ä¸ªè¯ï¼Œæ€»è®¡ 100 ä¸ªè¯\n",
      "ğŸ“š HowNetä¸­æ–‡è¯æ±‡æ€»æ•°: 130347 ä¸ª\n",
      "ğŸ” æ­£åœ¨åˆ†æè¯æ±‡è¯æ€§...\n",
      "   å·²å¤„ç† 1000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 2000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 3000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 4000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 5000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 6000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 7000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 8000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 9000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 10000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 11000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 12000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 13000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 14000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 15000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 16000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 17000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 18000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 19000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 20000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 21000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 22000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 23000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 24000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 25000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 26000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 27000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 28000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 29000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 30000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 31000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 32000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 33000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 34000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 35000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 36000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 37000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 38000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 39000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 40000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 41000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 42000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 43000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 44000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 45000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 46000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 47000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 48000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 49000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 50000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 51000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 52000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 53000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 54000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 55000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 56000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 57000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 58000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 59000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 60000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 61000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 62000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 63000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 64000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 65000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 66000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 67000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 68000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 69000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 70000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 71000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 72000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 73000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 74000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 75000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 76000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 77000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 78000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 79000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 80000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 81000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 82000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 83000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 84000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 85000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 86000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 87000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 88000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 89000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 90000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 91000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 92000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 93000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 94000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 95000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 96000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 97000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 98000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 99000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 100000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 101000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 102000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 103000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 104000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 105000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 106000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 107000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 108000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 109000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 110000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 111000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 112000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 113000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 114000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 115000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 116000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 117000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 118000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 119000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 120000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 121000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 122000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 123000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 124000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 125000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 126000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 127000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 128000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 129000/130347 ä¸ªè¯æ±‡\n",
      "   å·²å¤„ç† 130000/130347 ä¸ªè¯æ±‡\n",
      "\n",
      "ğŸ“ˆ è¯æ€§åˆ†å¸ƒç»Ÿè®¡:\n",
      "   noun: 66456 ä¸ªå€™é€‰è¯\n",
      "   verb: 31994 ä¸ªå€™é€‰è¯\n",
      "   adj: 12641 ä¸ªå€™é€‰è¯\n",
      "   adv: 2217 ä¸ªå€™é€‰è¯\n",
      "\n",
      "ğŸ¯ å¼€å§‹é€‰æ‹©ç›®æ ‡è¯æ±‡å¹¶ç”Ÿæˆç¦ç”¨è¯...\n",
      "\n",
      "ğŸ”„ æ­£åœ¨å¤„ç† noun ç±»è¯æ±‡ (25 ä¸ª)...\n",
      "   å¤„ç† 1/25: æ»¤æ¶²\n",
      "   å¤„ç† 2/25: å²æ—¶\n",
      "   å¤„ç† 3/25: è”ç»œå‘˜\n",
      "   å¤„ç† 4/25: èµŒåŸ\n",
      "   å¤„ç† 5/25: è¯„å§”ä¼š\n",
      "   å¤„ç† 6/25: çº¢çƒ§é²´é±¼\n",
      "   å¤„ç† 7/25: ä½›æ‰‹\n",
      "   å¤„ç† 8/25: å‡é€Ÿå‰‚\n",
      "   å¤„ç† 9/25: è¯„è®ºå‘˜æ–‡ç« \n",
      "   å¤„ç† 10/25: å¼€å±€\n",
      "   å¤„ç† 11/25: è„Šé”¯\n",
      "   å¤„ç† 12/25: å½•å–åˆ†æ•°çº¿\n",
      "   å¤„ç† 13/25: åƒé‡Œé¹…æ¯›\n",
      "   å¤„ç† 14/25: å“ç‰Œ\n",
      "   å¤„ç† 15/25: å…¨åœº\n",
      "   å¤„ç† 16/25: è¿‡å±±è½¦\n",
      "   å¤„ç† 17/25: ä»å…„\n",
      "   å¤„ç† 18/25: ç‰©è±¡\n",
      "   å¤„ç† 19/25: å¤§çƒ›å°\n",
      "   å¤„ç† 20/25: ç»¿è±†ç³•\n",
      "   å¤„ç† 21/25: æ’‘æ†\n",
      "   å¤„ç† 22/25: è¯å•\n",
      "   å¤„ç† 23/25: é¶å™¨å®˜ä¿æŠ¤\n",
      "   å¤„ç† 24/25: å¸ƒåº—\n",
      "   å¤„ç† 25/25: è·³ä¼å¡”\n",
      "\n",
      "ğŸ”„ æ­£åœ¨å¤„ç† verb ç±»è¯æ±‡ (25 ä¸ª)...\n",
      "   å¤„ç† 1/25: çŠ¯å¢ƒ\n",
      "   å¤„ç† 2/25: æ€’éª‚\n",
      "   å¤„ç† 3/25: è„šè¸©ä¸¤åªèˆ¹\n",
      "   å¤„ç† 4/25: æ‹æŠ¥\n",
      "   å¤„ç† 5/25: å± åŸ\n",
      "   å¤„ç† 6/25: èµ°æ°´\n",
      "   å¤„ç† 7/25: å“€å¹\n",
      "   å¤„ç† 8/25: å¥å‚\n",
      "   å¤„ç† 9/25: è½å®\n",
      "   å¤„ç† 10/25: è´Ÿæ°”\n",
      "   å¤„ç† 11/25: åŠ¿åœ¨å¿…è¡Œ\n",
      "   å¤„ç† 12/25: å‡å°‘åº“å­˜\n",
      "   å¤„ç† 13/25: æ”€æ–°é«˜\n",
      "   å¤„ç† 14/25: æœ›è§\n",
      "   å¤„ç† 15/25: æ€œè´«æƒœè€\n",
      "   å¤„ç† 16/25: ç–²åŠ³\n",
      "   å¤„ç† 17/25: è¿œè¶³\n",
      "   å¤„ç† 18/25: æŠ•æ³¨\n",
      "   å¤„ç† 19/25: å¯„å…»\n",
      "   å¤„ç† 20/25: é•—\n",
      "   å¤„ç† 21/25: è¿‡çœ¼çƒŸäº‘\n",
      "   å¤„ç† 22/25: ç»§ç»­æ‹…ä»»\n",
      "   å¤„ç† 23/25: å¼•äº§\n",
      "   å¤„ç† 24/25: ä¹é¡¿\n",
      "   å¤„ç† 25/25: æ­¢ç—’\n",
      "\n",
      "ğŸ”„ æ­£åœ¨å¤„ç† adj ç±»è¯æ±‡ (25 ä¸ª)...\n",
      "   å¤„ç† 1/25: æ›´åŠ æ¼‚äº®\n",
      "   å¤„ç† 2/25: è´¼æºœæºœ\n",
      "   å¤„ç† 3/25: æ— è¾¹\n",
      "   å¤„ç† 4/25: ç¿å“²\n",
      "   å¤„ç† 5/25: ç«ç‘°ç´«\n",
      "   å¤„ç† 6/25: æ±¡ç§½\n",
      "   å¤„ç† 7/25: å¤Ÿå‘³å„¿\n",
      "   å¤„ç† 8/25: ç¥ä¸çŸ¥é¬¼ä¸è§‰\n",
      "   å¤„ç† 9/25: å¤šé›¨\n",
      "   å¤„ç† 10/25: ä¿¡æ¯ä¸°å¯Œ\n",
      "   å¤„ç† 11/25: æ— ç‰¹è‰²\n",
      "   å¤„ç† 12/25: ä¸€æ— å¯å–\n",
      "   å¤„ç† 13/25: å‡Š\n",
      "   å¤„ç† 14/25: å£ç«‹åƒä»\n",
      "   å¤„ç† 15/25: é€Ÿæˆ\n",
      "   å¤„ç† 16/25: æ´ªéƒ½æ‹‰æ–¯\n",
      "   å¤„ç† 17/25: æ·¼\n",
      "   å¤„ç† 18/25: å¨´é™\n",
      "   å¤„ç† 19/25: è€å¹¼\n",
      "   å¤„ç† 20/25: è€æ°”æ¨ªç§‹\n",
      "   å¤„ç† 21/25: æ°´æ¸Œæ¸Œ\n",
      "   å¤„ç† 22/25: æœ‰èƒ½åŠ›\n",
      "   å¤„ç† 23/25: æµ…è‰²\n",
      "   å¤„ç† 24/25: ä¸æµè¡€\n",
      "   å¤„ç† 25/25: çƒ­é—¨\n",
      "\n",
      "ğŸ”„ æ­£åœ¨å¤„ç† adv ç±»è¯æ±‡ (25 ä¸ª)...\n",
      "   å¤„ç† 1/25: å¹´ä»£é”™è¯¯\n",
      "   å¤„ç† 2/25: è¿œè¿œ\n",
      "   å¤„ç† 3/25: æ€»ç®—\n",
      "   å¤„ç† 4/25: å¾ˆå¿«\n",
      "   å¤„ç† 5/25: ä¸å…¬æ­£åœ°\n",
      "   å¤„ç† 6/25: åœ¨æŸäº›æ–¹é¢\n",
      "   å¤„ç† 7/25: å¯é åœ°\n",
      "   å¤„ç† 8/25: ä½•å…¶\n",
      "   å¤„ç† 9/25: ææ€•\n",
      "   å¤„ç† 10/25: ä¸ºä¹‹\n",
      "   å¤„ç† 11/25: åéªŒåœ°\n",
      "   å¤„ç† 12/25: å›ºå®šåœ°\n",
      "   å¤„ç† 13/25: æŒ¨è‚©\n",
      "   å¤„ç† 14/25: é—ä¼ å­¦ä¸Š\n",
      "   å¤„ç† 15/25: äºæ­¤\n",
      "   å¤„ç† 16/25: åˆšåˆš\n",
      "   å¤„ç† 17/25: ä¸ºä»€ä¹ˆä¸\n",
      "   å¤„ç† 18/25: è°çŸ¥é“\n",
      "   å¤„ç† 19/25: å…¬ç„¶\n",
      "   å¤„ç† 20/25: æŒ¨è¾¹\n",
      "   å¤„ç† 21/25: å³åˆ»\n",
      "   å¤„ç† 22/25: èµ·åˆ\n",
      "   å¤„ç† 23/25: æ— å¥¢æœ›åœ°\n",
      "   å¤„ç† 24/25: è¿˜æ²¡æœ‰\n",
      "   å¤„ç† 25/25: å³æ˜¯è¯´\n",
      "\n",
      "âœ… ä¸­æ–‡Tabooæ•°æ®é›†æ„å»ºå®Œæˆï¼\n",
      "ğŸ“Š æ€»è¯æ±‡æ•°: 100 ä¸ª\n"
     ]
    }
   ],
   "source": [
    "# 4. æ„å»ºä¸­æ–‡Tabooæ•°æ®é›†\n",
    "print(\"ğŸ—ï¸ å¼€å§‹æ„å»ºä¸­æ–‡Tabooæ•°æ®é›†...\")\n",
    "\n",
    "def build_chinese_taboo_dataset(hownet_dict, target_count_per_pos: int = 25) -> List[Dict[str, Any]]:\n",
    "    \"\"\"æ„å»ºä¸­æ–‡Tabooæ•°æ®é›†\"\"\"\n",
    "    \n",
    "    if hownet_dict is None:\n",
    "        print(\"âŒ HowNetè¯å…¸æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ„å»ºæ•°æ®é›†\")\n",
    "        return []\n",
    "    \n",
    "    pos_mapping = get_pos_mapping()\n",
    "    target_pos_list = ['noun', 'verb', 'adj', 'adv']\n",
    "    dataset = []\n",
    "    \n",
    "    print(f\"ğŸ“Š ç›®æ ‡: æ¯ä¸ªè¯æ€§ {target_count_per_pos} ä¸ªè¯ï¼Œæ€»è®¡ {target_count_per_pos * 4} ä¸ªè¯\")\n",
    "    \n",
    "    # è·å–HowNetä¸­æ–‡è¯æ±‡è¡¨\n",
    "    chinese_vocab = hownet_dict.get_zh_words()\n",
    "    chinese_vocab = [word for word in chinese_vocab if is_valid_chinese_word(word)]\n",
    "    print(f\"ğŸ“š HowNetä¸­æ–‡è¯æ±‡æ€»æ•°: {len(chinese_vocab)} ä¸ª\")\n",
    "    \n",
    "    # æŒ‰è¯æ€§åˆ†ç»„æ”¶é›†è¯æ±‡\n",
    "    words_by_pos = {pos: [] for pos in target_pos_list}\n",
    "    \n",
    "    print(\"ğŸ” æ­£åœ¨åˆ†æè¯æ±‡è¯æ€§...\")\n",
    "    progress_count = 0\n",
    "    \n",
    "    for word in chinese_vocab:\n",
    "        progress_count += 1\n",
    "        if progress_count % 1000 == 0:\n",
    "            print(f\"   å·²å¤„ç† {progress_count}/{len(chinese_vocab)} ä¸ªè¯æ±‡\")\n",
    "        \n",
    "        try:\n",
    "            # è·å–è¯æ±‡çš„ä¹‰é¡¹ä¿¡æ¯\n",
    "            senses = hownet_dict.get_sense(word)\n",
    "            if not senses:\n",
    "                continue\n",
    "            \n",
    "            # è·å–ä¸»è¦è¯æ€§ - OpenHowNetçš„Senseå¯¹è±¡ä½¿ç”¨å±æ€§è€Œä¸æ˜¯å­—å…¸\n",
    "            primary_sense = senses[0]\n",
    "            \n",
    "            # å°è¯•ä¸åŒçš„è¯æ€§è·å–æ–¹æ³•\n",
    "            pos_info = None\n",
    "            if hasattr(primary_sense, 'zh_grammar'):\n",
    "                pos_info = primary_sense.zh_grammar\n",
    "            elif hasattr(primary_sense, 'pos'):\n",
    "                pos_info = primary_sense.pos\n",
    "            elif hasattr(primary_sense, 'part_of_speech'):\n",
    "                pos_info = primary_sense.part_of_speech\n",
    "            \n",
    "            if not pos_info:\n",
    "                continue\n",
    "                \n",
    "            # æ˜ å°„åˆ°æ ‡å‡†è¯æ€§\n",
    "            standard_pos = pos_mapping.get(pos_info, None)\n",
    "            if standard_pos and standard_pos in target_pos_list:\n",
    "                words_by_pos[standard_pos].append({\n",
    "                    'word': word,\n",
    "                    'senses': senses,\n",
    "                    'primary_pos': standard_pos\n",
    "                })\n",
    "        \n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    print(\"\\nğŸ“ˆ è¯æ€§åˆ†å¸ƒç»Ÿè®¡:\")\n",
    "    for pos, words in words_by_pos.items():\n",
    "        print(f\"   {pos}: {len(words)} ä¸ªå€™é€‰è¯\")\n",
    "    \n",
    "    # ä¸ºæ¯ä¸ªè¯æ€§éšæœºé€‰æ‹©æŒ‡å®šæ•°é‡çš„è¯æ±‡\n",
    "    print(\"\\nğŸ¯ å¼€å§‹é€‰æ‹©ç›®æ ‡è¯æ±‡å¹¶ç”Ÿæˆç¦ç”¨è¯...\")\n",
    "    \n",
    "    for pos in target_pos_list:\n",
    "        available_words = words_by_pos[pos]\n",
    "        if len(available_words) < target_count_per_pos:\n",
    "            print(f\"âš ï¸ {pos} è¯æ€§å¯ç”¨è¯æ±‡ä¸è¶³ ({len(available_words)} < {target_count_per_pos})\")\n",
    "            selected_count = len(available_words)\n",
    "        else:\n",
    "            selected_count = target_count_per_pos\n",
    "        \n",
    "        # éšæœºé€‰æ‹©è¯æ±‡\n",
    "        selected_words = random.sample(available_words, selected_count)\n",
    "        print(f\"\\nğŸ”„ æ­£åœ¨å¤„ç† {pos} ç±»è¯æ±‡ ({selected_count} ä¸ª)...\")\n",
    "        \n",
    "        for i, word_info in enumerate(selected_words):\n",
    "            target_word = word_info['word']\n",
    "            senses = word_info['senses']\n",
    "            \n",
    "            print(f\"   å¤„ç† {i+1}/{selected_count}: {target_word}\")\n",
    "            \n",
    "            # ç”Ÿæˆç¦ç”¨è¯\n",
    "            taboo_words = extract_similar_words_from_hownet(\n",
    "                target_word, pos, hownet_dict, max_count=8\n",
    "            )\n",
    "            \n",
    "            # å¦‚æœç¦ç”¨è¯ä¸å¤Ÿï¼Œæ·»åŠ ä¸€äº›é€šç”¨çš„ç›¸å…³è¯\n",
    "            if len(taboo_words) < 5:\n",
    "                # ä½¿ç”¨jiebaåˆ†è¯ä»å®šä¹‰ä¸­æå–æ›´å¤šè¯æ±‡\n",
    "                for sense in senses[:2]:  # åªå–å‰ä¸¤ä¸ªä¹‰é¡¹\n",
    "                    # æ­£ç¡®ä½¿ç”¨å±æ€§è®¿é—®è€Œä¸æ˜¯å­—å…¸è®¿é—®\n",
    "                    definition = getattr(sense, 'Def', '') if hasattr(sense, 'Def') else ''\n",
    "                    def_words = jieba.lcut(definition)\n",
    "                    for def_word in def_words:\n",
    "                        if (is_valid_chinese_word(def_word) and \n",
    "                            def_word != target_word and \n",
    "                            len(def_word) >= 2 and \n",
    "                            def_word not in taboo_words):\n",
    "                            taboo_words.append(def_word)\n",
    "                            if len(taboo_words) >= 5:\n",
    "                                break\n",
    "            \n",
    "            # ç¡®ä¿è‡³å°‘æœ‰5ä¸ªç¦ç”¨è¯\n",
    "            taboo_words = taboo_words[:5]  # é™åˆ¶ä¸º5ä¸ª\n",
    "            if len(taboo_words) < 5:\n",
    "                # å¦‚æœè¿˜æ˜¯ä¸å¤Ÿï¼Œæ·»åŠ ä¸€äº›é€šç”¨è¯æ±‡\n",
    "                generic_taboos = ['ä¸œè¥¿', 'äº‹ç‰©', 'ç‰©å“', 'æ¦‚å¿µ', 'å†…å®¹']\n",
    "                for generic in generic_taboos:\n",
    "                    if generic not in taboo_words and generic != target_word:\n",
    "                        taboo_words.append(generic)\n",
    "                        if len(taboo_words) >= 5:\n",
    "                            break\n",
    "            \n",
    "            # å°† Sense å¯¹è±¡è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸æ ¼å¼\n",
    "            serializable_senses = []\n",
    "            for sense in senses:\n",
    "                sense_dict = {\n",
    "                    'zh_word': getattr(sense, 'zh_word', ''),\n",
    "                    'en_word': getattr(sense, 'en_word', ''),\n",
    "                    'zh_grammar': getattr(sense, 'zh_grammar', ''),\n",
    "                    'en_grammar': getattr(sense, 'en_grammar', ''),\n",
    "                    'Def': getattr(sense, 'Def', ''),\n",
    "                    'No': getattr(sense, 'No', ''),\n",
    "                    'sememes': str(getattr(sense, 'sememes', []))  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²\n",
    "                }\n",
    "                serializable_senses.append(sense_dict)\n",
    "            \n",
    "            # æ„å»ºæ•°æ®é›†æ¡ç›®\n",
    "            entry = {\n",
    "                'target': target_word,\n",
    "                'part_of_speech': pos,\n",
    "                'taboo': taboo_words[:5],  # ç¡®ä¿æ­£å¥½5ä¸ªç¦ç”¨è¯\n",
    "                'category': 'chinese_general',\n",
    "                'senses': serializable_senses,  # ä½¿ç”¨å¯åºåˆ—åŒ–çš„ç‰ˆæœ¬\n",
    "                'metadata': {\n",
    "                    'sense_count': len(senses),\n",
    "                    'taboo_count': len(taboo_words[:5]),\n",
    "                    'source': 'openhownet'\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            dataset.append(entry)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# æ„å»ºæ•°æ®é›†\n",
    "chinese_dataset = build_chinese_taboo_dataset(hownet_dict, target_count_per_pos=25)\n",
    "print(f\"\\nâœ… ä¸­æ–‡Tabooæ•°æ®é›†æ„å»ºå®Œæˆï¼\")\n",
    "print(f\"ğŸ“Š æ€»è¯æ±‡æ•°: {len(chinese_dataset)} ä¸ª\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'taboo_benchmark/data/chinese_dataset.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# å¦‚æœchinese_datasetè¿˜æ²¡åŠ è½½ï¼Œå…ˆåŠ è½½\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtaboo_benchmark/data/chinese_dataset.json\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      6\u001b[39m     chinese_dataset = json.load(f)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# æŒ‰è¯æ€§åˆ†ç»„\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/IPython/core/interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'taboo_benchmark/data/chinese_dataset.json'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "# å¦‚æœchinese_datasetè¿˜æ²¡åŠ è½½ï¼Œå…ˆåŠ è½½\n",
    "with open('taboo_benchmark/data/chinese_dataset.json', 'r', encoding='utf-8') as f:\n",
    "    chinese_dataset = json.load(f)\n",
    "\n",
    "# æŒ‰è¯æ€§åˆ†ç»„\n",
    "pos_groups = {'noun': [], 'verb': [], 'adj': [], 'adv': []}\n",
    "for item in chinese_dataset:\n",
    "    pos = item.get('part_of_speech')\n",
    "    if pos in pos_groups:\n",
    "        pos_groups[pos].append(item)\n",
    "\n",
    "# æ¯ç±»éšæœºæŠ½å–10ä¸ª\n",
    "sampled_dataset = []\n",
    "for pos, items in pos_groups.items():\n",
    "    sampled = random.sample(items, min(10, len(items)))\n",
    "    sampled_dataset.extend(sampled)\n",
    "\n",
    "# æ£€æŸ¥ç»“æœ\n",
    "for pos in pos_groups:\n",
    "    count = len([x for x in sampled_dataset if x['part_of_speech'] == pos])\n",
    "    print(f\"{pos}: {count} ä¸ª\")\n",
    "\n",
    "# å¦‚éœ€ä¿å­˜\n",
    "with open('taboo_benchmark/data/chinese_dataset_sample10.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(sampled_dataset, f, ensure_ascii=False, indent=2)\n",
    "print(\"å·²ä¿å­˜åˆ° taboo_benchmark/data/chinese_dataset_sample10.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ä¸­æ–‡Tabooæ•°æ®é›†ç»Ÿè®¡åˆ†æ:\n",
      "==================================================\n",
      "ğŸ“ æ€»è¯æ±‡æ•°: 100\n",
      "\n",
      "ğŸ·ï¸ è¯æ€§åˆ†å¸ƒ:\n",
      "   noun: 25 ä¸ª (25.0%)\n",
      "   verb: 25 ä¸ª (25.0%)\n",
      "   adj: 25 ä¸ª (25.0%)\n",
      "   adv: 25 ä¸ª (25.0%)\n",
      "\n",
      "ğŸš« ç¦ç”¨è¯ç»Ÿè®¡:\n",
      "   å¹³å‡æ•°é‡: 5.0\n",
      "   èŒƒå›´: 5 - 5\n",
      "\n",
      "ğŸ’­ ä¹‰é¡¹ç»Ÿè®¡:\n",
      "   å¹³å‡æ•°é‡: 1.8\n",
      "   èŒƒå›´: 1 - 12\n",
      "\n",
      "ğŸ“‹ æ•°æ®æ ·æœ¬ (éšæœº5ä¸ª):\n",
      "\n",
      "   æ ·æœ¬ 1:\n",
      "     ç›®æ ‡è¯: å¨´é™\n",
      "     è¯æ€§: adj\n",
      "     ç¦ç”¨è¯: ['ä¸œè¥¿', 'äº‹ç‰©', 'ç‰©å“', 'æ¦‚å¿µ', 'å†…å®¹']\n",
      "     å®šä¹‰: {gracious|é›…}...\n",
      "\n",
      "   æ ·æœ¬ 2:\n",
      "     ç›®æ ‡è¯: æ€»ç®—\n",
      "     è¯æ€§: adv\n",
      "     ç¦ç”¨è¯: ['åŠŸèƒ½', 'æ—¶é—´', 'ç‰¹æ€§', 'ä¸œè¥¿', 'äº‹ç‰©']\n",
      "     å®šä¹‰: {FuncWord|åŠŸèƒ½è¯:comment={?}}...\n",
      "\n",
      "   æ ·æœ¬ 3:\n",
      "     ç›®æ ‡è¯: ç«ç‘°ç´«\n",
      "     è¯æ€§: adj\n",
      "     ç¦ç”¨è¯: ['ä¸œè¥¿', 'äº‹ç‰©', 'ç‰©å“', 'æ¦‚å¿µ', 'å†…å®¹']\n",
      "     å®šä¹‰: {red|çº¢}...\n",
      "\n",
      "   æ ·æœ¬ 4:\n",
      "     ç›®æ ‡è¯: è„šè¸©ä¸¤åªèˆ¹\n",
      "     è¯æ€§: verb\n",
      "     ç¦ç”¨è¯: ['å¾—ç½ª', 'å›é¿', 'ä¸œè¥¿', 'äº‹ç‰©', 'ç‰©å“']\n",
      "     å®šä¹‰: {evade|å›é¿:content={offend|å¾—ç½ª}}...\n",
      "\n",
      "   æ ·æœ¬ 5:\n",
      "     ç›®æ ‡è¯: è€æ°”æ¨ªç§‹\n",
      "     è¯æ€§: adj\n",
      "     ç¦ç”¨è¯: ['æ¶ˆæ', 'ä¸œè¥¿', 'äº‹ç‰©', 'ç‰©å“', 'æ¦‚å¿µ']\n",
      "     å®šä¹‰: {inactive|æ¶ˆæ}...\n",
      "\n",
      "âœ… ç»Ÿè®¡åˆ†æå®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "# 5. æ•°æ®é›†ç»Ÿè®¡åˆ†æ\n",
    "print(\"ğŸ“Š ä¸­æ–‡Tabooæ•°æ®é›†ç»Ÿè®¡åˆ†æ:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# åŸºæœ¬ç»Ÿè®¡\n",
    "total_words = len(chinese_dataset)\n",
    "print(f\"ğŸ“ æ€»è¯æ±‡æ•°: {total_words}\")\n",
    "\n",
    "# è¯æ€§åˆ†å¸ƒ\n",
    "pos_counts = {}\n",
    "taboo_counts = []\n",
    "sense_counts = []\n",
    "\n",
    "for item in chinese_dataset:\n",
    "    pos = item.get('part_of_speech', 'unknown')\n",
    "    pos_counts[pos] = pos_counts.get(pos, 0) + 1\n",
    "    taboo_counts.append(len(item.get('taboo', [])))\n",
    "    sense_counts.append(len(item.get('senses', [])))\n",
    "\n",
    "print(f\"\\nğŸ·ï¸ è¯æ€§åˆ†å¸ƒ:\")\n",
    "for pos, count in sorted(pos_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    percentage = count / total_words * 100\n",
    "    print(f\"   {pos}: {count} ä¸ª ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nğŸš« ç¦ç”¨è¯ç»Ÿè®¡:\")\n",
    "print(f\"   å¹³å‡æ•°é‡: {sum(taboo_counts) / len(taboo_counts):.1f}\")\n",
    "print(f\"   èŒƒå›´: {min(taboo_counts)} - {max(taboo_counts)}\")\n",
    "\n",
    "print(f\"\\nğŸ’­ ä¹‰é¡¹ç»Ÿè®¡:\")\n",
    "print(f\"   å¹³å‡æ•°é‡: {sum(sense_counts) / len(sense_counts):.1f}\")\n",
    "print(f\"   èŒƒå›´: {min(sense_counts)} - {max(sense_counts)}\")\n",
    "\n",
    "# æ˜¾ç¤ºæ•°æ®æ ·æœ¬\n",
    "print(f\"\\nğŸ“‹ æ•°æ®æ ·æœ¬ (éšæœº5ä¸ª):\")\n",
    "sample_items = random.sample(chinese_dataset, min(5, len(chinese_dataset)))\n",
    "for i, item in enumerate(sample_items, 1):\n",
    "    print(f\"\\n   æ ·æœ¬ {i}:\")\n",
    "    print(f\"     ç›®æ ‡è¯: {item['target']}\")\n",
    "    print(f\"     è¯æ€§: {item['part_of_speech']}\")\n",
    "    print(f\"     ç¦ç”¨è¯: {item['taboo']}\")\n",
    "    if item.get('senses') and len(item['senses']) > 0:\n",
    "        # senses ç°åœ¨æ˜¯å­—å…¸åˆ—è¡¨ï¼Œç›´æ¥è®¿é—®å­—å…¸é”®\n",
    "        sense = item['senses'][0]\n",
    "        definition = sense.get('Def', 'æ— å®šä¹‰')\n",
    "        if definition and definition != 'æ— å®šä¹‰':\n",
    "            print(f\"     å®šä¹‰: {definition[:50]}...\")\n",
    "        else:\n",
    "            print(f\"     å®šä¹‰: æ— å®šä¹‰\")\n",
    "\n",
    "print(f\"\\nâœ… ç»Ÿè®¡åˆ†æå®Œæˆ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (91919430.py, line 58)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mtry:\\n\",\u001b[39m\n         ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "# å®Œæ•´çš„ä¸­æ–‡Tabooå®éªŒ\n",
    "print(\"ğŸ§ª å®Œæ•´ä¸­æ–‡Tabooå®éªŒç³»ç»Ÿ...\")\n",
    "\n",
    "def run_full_chinese_experiment(client, models, dataset, experiment_name=\"chinese_taboo\"):\n",
    "    \"\"\"è¿è¡Œå®Œæ•´çš„ä¸­æ–‡Tabooå®éªŒ\"\"\"\n",
    "    \n",
    "    if not client:\n",
    "        print(\"âŒ APIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ‰§è¡Œå®éªŒ\")\n",
    "        return None\n",
    "    \n",
    "    # å®éªŒé…ç½®\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    experiment_id = f\"{experiment_name}_{timestamp}\"\n",
    "    \n",
    "    print(f\"\\nğŸ¯ å®Œæ•´å®éªŒé…ç½®:\")\n",
    "    print(f\"   å®éªŒID: {experiment_id}\")\n",
    "    print(f\"   è¯æ±‡æ•°é‡: {len(dataset)}\")\n",
    "    print(f\"   æ¨¡å‹æ•°é‡: {len(models)}\")\n",
    "    print(f\"   æ€»æ¸¸æˆæ•°: {len(dataset) * len(models) * len(models)}\")\n",
    "    \n",
    "    # åˆ›å»ºç»“æœç›®å½•\n",
    "    results_dir = f\"results/{experiment_id}\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # åˆ†æ‰¹å¤„ç†ä»¥é¿å…è¿‡é•¿æ—¶é—´è¿è¡Œ\n",
    "    batch_size = 10  # æ¯æ‰¹å¤„ç†10ä¸ªè¯æ±‡\n",
    "    all_results = []\n",
    "    \n",
    "    print(f\"\\nğŸš€ å¼€å§‹å®Œæ•´å®éªŒï¼ˆåˆ†æ‰¹å¤„ç†ï¼‰...\")\n",
    "    \n",
    "    # æŒ‰æ‰¹æ¬¡å¤„ç†æ•°æ®é›†\n",
    "    for batch_start in range(0, len(dataset), batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(dataset))\n",
    "        batch_dataset = dataset[batch_start:batch_end]\n",
    "        batch_num = (batch_start // batch_size) + 1\n",
    "        total_batches = (len(dataset) + batch_size - 1) // batch_size\n",
    "        \n",
    "        print(f\"\\nğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} (è¯æ±‡ {batch_start+1}-{batch_end})...\")\n",
    "        \n",
    "        batch_results = []\n",
    "        game_counter = batch_start * len(models) * len(models)\n",
    "        \n",
    "        for word_data in batch_dataset:\n",
    "            target_word = word_data['target']\n",
    "            taboo_words = word_data['taboo']\n",
    "            \n",
    "            print(f\"\\nğŸ¯ è¯æ±‡: {target_word} ({word_data['part_of_speech']})\\\")\")\n",
    "            print(f\"ğŸš« ç¦ç”¨è¯: {taboo_words}\")\n",
    "            \n",
    "            for hinter_model in models:\n",
    "                for guesser_model in models:\n",
    "                    game_counter += 1\n",
    "                    hinter_name = hinter_model.split('/')[-1]\n",
    "                    guesser_name = guesser_model.split('/')[-1]\n",
    "                    \n",
    "                    print(f\"  ğŸ”„ æ¸¸æˆ {game_counter}: {hinter_name} â†’ {guesser_name}\")\n",
    "                    \n",
    "                    try:\\n\",\n",
    "                        start_time = time.time()\n",
    "                        \n",
    "                        # æ‰§è¡Œæ¸¸æˆ\n",
    "                        game_result = play_chinese_taboo_game(\n",
    "                            client, hinter_model, guesser_model, \n",
    "                            target_word, taboo_words, max_turns=5\n",
    "                        )\n",
    "                        \n",
    "                        duration = round(time.time() - start_time, 2)\n",
    "                        \n",
    "                        # æ„å»ºè¯¦ç»†ç»“æœè®°å½•\n",
    "                        result = {\n",
    "                            'experiment_id': experiment_id,\n",
    "                            'batch_num': batch_num,\n",
    "                            'game_id': game_counter,\n",
    "                            'target_word': target_word,\n",
    "                            'part_of_speech': word_data['part_of_speech'],\n",
    "                            'category': word_data['category'],\n",
    "                            'taboo_words': '|'.join(taboo_words),\n",
    "                            'hinter_model': hinter_model,\n",
    "                            'guesser_model': guesser_model,\n",
    "                            'model_pair': f\\\"{hinter_name}_vs_{guesser_name}\\\",\n",
    "                            'success': game_result['success'],\\n\",\n",
    "                            'turns_used': game_result['turns'],\n",
    "                            'final_guess': game_result['final_guess'],\n",
    "                            'failure_reason': game_result.get('failure_reason', None),\n",
    "                            'taboo_violation_turn': game_result.get('taboo_violation_turn', None),\n",
    "                            'taboo_violation_hint': game_result.get('taboo_violation_hint', None),\n",
    "                            'has_taboo_violation': game_result.get('failure_reason') == 'TABOO_VIOLATION',\n",
    "                            'has_format_errors': len(game_result.get('format_errors', [])) > 0,\n",
    "                            'all_hints': ' | '.join(game_result.get('all_hints', [])),\n",
    "                            'all_guesses': ' | '.join(game_result.get('all_guesses', [])),\n",
    "                            'conversation': ' | '.join(game_result.get('conversation', [])),\n",
    "                            'total_api_attempts': game_result.get('total_hinter_attempts', 0) + game_result.get('total_guesser_attempts', 0),\n",
    "                            'hinter_attempts': game_result.get('total_hinter_attempts', 0),\n",
    "                            'guesser_attempts': game_result.get('total_guesser_attempts', 0),\n",
    "                            'format_errors': ' | '.join(game_result.get('format_errors', [])),\n",
    "                            'hinter_failed_outputs': ' | '.join(game_result.get('hinter_failed_outputs', [])),\n",
    "                            'guesser_failed_outputs': ' | '.join(game_result.get('guesser_failed_outputs', [])),\n",
    "                            'duration_seconds': duration,\n",
    "                            'timestamp': datetime.now().strftime(\\\"%Y-%m-%d %H:%M:%S\\\"),\n",
    "                            'language': 'chinese',\n",
    "                            'dataset_source': word_data.get('metadata', {}).get('source', 'unknown')\n",
    "                        }\n",
    "                        \n",
    "                        if 'error' in game_result:\n",
    "                            result['error'] = game_result['error']\n",
    "                        \n",
    "                        batch_results.append(result)\n",
    "                        \n",
    "                        # æ˜¾ç¤ºç»“æœ\n",
    "                        status = \\\"âœ… æˆåŠŸ\\\" if game_result['success'] else \\\"âŒ å¤±è´¥\\\"\n",
    "                        extra_info = \\\"\\\"\n",
    "                        if not game_result['success']:\n",
    "                            reason = game_result.get('failure_reason', 'unknown')\n",
    "                            if reason == 'TABOO_VIOLATION':\n",
    "                                extra_info = \\\" (è¿è§„)\\\"\n",
    "                            elif reason == 'FORMAT_FAILURE':\n",
    "                                extra_info = \\\" (æ ¼å¼)\\\"\n",
    "                            elif reason == 'MAX_TURNS_EXCEEDED':\n",
    "                                extra_info = \\\" (è½®æ•°)\\\"\n",
    "                        \n",
    "                        print(f\\\"     {status}{extra_info} | {game_result['turns']}è½® | {duration}s | {game_result['final_guess']}\\\")\\n\",\n",
    "                        \n",
    "                    except Exception as e:\\n\",\n",
    "                        print(f\\\"     âŒ æ‰§è¡Œå¼‚å¸¸: {str(e)[:50]}...\\\")\n",
    "                        # è®°å½•å¼‚å¸¸\n",
    "                        error_result = {\\n\",\n",
    "                            'experiment_id': experiment_id,\n",
    "                            'batch_num': batch_num,\n",
    "                            'game_id': game_counter,\n",
    "                            'target_word': target_word,\n",
    "                            'hinter_model': hinter_model,\\n\",\n",
    "                            'guesser_model': guesser_model,\\n\",\n",
    "                            'success': False,\\n\",\n",
    "                            'failure_reason': 'EXCEPTION',\\n\",\n",
    "                            'error': str(e),\\n\",\n",
    "                            'timestamp': datetime.now().strftime(\\\"%Y-%m-%d %H:%M:%S\\\"),\\n\",\n",
    "                            'language': 'chinese'\\n\",\n",
    "                        }\\n\",\n",
    "                        batch_results.append(error_result)\\n\",\n",
    "                    \\n\",\n",
    "                    time.sleep(0.3)  # APIè°ƒç”¨é—´éš”\\n\",\n",
    "        \\n\",\n",
    "        # ä¿å­˜æ‰¹æ¬¡ç»“æœ\\n\",\n",
    "        batch_df = pd.DataFrame(batch_results)\\n\",\n",
    "        batch_file = f\\\"{results_dir}/batch_{batch_num:03d}.csv\\\"\\n\",\n",
    "        batch_df.to_csv(batch_file, index=False, encoding='utf-8-sig')\\n\",\n",
    "        print(f\\\"ğŸ’¾ æ‰¹æ¬¡ {batch_num} ç»“æœå·²ä¿å­˜: {batch_file}\\\")\\n\",\n",
    "        \\n\",\n",
    "        all_results.extend(batch_results)\\n\",\n",
    "        \\n\",\n",
    "        # æ˜¾ç¤ºæ‰¹æ¬¡ç»Ÿè®¡\\n\",\n",
    "        batch_success = len([r for r in batch_results if r.get('success', False)])\\n\",\n",
    "        batch_total = len(batch_results)\\n\",\n",
    "        print(f\\\"ğŸ“Š æ‰¹æ¬¡ {batch_num} æˆåŠŸç‡: {batch_success}/{batch_total} ({batch_success/batch_total*100:.1f}%)\\\")\\n\",\n",
    "    \\n\",\n",
    "    # ä¿å­˜å®Œæ•´ç»“æœ\\n\",\n",
    "    complete_df = pd.DataFrame(all_results)\\n\",\n",
    "    complete_file = f\\\"{results_dir}/complete_experiment_results.csv\\\"\\n\",\n",
    "    complete_df.to_csv(complete_file, index=False, encoding='utf-8-sig')\\n\",\n",
    "    \\n\",\n",
    "    print(f\\\"\\\\nğŸ‰ å®Œæ•´å®éªŒå®Œæˆï¼\\\")\\n\",\n",
    "    print(f\\\"ğŸ“ ç»“æœç›®å½•: {results_dir}\\\")\\n\",\n",
    "    print(f\\\"ğŸ“Š æ€»æ¸¸æˆæ•°: {len(all_results)}\\\")\\n\",\n",
    "    \\n\",\n",
    "    # ç”Ÿæˆå®éªŒæŠ¥å‘Š\\n\",\n",
    "    generate_experiment_report(all_results, results_dir, experiment_id)\\n\",\n",
    "    \\n\",\n",
    "    return all_results\\n\",\n",
    "\n",
    "def generate_experiment_report(results, results_dir, experiment_id):\n",
    "    \\\"\\\"\\\"ç”Ÿæˆè¯¦ç»†çš„å®éªŒæŠ¥å‘Š\\\"\\\"\\\"\n",
    "    print(f\\\"\\\\nğŸ“‹ ç”Ÿæˆå®éªŒæŠ¥å‘Š...\\\")\\n\",\n",
    "    \\n\",\n",
    "    total_games = len(results)\\n\",\n",
    "    successful_games = [r for r in results if r.get('success', False)]\\n\",\n",
    "    success_rate = len(successful_games) / total_games * 100 if total_games > 0 else 0\\n\",\n",
    "    \\n\",\n",
    "    # æŒ‰æ¨¡å‹ç»Ÿè®¡\\n\",\n",
    "    models_used = list(set([r.get('hinter_model', 'unknown') for r in results if 'hinter_model' in r]))\\n\",\n",
    "    model_stats = {}\\n\",\n",
    "    \\n\",\n",
    "    for model in models_used:\\n\",\n",
    "        model_name = model.split('/')[-1] if '/' in model else model\\n\",\n",
    "        \\n\",\n",
    "        # ä½œä¸ºhinterçš„è¡¨ç°\\n\",\n",
    "        as_hinter = [r for r in results if r.get('hinter_model') == model]\\n\",\n",
    "        hinter_success = len([r for r in as_hinter if r.get('success', False)])\\n\",\n",
    "        \\n\",\n",
    "        # ä½œä¸ºguesserçš„è¡¨ç°\\n\",\n",
    "        as_guesser = [r for r in results if r.get('guesser_model') == model]\\n\",\n",
    "        guesser_success = len([r for r in as_guesser if r.get('success', False)])\\n\",\n",
    "        \\n\",\n",
    "        model_stats[model_name] = {\\n\",\n",
    "            'as_hinter': {'total': len(as_hinter), 'success': hinter_success, 'rate': hinter_success/len(as_hinter)*100 if as_hinter else 0},\\n\",\n",
    "            'as_guesser': {'total': len(as_guesser), 'success': guesser_success, 'rate': guesser_success/len(as_guesser)*100 if as_guesser else 0}\\n\",\n",
    "        }\\n\",\n",
    "    \\n\",\n",
    "    # å¤±è´¥åŸå› ç»Ÿè®¡\\n\",\n",
    "    failure_reasons = {}\\n\",\n",
    "    failed_games = [r for r in results if not r.get('success', True)]\\n\",\n",
    "    for game in failed_games:\\n\",\n",
    "        reason = game.get('failure_reason', 'unknown')\\n\",\n",
    "        failure_reasons[reason] = failure_reasons.get(reason, 0) + 1\\n\",\n",
    "    \\n\",\n",
    "    # è¯æ€§è¡¨ç°\\n\",\n",
    "    pos_stats = {}\\n\",\n",
    "    for result in results:\\n\",\n",
    "        pos = result.get('part_of_speech', 'unknown')\\n\",\n",
    "        if pos not in pos_stats:\\n\",\n",
    "            pos_stats[pos] = {'total': 0, 'success': 0}\\n\",\n",
    "        pos_stats[pos]['total'] += 1\\n\",\n",
    "        if result.get('success', False):\\n\",\n",
    "            pos_stats[pos]['success'] += 1\\n\",\n",
    "    \\n\",\n",
    "    # ç”ŸæˆæŠ¥å‘Š\\n\",\n",
    "    report = {\\n\",\n",
    "        'experiment_info': {\\n\",\n",
    "            'experiment_id': experiment_id,\\n\",\n",
    "            'total_games': total_games,\\n\",\n",
    "            'successful_games': len(successful_games),\\n\",\n",
    "            'success_rate': round(success_rate, 2),\\n\",\n",
    "            'completion_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\\n\",\n",
    "            'models_tested': len(models_used),\\n\",\n",
    "            'language': 'chinese'\\n\",\n",
    "        },\\n\",\n",
    "        'model_performance': model_stats,\\n\",\n",
    "        'failure_analysis': failure_reasons,\\n\",\n",
    "        'pos_performance': {pos: {'total': stats['total'], 'success': stats['success'], 'rate': round(stats['success']/stats['total']*100, 1)} for pos, stats in pos_stats.items()}\\n\",\n",
    "    }\\n\",\n",
    "    \\n\",\n",
    "    # ä¿å­˜æŠ¥å‘Š\\n\",\n",
    "    report_file = f\\\"{results_dir}/experiment_report.json\\\"\\n\",\n",
    "    with open(report_file, 'w', encoding='utf-8') as f:\\n\",\n",
    "        json.dump(report, f, ensure_ascii=False, indent=2)\\n\",\n",
    "    \\n\",\n",
    "    # æ˜¾ç¤ºæŠ¥å‘Šæ‘˜è¦\\n\",\n",
    "    print(f\\\"\\\\nğŸ“ˆ å®éªŒç»“æœæ‘˜è¦:\\\")\\n\",\n",
    "    print(f\\\"   æ€»æ¸¸æˆæ•°: {total_games}\\\")\\n\",\n",
    "    print(f\\\"   æˆåŠŸæ¸¸æˆ: {len(successful_games)}\\\")\\n\",\n",
    "    print(f\\\"   æ•´ä½“æˆåŠŸç‡: {success_rate:.1f}%\\\")\\n\",\n",
    "    \\n\",\n",
    "    print(f\\\"\\\\nğŸ¤– æ¨¡å‹è¡¨ç°:\\\")\\n\",\n",
    "    for model_name, stats in model_stats.items():\\n\",\n",
    "        print(f\\\"   {model_name}:\\\")\\n\",\n",
    "        print(f\\\"     ä½œä¸ºæç¤ºè€…: {stats['as_hinter']['success']}/{stats['as_hinter']['total']} ({stats['as_hinter']['rate']:.1f}%)\\\")\\n\",\n",
    "        print(f\\\"     ä½œä¸ºçŒœæµ‹è€…: {stats['as_guesser']['success']}/{stats['as_guesser']['total']} ({stats['as_guesser']['rate']:.1f}%)\\\")\\n\",\n",
    "    \\n\",\n",
    "    if failure_reasons:\\n\",\n",
    "        print(f\\\"\\\\nâŒ å¤±è´¥åŸå› åˆ†æ:\\\")\\n\",\n",
    "        for reason, count in failure_reasons.items():\\n\",\n",
    "            print(f\\\"   {reason}: {count} æ¬¡ ({count/len(failed_games)*100:.1f}%)\\\")\\n\",\n",
    "    \\n\",\n",
    "    print(f\\\"\\\\nğŸ“ æŠ¥å‘Šå·²ä¿å­˜: {report_file}\\\")\\n\",\n",
    "\n",
    "print(\\\"âœ… å®Œæ•´ä¸­æ–‡Tabooå®éªŒç³»ç»Ÿå·²å®šä¹‰\\\")\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ ä¿å­˜ä¸­æ–‡Tabooæ•°æ®é›†...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type Sense is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m chinese_dataset_path = os.path.join(data_dir, \u001b[33m\"\u001b[39m\u001b[33mchinese_dataset.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(chinese_dataset_path, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchinese_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ… å®Œæ•´æ•°æ®é›†å·²ä¿å­˜: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchinese_dataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# åˆ›å»ºç®€åŒ–ç‰ˆæ•°æ®é›†ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py:179\u001b[39m, in \u001b[36mdump\u001b[39m\u001b[34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[39m\n\u001b[32m    173\u001b[39m     iterable = \u001b[38;5;28mcls\u001b[39m(skipkeys=skipkeys, ensure_ascii=ensure_ascii,\n\u001b[32m    174\u001b[39m         check_circular=check_circular, allow_nan=allow_nan, indent=indent,\n\u001b[32m    175\u001b[39m         separators=separators,\n\u001b[32m    176\u001b[39m         default=default, sort_keys=sort_keys, **kw).iterencode(obj)\n\u001b[32m    177\u001b[39m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:430\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode\u001b[39m\u001b[34m(o, _current_indent_level)\u001b[39m\n\u001b[32m    428\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m _floatstr(o)\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    432\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:326\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_list\u001b[39m\u001b[34m(lst, _current_indent_level)\u001b[39m\n\u001b[32m    324\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    325\u001b[39m             chunks = _iterencode(value, _current_indent_level)\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    328\u001b[39m     _current_indent_level -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:406\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_dict\u001b[39m\u001b[34m(dct, _current_indent_level)\u001b[39m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    405\u001b[39m             chunks = _iterencode(value, _current_indent_level)\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    408\u001b[39m     _current_indent_level -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:326\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_list\u001b[39m\u001b[34m(lst, _current_indent_level)\u001b[39m\n\u001b[32m    324\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    325\u001b[39m             chunks = _iterencode(value, _current_indent_level)\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    328\u001b[39m     _current_indent_level -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:439\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode\u001b[39m\u001b[34m(o, _current_indent_level)\u001b[39m\n\u001b[32m    437\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCircular reference detected\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    438\u001b[39m     markers[markerid] = o\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m o = \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:180\u001b[39m, in \u001b[36mJSONEncoder.default\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[32m    162\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[33;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[33;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    178\u001b[39m \n\u001b[32m    179\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    181\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mis not JSON serializable\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Object of type Sense is not JSON serializable"
     ]
    }
   ],
   "source": [
    "# 6. ä¿å­˜ä¸­æ–‡æ•°æ®é›†\n",
    "print(\"ğŸ’¾ ä¿å­˜ä¸­æ–‡Tabooæ•°æ®é›†...\")\n",
    "\n",
    "# åˆ›å»ºæ•°æ®ç›®å½•\n",
    "data_dir = \"data\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "    print(f\"ğŸ“ åˆ›å»ºæ•°æ®ç›®å½•: {data_dir}\")\n",
    "\n",
    "# ä¿å­˜å®Œæ•´æ•°æ®é›†\n",
    "chinese_dataset_path = os.path.join(data_dir, \"chinese_dataset.json\")\n",
    "with open(chinese_dataset_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(chinese_dataset, f, ensure_ascii=False, indent=2)\n",
    "print(f\"âœ… å®Œæ•´æ•°æ®é›†å·²ä¿å­˜: {chinese_dataset_path}\")\n",
    "\n",
    "# åˆ›å»ºç®€åŒ–ç‰ˆæ•°æ®é›†ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰\n",
    "simplified_dataset = []\n",
    "for item in chinese_dataset:\n",
    "    simplified_item = {\n",
    "        'target': item['target'],\n",
    "        'part_of_speech': item['part_of_speech'],\n",
    "        'taboo': item['taboo'],\n",
    "        'category': item['category']\n",
    "    }\n",
    "    simplified_dataset.append(simplified_item)\n",
    "\n",
    "simplified_path = os.path.join(data_dir, \"chinese_dataset_simple.json\")\n",
    "with open(simplified_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(simplified_dataset, f, ensure_ascii=False, indent=2)\n",
    "print(f\"âœ… ç®€åŒ–æ•°æ®é›†å·²ä¿å­˜: {simplified_path}\")\n",
    "\n",
    "# åˆ›å»ºå®‰å…¨çš„æ ·æœ¬æ•°æ®ï¼ˆç§»é™¤å¯èƒ½ä¸å¯åºåˆ—åŒ–çš„å†…å®¹ï¼‰\n",
    "safe_sample_items = []\n",
    "for item in sample_items:\n",
    "    safe_item = {\n",
    "        'target': item['target'],\n",
    "        'part_of_speech': item['part_of_speech'],\n",
    "        'taboo': item['taboo'],\n",
    "        'category': item['category'],\n",
    "        'sense_count': len(item.get('senses', [])),\n",
    "        'first_definition': item['senses'][0].get('Def', 'æ— å®šä¹‰')[:100] if item.get('senses') else 'æ— å®šä¹‰'\n",
    "    }\n",
    "    safe_sample_items.append(safe_item)\n",
    "\n",
    "# ç”Ÿæˆæ•°æ®é›†æŠ¥å‘Š\n",
    "report = {\n",
    "    'dataset_info': {\n",
    "        'total_words': len(chinese_dataset),\n",
    "        'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'source': 'OpenHowNet',\n",
    "        'language': 'Chinese',\n",
    "        'pos_distribution': pos_counts,\n",
    "        'avg_taboo_count': sum(taboo_counts) / len(taboo_counts),\n",
    "        'avg_sense_count': sum(sense_counts) / len(sense_counts)\n",
    "    },\n",
    "    'sample_data': safe_sample_items\n",
    "}\n",
    "\n",
    "report_path = os.path.join(data_dir, \"chinese_dataset_report.json\")\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "print(f\"âœ… æ•°æ®é›†æŠ¥å‘Šå·²ä¿å­˜: {report_path}\")\n",
    "\n",
    "print(f\"\\nğŸ‰ ä¸­æ–‡Tabooæ•°æ®é›†æ„å»ºå®Œæˆï¼\")\n",
    "print(f\"ğŸ“ æ•°æ®æ–‡ä»¶ä½ç½®:\")\n",
    "print(f\"   å®Œæ•´ç‰ˆ: {chinese_dataset_path}\")\n",
    "print(f\"   ç®€åŒ–ç‰ˆ: {simplified_path}\")\n",
    "print(f\"   æŠ¥å‘Š: {report_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. APIå®¢æˆ·ç«¯è®¾ç½®ï¼ˆæ”¯æŒä¸­æ–‡æ¨¡å‹ï¼‰\n",
    "print(\"ğŸ”§ è®¾ç½®ä¸­æ–‡Tabooå®éªŒAPIå®¢æˆ·ç«¯...\")\n",
    "\n",
    "def load_api_keys(keys_path: str = \"api_keys.json\") -> Dict[str, str]:\n",
    "    \"\"\"åŠ è½½APIå¯†é’¥\"\"\"\n",
    "    with open(keys_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "class ChineseTabooClient:\n",
    "    \"\"\"ä¸­æ–‡Tabooæ¸¸æˆä¸“ç”¨APIå®¢æˆ·ç«¯\"\"\"\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "        self.headers = {\n",
    "            \"Authorization\": f\"Bearer {api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "    \n",
    "    def call_model(self, model: str, messages: List[Dict[str, str]], temperature: float = 0.3) -> str:\n",
    "        \"\"\"è°ƒç”¨æ¨¡å‹API\"\"\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": 2000\n",
    "        }\n",
    "        \n",
    "        response = requests.post(self.base_url, headers=self.headers, json=payload, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        content = result['choices'][0]['message']['content'].strip()\n",
    "        \n",
    "        return content\n",
    "\n",
    "# åˆå§‹åŒ–APIå®¢æˆ·ç«¯\n",
    "try:\n",
    "    api_keys = load_api_keys()\n",
    "    chinese_client = ChineseTabooClient(api_keys[\"OPENROUTER_API_KEY\"])\n",
    "    print(\"âœ… ä¸­æ–‡Taboo APIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ APIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}\")\n",
    "    chinese_client = None\n",
    "\n",
    "# å®šä¹‰æ”¯æŒä¸­æ–‡çš„æµ‹è¯•æ¨¡å‹\n",
    "CHINESE_TEST_MODELS = [\n",
    "    \"openai/gpt-4o\",  # GPT-4o æ”¯æŒä¸­æ–‡\n",
    "    \"google/gemini-2.5-flash\",  # Gemini æ”¯æŒä¸­æ–‡\n",
    "    \"deepseek/deepseek-chat-v3-0324\",  # DeepSeek ä¸­æ–‡æ¨¡å‹\n",
    "    \"anthropic/claude-sonnet-4\",  # Claude æ”¯æŒä¸­æ–‡\n",
    "    \"moonshotai/kimi-k2\",  # kimi-k2 çš„APIè·¯å¾„\n",
    "]\n",
    "\n",
    "# ä¸­æ–‡Tabooæ¸¸æˆé€»è¾‘\n",
    "def play_chinese_taboo_game(client, hinter_model, guesser_model, target_word, taboo_words, max_turns=5):\n",
    "    \"\"\"æ‰§è¡Œä¸€åœºä¸­æ–‡Tabooæ¸¸æˆ\"\"\"\n",
    "    \n",
    "    conversation = []\n",
    "    all_hints = []\n",
    "    all_guesses = []\n",
    "    format_errors = []\n",
    "    hinter_failed_outputs = []\n",
    "    guesser_failed_outputs = []\n",
    "    total_hinter_attempts = 0\n",
    "    total_guesser_attempts = 0\n",
    "    \n",
    "    try:\n",
    "        for turn in range(1, max_turns + 1):\n",
    "            # Hinterç»™å‡ºæç¤º\n",
    "            hinter_prompt = f\"\"\"ä½ æ­£åœ¨ç©ä¸­æ–‡Tabooæ¸¸æˆã€‚ä½ éœ€è¦ç”¨ä¸­æ–‡æè¿°ä¸€ä¸ªè¯æ±‡ï¼Œè®©å¯¹æ–¹çŒœå‡ºè¿™ä¸ªè¯ã€‚\n",
    "\n",
    "ç›®æ ‡è¯æ±‡: {target_word}\n",
    "ç¦ç”¨è¯æ±‡: {', '.join(taboo_words)}\n",
    "\n",
    "è§„åˆ™:\n",
    "1. ä½ å¿…é¡»ç”¨ä¸­æ–‡æè¿°ç›®æ ‡è¯æ±‡\n",
    "2. ä½ ä¸èƒ½ç›´æ¥è¯´å‡ºç›®æ ‡è¯æ±‡æˆ–ä»»ä½•ç¦ç”¨è¯æ±‡\n",
    "3. ä½ çš„å›ç­”åªèƒ½åŒ…å«æè¿°æ–‡å­—ï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹\n",
    "4. ä¿æŒæè¿°ç®€æ´ä½†æœ‰å¸®åŠ©\n",
    "\n",
    "è¯·ç»™å‡ºä½ çš„æè¿°:\"\"\"\n",
    "\n",
    "            # è·å–hinterçš„æç¤º\n",
    "            hint = None\n",
    "            hinter_attempts = 0\n",
    "            while hint is None and hinter_attempts < 3:\n",
    "                hinter_attempts += 1\n",
    "                total_hinter_attempts += 1\n",
    "                try:\n",
    "                    hint_response = client.call_model(hinter_model, [{\"role\": \"user\", \"content\": hinter_prompt}])\n",
    "                    hint = hint_response.strip()\n",
    "                    \n",
    "                    # æ£€æŸ¥æ˜¯å¦è¿åç¦ç”¨è¯\n",
    "                    hint_lower = hint.lower()\n",
    "                    target_lower = target_word.lower()\n",
    "                    \n",
    "                    if target_lower in hint_lower:\n",
    "                        return {\n",
    "                            'success': False,\n",
    "                            'failure_reason': 'TABOO_VIOLATION',\n",
    "                            'turns': turn,\n",
    "                            'final_guess': '',\n",
    "                            'taboo_violation_turn': turn,\n",
    "                            'taboo_violation_hint': hint,\n",
    "                            'conversation': conversation,\n",
    "                            'all_hints': all_hints,\n",
    "                            'all_guesses': all_guesses,\n",
    "                            'format_errors': format_errors,\n",
    "                            'hinter_failed_outputs': hinter_failed_outputs,\n",
    "                            'guesser_failed_outputs': guesser_failed_outputs,\n",
    "                            'total_hinter_attempts': total_hinter_attempts,\n",
    "                            'total_guesser_attempts': total_guesser_attempts\n",
    "                        }\n",
    "                    \n",
    "                    for taboo_word in taboo_words:\n",
    "                        if taboo_word.lower() in hint_lower:\n",
    "                            return {\n",
    "                                'success': False,\n",
    "                                'failure_reason': 'TABOO_VIOLATION',\n",
    "                                'turns': turn,\n",
    "                                'final_guess': '',\n",
    "                                'taboo_violation_turn': turn,\n",
    "                                'taboo_violation_hint': hint,\n",
    "                                'conversation': conversation,\n",
    "                                'all_hints': all_hints,\n",
    "                                'all_guesses': all_guesses,\n",
    "                                'format_errors': format_errors,\n",
    "                                'hinter_failed_outputs': hinter_failed_outputs,\n",
    "                                'guesser_failed_outputs': guesser_failed_outputs,\n",
    "                                'total_hinter_attempts': total_hinter_attempts,\n",
    "                                'total_guesser_attempts': total_guesser_attempts\n",
    "                            }\n",
    "                    \n",
    "                    break\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    if hinter_attempts == 3:\n",
    "                        return {\n",
    "                            'success': False,\n",
    "                            'failure_reason': 'API_FAILURE',\n",
    "                            'turns': turn,\n",
    "                            'final_guess': '',\n",
    "                            'error': f\"Hinter API failure: {e}\",\n",
    "                            'conversation': conversation,\n",
    "                            'all_hints': all_hints,\n",
    "                            'all_guesses': all_guesses,\n",
    "                            'format_errors': format_errors,\n",
    "                            'hinter_failed_outputs': hinter_failed_outputs,\n",
    "                            'guesser_failed_outputs': guesser_failed_outputs,\n",
    "                            'total_hinter_attempts': total_hinter_attempts,\n",
    "                            'total_guesser_attempts': total_guesser_attempts\n",
    "                        }\n",
    "                    hinter_failed_outputs.append(str(e))\n",
    "                    time.sleep(1)\n",
    "            \n",
    "            if hint is None:\n",
    "                return {\n",
    "                    'success': False,\n",
    "                    'failure_reason': 'FORMAT_FAILURE',\n",
    "                    'turns': turn,\n",
    "                    'final_guess': '',\n",
    "                    'conversation': conversation,\n",
    "                    'all_hints': all_hints,\n",
    "                    'all_guesses': all_guesses,\n",
    "                    'format_errors': format_errors,\n",
    "                    'hinter_failed_outputs': hinter_failed_outputs,\n",
    "                    'guesser_failed_outputs': guesser_failed_outputs,\n",
    "                    'total_hinter_attempts': total_hinter_attempts,\n",
    "                    'total_guesser_attempts': total_guesser_attempts\n",
    "                }\n",
    "            \n",
    "            all_hints.append(hint)\n",
    "            conversation.append(f\"ç¬¬{turn}è½® æç¤º: {hint}\")\n",
    "            \n",
    "            # GuesserçŒœæµ‹\n",
    "            conversation_history = \"\\n\".join(conversation)\n",
    "            guesser_prompt = f\"\"\"ä½ æ­£åœ¨ç©ä¸­æ–‡Tabooæ¸¸æˆã€‚æ ¹æ®å¯¹æ–¹çš„æè¿°ï¼Œä½ éœ€è¦çŒœå‡ºä»–æƒ³è®©ä½ çŒœçš„è¯æ±‡ã€‚\n",
    "\n",
    "æ¸¸æˆå†å²:\n",
    "{conversation_history}\n",
    "\n",
    "è¯·æ ¹æ®ä»¥ä¸Šæè¿°çŒœæµ‹ç›®æ ‡è¯æ±‡ã€‚ä½ çš„å›ç­”åªéœ€è¦åŒ…å«ä½ çŒœæµ‹çš„è¯æ±‡ï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡Šã€‚\n",
    "\n",
    "ä½ çš„çŒœæµ‹:\"\"\"\n",
    "\n",
    "            # è·å–guesserçš„çŒœæµ‹\n",
    "            guess = None\n",
    "            guesser_attempts = 0\n",
    "            while guess is None and guesser_attempts < 3:\n",
    "                guesser_attempts += 1\n",
    "                total_guesser_attempts += 1\n",
    "                try:\n",
    "                    guess_response = client.call_model(guesser_model, [{\"role\": \"user\", \"content\": guesser_prompt}])\n",
    "                    guess = guess_response.strip()\n",
    "                    \n",
    "                    # ç®€å•çš„æ ¼å¼æ£€æŸ¥\n",
    "                    if len(guess) > 50:  # å›ç­”å¤ªé•¿ï¼Œå¯èƒ½åŒ…å«è§£é‡Š\n",
    "                        # å°è¯•æå–æœ€å¯èƒ½çš„è¯æ±‡\n",
    "                        words = jieba.lcut(guess)\n",
    "                        chinese_words = [w for w in words if is_valid_chinese_word(w)]\n",
    "                        if chinese_words:\n",
    "                            guess = chinese_words[0]\n",
    "                        else:\n",
    "                            guess = guess[:10]  # æˆªå–å‰10ä¸ªå­—ç¬¦\n",
    "                    \n",
    "                    break\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    if guesser_attempts == 3:\n",
    "                        return {\n",
    "                            'success': False,\n",
    "                            'failure_reason': 'API_FAILURE',\n",
    "                            'turns': turn,\n",
    "                            'final_guess': '',\n",
    "                            'error': f\"Guesser API failure: {e}\",\n",
    "                            'conversation': conversation,\n",
    "                            'all_hints': all_hints,\n",
    "                            'all_guesses': all_guesses,\n",
    "                            'format_errors': format_errors,\n",
    "                            'hinter_failed_outputs': hinter_failed_outputs,\n",
    "                            'guesser_failed_outputs': guesser_failed_outputs,\n",
    "                            'total_hinter_attempts': total_hinter_attempts,\n",
    "                            'total_guesser_attempts': total_guesser_attempts\n",
    "                        }\n",
    "                    guesser_failed_outputs.append(str(e))\n",
    "                    time.sleep(1)\n",
    "            \n",
    "            if guess is None:\n",
    "                return {\n",
    "                    'success': False,\n",
    "                    'failure_reason': 'FORMAT_FAILURE',\n",
    "                    'turns': turn,\n",
    "                    'final_guess': '',\n",
    "                    'conversation': conversation,\n",
    "                    'all_hints': all_hints,\n",
    "                    'all_guesses': all_guesses,\n",
    "                    'format_errors': format_errors,\n",
    "                    'hinter_failed_outputs': hinter_failed_outputs,\n",
    "                    'guesser_failed_outputs': guesser_failed_outputs,\n",
    "                    'total_hinter_attempts': total_hinter_attempts,\n",
    "                    'total_guesser_attempts': total_guesser_attempts\n",
    "                }\n",
    "            \n",
    "            all_guesses.append(guess)\n",
    "            conversation.append(f\"ç¬¬{turn}è½® çŒœæµ‹: {guess}\")\n",
    "            \n",
    "            # æ£€æŸ¥æ˜¯å¦çŒœå¯¹\n",
    "            if guess.lower().strip() == target_word.lower().strip():\n",
    "                return {\n",
    "                    'success': True,\n",
    "                    'turns': turn,\n",
    "                    'final_guess': guess,\n",
    "                    'conversation': conversation,\n",
    "                    'all_hints': all_hints,\n",
    "                    'all_guesses': all_guesses,\n",
    "                    'format_errors': format_errors,\n",
    "                    'hinter_failed_outputs': hinter_failed_outputs,\n",
    "                    'guesser_failed_outputs': guesser_failed_outputs,\n",
    "                    'total_hinter_attempts': total_hinter_attempts,\n",
    "                    'total_guesser_attempts': total_guesser_attempts\n",
    "                }\n",
    "        \n",
    "        # å¦‚æœæ‰€æœ‰è½®æ¬¡éƒ½ç”¨å®Œäº†è¿˜æ²¡çŒœå¯¹\n",
    "        return {\n",
    "            'success': False,\n",
    "            'failure_reason': 'MAX_TURNS_EXCEEDED',\n",
    "            'turns': max_turns,\n",
    "            'final_guess': all_guesses[-1] if all_guesses else '',\n",
    "            'conversation': conversation,\n",
    "            'all_hints': all_hints,\n",
    "            'all_guesses': all_guesses,\n",
    "            'format_errors': format_errors,\n",
    "            'hinter_failed_outputs': hinter_failed_outputs,\n",
    "            'guesser_failed_outputs': guesser_failed_outputs,\n",
    "            'total_hinter_attempts': total_hinter_attempts,\n",
    "            'total_guesser_attempts': total_guesser_attempts\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'failure_reason': 'EXCEPTION',\n",
    "            'turns': 0,\n",
    "            'final_guess': '',\n",
    "            'error': str(e),\n",
    "            'conversation': conversation,\n",
    "            'all_hints': all_hints,\n",
    "            'all_guesses': all_guesses,\n",
    "            'format_errors': format_errors,\n",
    "            'hinter_failed_outputs': hinter_failed_outputs,\n",
    "            'guesser_failed_outputs': guesser_failed_outputs,\n",
    "            'total_hinter_attempts': total_hinter_attempts,\n",
    "            'total_guesser_attempts': total_guesser_attempts\n",
    "        }\n",
    "\n",
    "print(\"âœ… ä¸­æ–‡Tabooæ¸¸æˆé€»è¾‘å·²å®šä¹‰\")\n",
    "\n",
    "print(f\"ğŸ¤– ä¸­æ–‡å®éªŒæ¨¡å‹åˆ—è¡¨ ({len(CHINESE_TEST_MODELS)} ä¸ª):\")\n",
    "for i, model in enumerate(CHINESE_TEST_MODELS, 1):\n",
    "    print(f\"   {i}. {model}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ é€‰æ‹©è¾ƒå°‘æ¨¡å‹è¿›è¡Œæµ‹è¯•ä»¥èŠ‚çœæˆæœ¬å’Œæ—¶é—´\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. è¿è¡Œå®Œæ•´ä¸­æ–‡Tabooå®éªŒ\n",
    "print(\"ğŸš€ å¼€å§‹è¿è¡Œå®Œæ•´ä¸­æ–‡Tabooå®éªŒ...\")\n",
    "\n",
    "# é€‰æ‹©å®éªŒè§„æ¨¡\n",
    "experiment_scales = {\n",
    "    'quick': {'count': 5, 'desc': 'å¿«é€Ÿæµ‹è¯•ï¼ˆ5ä¸ªè¯æ±‡ï¼‰'},\n",
    "    'medium': {'count': 20, 'desc': 'ä¸­ç­‰è§„æ¨¡ï¼ˆ20ä¸ªè¯æ±‡ï¼‰'},\n",
    "    'full': {'count': 100, 'desc': 'å®Œæ•´å®éªŒï¼ˆ100ä¸ªè¯æ±‡ï¼‰'}\n",
    "}\n",
    "\n",
    "# è®¾ç½®å®éªŒè§„æ¨¡ - å¯ä»¥ä¿®æ”¹è¿™é‡Œé€‰æ‹©ä¸åŒè§„æ¨¡\n",
    "EXPERIMENT_SCALE = 'quick'  # æ”¹ä¸º 'medium' æˆ– 'full' æ¥è¿è¡Œæ›´å¤§è§„æ¨¡å®éªŒ\n",
    "\n",
    "scale_config = experiment_scales[EXPERIMENT_SCALE]\n",
    "experiment_dataset = chinese_dataset[:scale_config['count']]\n",
    "\n",
    "print(f\"ğŸ“Š å®éªŒé…ç½®: {scale_config['desc']}\")\n",
    "print(f\"ğŸ¯ è¯æ±‡æ•°é‡: {len(experiment_dataset)}\")\n",
    "print(f\"ğŸ¤– æ¨¡å‹æ•°é‡: {len(CHINESE_TEST_MODELS)}\")\n",
    "print(f\"ğŸ® æ€»æ¸¸æˆæ•°: {len(experiment_dataset) * len(CHINESE_TEST_MODELS) * len(CHINESE_TEST_MODELS)}\")\n",
    "\n",
    "if chinese_client and chinese_dataset:\n",
    "    try:\n",
    "        # è¿è¡Œå®éªŒ\n",
    "        experiment_results = run_full_chinese_experiment(\n",
    "            chinese_client, \n",
    "            CHINESE_TEST_MODELS, \n",
    "            experiment_dataset,\n",
    "            f\"chinese_taboo_{EXPERIMENT_SCALE}\"\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nğŸ‰ {scale_config['desc']}å®Œæˆï¼\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å®éªŒæ‰§è¡Œå¤±è´¥: {e}\")\n",
    "        print(\"ğŸ’¡ è¯·æ£€æŸ¥APIå¯†é’¥å’Œç½‘ç»œè¿æ¥\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ æ— æ³•è¿è¡Œå®éªŒï¼šAPIå®¢æˆ·ç«¯æˆ–æ•°æ®é›†æœªå‡†å¤‡å°±ç»ª\")\n",
    "    if not chinese_client:\n",
    "        print(\"   - APIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–\")\n",
    "    if not chinese_dataset:\n",
    "        print(\"   - æ•°æ®é›†æœªæ„å»º\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯åŠ¨ä¸­æ–‡Tabooå®Œæ•´å®éªŒ\n",
    "print(\"ğŸš€ å¯åŠ¨ä¸­æ–‡Tabooå®Œæ•´å®éªŒ...\")\n",
    "\n",
    "# å®éªŒé€‰é¡¹ï¼šç”¨æˆ·å¯ä»¥é€‰æ‹©å®éªŒè§„æ¨¡\n",
    "EXPERIMENT_OPTIONS = {\n",
    "    'quick': {'size': 5, 'name': 'å¿«é€Ÿæµ‹è¯•'},\n",
    "    'medium': {'size': 20, 'name': 'ä¸­ç­‰è§„æ¨¡'}, \n",
    "    'full': {'size': 100, 'name': 'å®Œæ•´å®éªŒ'}\n",
    "}\n",
    "\n",
    "# é»˜è®¤é€‰æ‹©å¿«é€Ÿæµ‹è¯•\n",
    "selected_option = 'quick'\n",
    "selected_size = EXPERIMENT_OPTIONS[selected_option]['size']\n",
    "\n",
    "print(f\"ğŸ“Š å®éªŒè§„æ¨¡: {EXPERIMENT_OPTIONS[selected_option]['name']} ({selected_size} ä¸ªè¯æ±‡)\")\n",
    "print(f\"ğŸ¤– æµ‹è¯•æ¨¡å‹: {len(CHINESE_TEST_MODELS)} ä¸ª\")\n",
    "print(f\"ğŸ¯ é¢„è®¡æ¸¸æˆæ•°: {selected_size * len(CHINESE_TEST_MODELS) * len(CHINESE_TEST_MODELS)}\")\n",
    "\n",
    "# æ£€æŸ¥å¿…è¦ç»„ä»¶\n",
    "if 'chinese_client' in globals() and chinese_client and 'chinese_dataset' in globals() and chinese_dataset:\n",
    "    print(\"âœ… æ‰€æœ‰ç»„ä»¶å·²å°±ç»ªï¼Œå¯ä»¥å¼€å§‹å®éªŒ\")\n",
    "    \n",
    "    # é€‰æ‹©æ•°æ®é›†å­é›†\n",
    "    test_dataset = chinese_dataset[:selected_size]\n",
    "    \n",
    "    print(f\"\\\\nğŸ¯ é€‰æ‹©çš„æµ‹è¯•è¯æ±‡:\")\n",
    "    for i, item in enumerate(test_dataset[:5], 1):  # æ˜¾ç¤ºå‰5ä¸ª\n",
    "        print(f\"   {i}. {item['target']} ({item['part_of_speech']}) - ç¦ç”¨è¯: {item['taboo']}\")\n",
    "    if len(test_dataset) > 5:\n",
    "        print(f\"   ... è¿˜æœ‰ {len(test_dataset) - 5} ä¸ªè¯æ±‡\")\n",
    "    \n",
    "    print(f\"\\\\nğŸ’¡ è¦æ‰§è¡Œå®Œæ•´å®éªŒï¼Œè¯·è¿è¡Œä¸‹ä¸€ä¸ªcell\")\n",
    "    print(f\"ğŸ’¡ è¦ä¿®æ”¹å®éªŒè§„æ¨¡ï¼Œè¯·ä¿®æ”¹ä¸Šé¢çš„ selected_option å˜é‡\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ ç¼ºå°‘å¿…è¦ç»„ä»¶:\")\n",
    "    if 'chinese_client' not in globals() or not chinese_client:\n",
    "        print(\"   - APIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–\")\n",
    "    if 'chinese_dataset' not in globals() or not chinese_dataset:\n",
    "        print(\"   - ä¸­æ–‡æ•°æ®é›†æœªå‡†å¤‡å¥½\")\n",
    "    print(\"ğŸ’¡ è¯·å…ˆè¿è¡Œå‰é¢çš„cellæ¥åˆå§‹åŒ–è¿™äº›ç»„ä»¶\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰§è¡Œä¸­æ–‡Tabooå®Œæ•´å®éªŒ\n",
    "print(\"ğŸ® æ‰§è¡Œä¸­æ–‡Tabooå®Œæ•´å®éªŒ...\")\n",
    "\n",
    "def execute_chinese_experiment(dataset, models, client):\n",
    "    \"\"\"æ‰§è¡Œä¸­æ–‡Tabooå®éªŒ\"\"\"\n",
    "    \n",
    "    # åˆ›å»ºç»“æœç›®å½•\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    results_dir = f\"results/chinese_experiment_{timestamp}\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"ğŸ“ ç»“æœä¿å­˜ç›®å½•: {results_dir}\")\n",
    "    \n",
    "    all_results = []\n",
    "    game_counter = 0\n",
    "    total_games = len(dataset) * len(models) * len(models)\n",
    "    \n",
    "    print(f\"ğŸš€ å¼€å§‹æ‰§è¡Œ {total_games} ä¸ªæ¸¸æˆ...\")\n",
    "    \n",
    "    for word_idx, word_data in enumerate(dataset, 1):\n",
    "        target_word = word_data['target']\n",
    "        taboo_words = word_data['taboo']\n",
    "        pos = word_data['part_of_speech']\n",
    "        \n",
    "        print(f\"\\\\nğŸ¯ è¯æ±‡ {word_idx}/{len(dataset)}: {target_word} ({pos})\")\n",
    "        print(f\"ğŸš« ç¦ç”¨è¯: {taboo_words}\")\n",
    "        \n",
    "        word_success = 0\n",
    "        word_total = 0\n",
    "        \n",
    "        for hinter_model in models:\n",
    "            for guesser_model in models:\n",
    "                game_counter += 1\n",
    "                word_total += 1\n",
    "                \n",
    "                hinter_name = hinter_model.split('/')[-1]\n",
    "                guesser_name = guesser_model.split('/')[-1]\n",
    "                \n",
    "                print(f\"  ğŸ”„ {game_counter}/{total_games}: {hinter_name}â†’{guesser_name}\", end=\" \")\n",
    "                \n",
    "                try:\n",
    "                    start_time = time.time()\n",
    "                    \n",
    "                    # æ‰§è¡Œæ¸¸æˆ\n",
    "                    game_result = play_chinese_taboo_game(\n",
    "                        client, hinter_model, guesser_model, \n",
    "                        target_word, taboo_words, max_turns=5\n",
    "                    )\n",
    "                    \n",
    "                    duration = round(time.time() - start_time, 2)\n",
    "                    \n",
    "                    # è®°å½•ç»“æœ\n",
    "                    result = {\n",
    "                        'game_id': game_counter,\n",
    "                        'word_index': word_idx,\n",
    "                        'target_word': target_word,\n",
    "                        'part_of_speech': pos,\n",
    "                        'taboo_words': '|'.join(taboo_words),\n",
    "                        'hinter_model': hinter_model,\n",
    "                        'guesser_model': guesser_model,\n",
    "                        'success': game_result['success'],\n",
    "                        'turns_used': game_result['turns'],\n",
    "                        'final_guess': game_result['final_guess'],\n",
    "                        'failure_reason': game_result.get('failure_reason', ''),\n",
    "                        'duration_seconds': duration,\n",
    "                        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    }\n",
    "                    \n",
    "                    all_results.append(result)\n",
    "                    \n",
    "                    if game_result['success']:\n",
    "                        word_success += 1\n",
    "                        print(f\"âœ… {game_result['turns']}è½® {duration}s\")\n",
    "                    else:\n",
    "                        reason = game_result.get('failure_reason', 'unknown')\n",
    "                        if reason == 'TABOO_VIOLATION':\n",
    "                            print(f\"âŒ è¿è§„ {duration}s\")\n",
    "                        elif reason == 'MAX_TURNS_EXCEEDED':\n",
    "                            print(f\"âŒ è½®æ•° {duration}s\")\n",
    "                        else:\n",
    "                            print(f\"âŒ {reason[:10]} {duration}s\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"âŒ å¼‚å¸¸: {str(e)[:20]}...\")\n",
    "                    result = {\n",
    "                        'game_id': game_counter,\n",
    "                        'target_word': target_word,\n",
    "                        'hinter_model': hinter_model,\n",
    "                        'guesser_model': guesser_model,\n",
    "                        'success': False,\n",
    "                        'failure_reason': 'EXCEPTION',\n",
    "                        'error': str(e),\n",
    "                        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    }\n",
    "                    all_results.append(result)\n",
    "                \n",
    "                time.sleep(0.1)  # é¿å…APIé™åˆ¶\n",
    "        \n",
    "        # æ˜¾ç¤ºè¯æ±‡å°ç»“\n",
    "        word_rate = word_success / word_total * 100 if word_total > 0 else 0\n",
    "        print(f\"  ğŸ“Š '{target_word}' æˆåŠŸç‡: {word_success}/{word_total} ({word_rate:.1f}%)\")\n",
    "    \n",
    "    # ä¿å­˜å®Œæ•´ç»“æœ\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_file = f\"{results_dir}/complete_results.csv\"\n",
    "    results_df.to_csv(results_file, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    # ç”ŸæˆæŠ¥å‘Š\n",
    "    total_success = len([r for r in all_results if r.get('success', False)])\n",
    "    overall_rate = total_success / len(all_results) * 100 if all_results else 0\n",
    "    \n",
    "    print(f\"\\\\nğŸ‰ å®éªŒå®Œæˆï¼\")\n",
    "    print(f\"ğŸ“Š æ•´ä½“ç»“æœ:\")\n",
    "    print(f\"   æ€»æ¸¸æˆæ•°: {len(all_results)}\")\n",
    "    print(f\"   æˆåŠŸæ•°: {total_success}\")\n",
    "    print(f\"   æˆåŠŸç‡: {overall_rate:.1f}%\")\n",
    "    \n",
    "    # æŒ‰æ¨¡å‹ç»Ÿè®¡\n",
    "    print(f\"\\\\nğŸ¤– æ¨¡å‹è¡¨ç°:\")\n",
    "    for model in models:\n",
    "        model_name = model.split('/')[-1]\n",
    "        \n",
    "        # ä½œä¸ºæç¤ºè€…\n",
    "        as_hinter = [r for r in all_results if r.get('hinter_model') == model]\n",
    "        hinter_success = len([r for r in as_hinter if r.get('success', False)])\n",
    "        hinter_rate = hinter_success / len(as_hinter) * 100 if as_hinter else 0\n",
    "        \n",
    "        # ä½œä¸ºçŒœæµ‹è€…  \n",
    "        as_guesser = [r for r in all_results if r.get('guesser_model') == model]\n",
    "        guesser_success = len([r for r in as_guesser if r.get('success', False)])\n",
    "        guesser_rate = guesser_success / len(as_guesser) * 100 if as_guesser else 0\n",
    "        \n",
    "        print(f\"   {model_name}:\")\n",
    "        print(f\"     æç¤ºè€…: {hinter_success}/{len(as_hinter)} ({hinter_rate:.1f}%)\")\n",
    "        print(f\"     çŒœæµ‹è€…: {guesser_success}/{len(as_guesser)} ({guesser_rate:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\\\nğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜: {results_file}\")\n",
    "    return all_results, results_dir\n",
    "\n",
    "# æ‰§è¡Œå®éªŒ\n",
    "if 'test_dataset' in globals() and test_dataset and chinese_client:\n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ§ª å¼€å§‹ä¸­æ–‡Tabooå®Œæ•´å®éªŒ\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    experiment_results, results_directory = execute_chinese_experiment(\n",
    "        test_dataset, CHINESE_TEST_MODELS, chinese_client\n",
    "    )\n",
    "    \n",
    "    print(f\"\\\\nâœ… å®éªŒå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {results_directory}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ æ— æ³•æ‰§è¡Œå®éªŒï¼š\")\n",
    "    if 'test_dataset' not in globals():\n",
    "        print(\"   - æµ‹è¯•æ•°æ®é›†æœªå®šä¹‰ï¼Œè¯·å…ˆè¿è¡Œä¸Šä¸€ä¸ªcell\")\n",
    "    if 'chinese_client' not in globals() or not chinese_client:\n",
    "        print(\"   - APIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. ä¸­æ–‡Tabooæ¸¸æˆæ ¸å¿ƒé€»è¾‘å’Œå·¥å…·å‡½æ•°\n",
    "print(\"ğŸ® å®šä¹‰ä¸­æ–‡Tabooæ¸¸æˆæ ¸å¿ƒé€»è¾‘...\")\n",
    "\n",
    "def safe_chinese_text_cleanup(text: str, max_length: int = 300) -> str:\n",
    "    \"\"\"å®‰å…¨æ¸…ç†ä¸­æ–‡æ–‡æœ¬\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # ä¿ç•™ä¸­æ–‡å­—ç¬¦ã€è‹±æ–‡å­—ç¬¦ã€æ•°å­—å’Œå¸¸ç”¨æ ‡ç‚¹\n",
    "    import re\n",
    "    cleaned = re.sub(r'[^\\u4e00-\\u9fff\\w\\s\\.,!?;:\"\\'()[\\]{}\\-]', '', str(text))\n",
    "    \n",
    "    if len(cleaned) > max_length:\n",
    "        cleaned = cleaned[:max_length] + \"...\"\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def extract_chinese_clue_text(response: str) -> str:\n",
    "    \"\"\"ä»å“åº”ä¸­æå–ä¸­æ–‡çº¿ç´¢æ–‡æœ¬\"\"\"\n",
    "    if \"FORMAT_ERROR_EXCEEDED\" in response:\n",
    "        return \"FORMAT_ERROR\"\n",
    "    \n",
    "    # æ£€æŸ¥ä¸­æ–‡æ ¼å¼æ ‡è®°\n",
    "    if '[çº¿ç´¢]' in response or '[CLUE]' in response.upper():\n",
    "        import re\n",
    "        # ä¼˜å…ˆåŒ¹é…ä¸­æ–‡æ ‡è®°\n",
    "        match = re.search(r'\\[çº¿ç´¢\\]\\s*(.+)', response, re.DOTALL)\n",
    "        if not match:\n",
    "            match = re.search(r'\\[CLUE\\]\\s*(.+)', response, re.IGNORECASE | re.DOTALL)\n",
    "        \n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "    \n",
    "    # å¤‡ç”¨æ ¼å¼\n",
    "    if 'çº¿ç´¢:' in response or 'Clue:' in response:\n",
    "        if 'çº¿ç´¢:' in response:\n",
    "            return response.split('çº¿ç´¢:')[1].strip()\n",
    "        else:\n",
    "            return response.split('Clue:')[1].strip()\n",
    "    \n",
    "    return \"INVALID_FORMAT\"\n",
    "\n",
    "def extract_chinese_guess_word(response: str) -> str:\n",
    "    \"\"\"ä»å“åº”ä¸­æå–ä¸­æ–‡çŒœæµ‹è¯\"\"\"\n",
    "    if \"FORMAT_ERROR_EXCEEDED\" in response:\n",
    "        return \"FORMAT_ERROR\"\n",
    "    \n",
    "    # æ£€æŸ¥ä¸­æ–‡æ ¼å¼æ ‡è®°\n",
    "    if '[çŒœæµ‹]' in response or '[GUESS]' in response.upper():\n",
    "        import re\n",
    "        # ä¼˜å…ˆåŒ¹é…ä¸­æ–‡æ ‡è®°\n",
    "        match = re.search(r'\\[çŒœæµ‹\\]\\s*(.+)', response)\n",
    "        if not match:\n",
    "            match = re.search(r'\\[GUESS\\]\\s*(.+)', response, re.IGNORECASE)\n",
    "        \n",
    "        if match:\n",
    "            guess_part = match.group(1).strip()\n",
    "            # æå–ç¬¬ä¸€ä¸ªä¸­æ–‡è¯æ±‡\n",
    "            chinese_words = re.findall(r'[\\u4e00-\\u9fff]+', guess_part)\n",
    "            if chinese_words:\n",
    "                return chinese_words[0]\n",
    "    \n",
    "    # å¤‡ç”¨æ ¼å¼\n",
    "    if 'çŒœæµ‹:' in response or 'Guess:' in response:\n",
    "        if 'çŒœæµ‹:' in response:\n",
    "            guess_part = response.split('çŒœæµ‹:')[1].strip()\n",
    "        else:\n",
    "            guess_part = response.split('Guess:')[1].strip()\n",
    "        \n",
    "        chinese_words = re.findall(r'[\\u4e00-\\u9fff]+', guess_part)\n",
    "        if chinese_words:\n",
    "            return chinese_words[0]\n",
    "    \n",
    "    return \"INVALID_FORMAT\"\n",
    "\n",
    "def check_chinese_taboo_violation(hint: str, taboo_words: List[str]) -> bool:\n",
    "    \"\"\"æ£€æŸ¥ä¸­æ–‡çº¿ç´¢æ˜¯å¦è¿åç¦ç”¨è¯è§„åˆ™\"\"\"\n",
    "    hint_cleaned = re.sub(r'[^\\u4e00-\\u9fff]', '', hint.lower())\n",
    "    \n",
    "    for taboo in taboo_words:\n",
    "        taboo_cleaned = re.sub(r'[^\\u4e00-\\u9fff]', '', taboo.lower())\n",
    "        \n",
    "        # æ£€æŸ¥å®Œæ•´åŒ¹é…\n",
    "        if taboo_cleaned in hint_cleaned:\n",
    "            return True\n",
    "        \n",
    "        # æ£€æŸ¥éƒ¨åˆ†åŒ¹é…ï¼ˆå¯¹äºè¾ƒé•¿çš„è¯ï¼‰\n",
    "        if len(taboo_cleaned) >= 2:\n",
    "            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç¦ç”¨è¯çš„ä¸»è¦éƒ¨åˆ†\n",
    "            if len(taboo_cleaned) >= 3:\n",
    "                core_part = taboo_cleaned[:2]  # å–å‰ä¸¤ä¸ªå­—ç¬¦ä½œä¸ºæ ¸å¿ƒ\n",
    "                if core_part in hint_cleaned:\n",
    "                    return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def robust_chinese_api_call(client, model: str, base_prompt: str, expected_prefix: str, max_retries: int = 3):\n",
    "    \"\"\"å¥å£®çš„ä¸­æ–‡APIè°ƒç”¨\"\"\"\n",
    "    failed_outputs = []\n",
    "    \n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            if attempt == 1:\n",
    "                prompt = base_prompt\n",
    "            else:\n",
    "                prev_output = failed_outputs[-1] if failed_outputs else \"æœªçŸ¥\"\n",
    "                format_reminder = f\"\"\"\n",
    "\n",
    "âš ï¸ æ ¼å¼é”™è¯¯ âš ï¸\n",
    "æ‚¨ä¹‹å‰çš„å›å¤æ˜¯: \"{prev_output}\"\n",
    "\n",
    "å¿…éœ€æ ¼å¼:\n",
    "- æ‚¨å¿…é¡»ä»¥ '{expected_prefix}' å¼€å¤´ï¼ˆåŒ…æ‹¬æ–¹æ‹¬å·ï¼‰\n",
    "- ä¸è¦åœ¨ {expected_prefix} å‰æ·»åŠ ä»»ä½•æ–‡å­—\n",
    "\n",
    "è¯·ä½¿ç”¨æ­£ç¡®æ ¼å¼é‡è¯•:\"\"\"\n",
    "                prompt = base_prompt + format_reminder\n",
    "            \n",
    "            response = client.call_model(model, [{\"role\": \"user\", \"content\": prompt}])\n",
    "            \n",
    "            if (response.strip().startswith(expected_prefix) or \n",
    "                response.strip().upper().startswith(expected_prefix.upper())):\n",
    "                return {\n",
    "                    'success': True,\n",
    "                    'response': response,\n",
    "                    'attempts': attempt,\n",
    "                    'error': None,\n",
    "                    'failed_outputs': failed_outputs\n",
    "                }\n",
    "            else:\n",
    "                safe_response = safe_chinese_text_cleanup(response, max_length=150)\n",
    "                failed_outputs.append(safe_response)\n",
    "                \n",
    "                if attempt == max_retries:\n",
    "                    all_failed = \" | \".join(failed_outputs)\n",
    "                    return {\n",
    "                        'success': False,\n",
    "                        'response': f\"FORMAT_ERROR_EXCEEDED: {safe_response}\",\n",
    "                        'attempts': attempt,\n",
    "                        'error': f\"å°è¯• {max_retries} æ¬¡åå¤±è´¥ã€‚æœŸæœ›æ ¼å¼ '{expected_prefix}'ã€‚æ‰€æœ‰å¤±è´¥è¾“å‡º: {all_failed}\",\n",
    "                        'failed_outputs': failed_outputs\n",
    "                    }\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "        except Exception as e:\n",
    "            safe_error = safe_chinese_text_cleanup(str(e), max_length=150)\n",
    "            error_msg = f\"APIé”™è¯¯ (å°è¯• {attempt}/{max_retries}): {safe_error}\"\n",
    "            \n",
    "            if attempt == max_retries:\n",
    "                return {\n",
    "                    'success': False,\n",
    "                    'response': None,\n",
    "                    'attempts': attempt,\n",
    "                    'error': error_msg,\n",
    "                    'failed_outputs': failed_outputs\n",
    "                }\n",
    "            time.sleep(1.0)\n",
    "    \n",
    "    return {\n",
    "        'success': False,\n",
    "        'response': None,\n",
    "        'attempts': max_retries,\n",
    "        'error': \"è¶…å‡ºæœ€å¤§é‡è¯•æ¬¡æ•°\",\n",
    "        'failed_outputs': failed_outputs\n",
    "    }\n",
    "\n",
    "print(\"âœ… ä¸­æ–‡Tabooæ¸¸æˆæ ¸å¿ƒé€»è¾‘å·²å®šä¹‰\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. ä¸­æ–‡Tabooæ¸¸æˆä¸»å‡½æ•°\n",
    "def play_chinese_taboo_game(client, hinter_model, guesser_model, target_word, taboo_words, max_turns=5):\n",
    "    \"\"\"ä¸­æ–‡Tabooæ¸¸æˆä¸»å‡½æ•°\"\"\"\n",
    "    \n",
    "    conversation_history = []\n",
    "    taboo_list = 'ã€'.join(taboo_words)\n",
    "    \n",
    "    # ä¸­æ–‡ç³»ç»Ÿæç¤º - Hinter\n",
    "    hinter_system = f\"\"\"æ‚¨æ­£åœ¨ç©ä¸­æ–‡Tabooæ¸¸æˆï¼Œæ‹…ä»»æç¤ºè€…è§’è‰²ã€‚\n",
    "\n",
    "å…³é”®æ ¼å¼è¦æ±‚:\n",
    "- æ‚¨å¿…é¡»ä»¥ '[çº¿ç´¢]' å¼€å¤´å›å¤ï¼ˆåŒ…æ‹¬æ–¹æ‹¬å·ï¼‰\n",
    "- ä¸è¦åœ¨ [çº¿ç´¢] å‰æ·»åŠ ä»»ä½•æ–‡å­—\n",
    "- ç¤ºä¾‹: [çº¿ç´¢] è¿™æ˜¯ä¸€ç§æœ‰é•¿é¼»å­çš„å¤§å‹å“ºä¹³åŠ¨ç‰©\n",
    "\n",
    "æ¸¸æˆè§„åˆ™:\n",
    "1. ç»å¯¹ä¸èƒ½ä½¿ç”¨è¿™äº›ç¦ç”¨è¯: {taboo_list}\n",
    "2. ç»™å‡ºæœ‰ç”¨çš„çº¿ç´¢å¸®åŠ©çŒœæµ‹è€…æ‰¾åˆ°ç›®æ ‡è¯\n",
    "3. å¯ä»¥çœ‹åˆ°å¯¹è¯å†å²ï¼Œåœ¨ä¹‹å‰çº¿ç´¢åŸºç¡€ä¸Šè¡¥å……\n",
    "4. è¦æœ‰åˆ›æ„ä½†ä¸¥æ ¼éµå¾ªæ ¼å¼\n",
    "5. ä½¿ç”¨æ ‡å‡†ä¸­æ–‡è¡¨è¾¾\n",
    "\n",
    "ç›®æ ‡è¯: {target_word}\n",
    "ç¦ç”¨è¯ï¼ˆç»å¯¹ç¦æ­¢ï¼‰: {taboo_list}\"\"\"\n",
    "\n",
    "    # ä¸­æ–‡ç³»ç»Ÿæç¤º - Guesser\n",
    "    guesser_system = \"\"\"æ‚¨æ­£åœ¨ç©ä¸­æ–‡Tabooæ¸¸æˆï¼Œæ‹…ä»»çŒœæµ‹è€…è§’è‰²ã€‚\n",
    "\n",
    "å…³é”®æ ¼å¼è¦æ±‚:\n",
    "- æ‚¨å¿…é¡»ä»¥ '[çŒœæµ‹]' å¼€å¤´å›å¤ï¼ˆåŒ…æ‹¬æ–¹æ‹¬å·ï¼‰\n",
    "- ä¸è¦åœ¨ [çŒœæµ‹] å‰æ·»åŠ ä»»ä½•æ–‡å­—\n",
    "- ç¤ºä¾‹: [çŒœæµ‹] å¤§è±¡\n",
    "\n",
    "æ¸¸æˆè§„åˆ™:\n",
    "1. æ ¹æ®æ”¶åˆ°çš„æ‰€æœ‰çº¿ç´¢è¿›è¡Œæœ€ä½³çŒœæµ‹\n",
    "2. å¯ä»¥çœ‹åˆ°å¯¹è¯å†å²\n",
    "3. åœ¨ [çŒœæµ‹] ååªç»™å‡ºä¸€ä¸ªä¸­æ–‡è¯æ±‡ä½œä¸ºç­”æ¡ˆ\n",
    "4. ä½¿ç”¨æ ‡å‡†ä¸­æ–‡è¯æ±‡\"\"\"\n",
    "\n",
    "    # è®°å½•ç»Ÿè®¡ä¿¡æ¯\n",
    "    total_hinter_attempts = 0\n",
    "    total_guesser_attempts = 0\n",
    "    format_errors = []\n",
    "    hinter_failed_outputs = []\n",
    "    guesser_failed_outputs = []\n",
    "\n",
    "    for turn in range(1, max_turns + 1):\n",
    "        # æ„å»ºHinteræç¤º\n",
    "        if turn == 1:\n",
    "            hinter_prompt = f\"{hinter_system}\\n\\nè¯·æä¾›æ‚¨çš„ç¬¬ä¸€ä¸ªçº¿ç´¢:\"\n",
    "        else:\n",
    "            history_text = \"\\n\".join([f\"ç¬¬{i}è½®: {msg}\" for i, msg in enumerate(conversation_history, 1)])\n",
    "            hinter_prompt = f\"{hinter_system}\\n\\nå¯¹è¯å†å²:\\n{history_text}\\n\\nçŒœæµ‹è€…è¿˜æ²¡æœ‰æ‰¾åˆ°ç­”æ¡ˆã€‚è¯·æä¾›ä¸‹ä¸€ä¸ªçº¿ç´¢:\"\n",
    "        \n",
    "        # Hinterç»™å‡ºçº¿ç´¢\n",
    "        hinter_result = robust_chinese_api_call(client, hinter_model, hinter_prompt, \"[çº¿ç´¢]\", max_retries=3)\n",
    "        total_hinter_attempts += hinter_result['attempts']\n",
    "        \n",
    "        if hinter_result.get('failed_outputs'):\n",
    "            hinter_failed_outputs.extend(hinter_result['failed_outputs'])\n",
    "        \n",
    "        if not hinter_result['success']:\n",
    "            error_type = \"FORMAT_FAILURE\" if \"FORMAT_ERROR_EXCEEDED\" in str(hinter_result.get('response', '')) else \"API_FAILURE\"\n",
    "            format_errors.append(f\"ç¬¬{turn}è½® æç¤ºè€…: {hinter_result['error']}\")\n",
    "            \n",
    "            return {\n",
    "                'success': False,\n",
    "                'turns': turn,\n",
    "                'conversation': conversation_history,\n",
    "                'final_guess': f\"HINTER_{error_type}\",\n",
    "                'error': f\"{error_type}: {hinter_result['error']}\",\n",
    "                'failure_reason': error_type,\n",
    "                'total_hinter_attempts': total_hinter_attempts,\n",
    "                'total_guesser_attempts': total_guesser_attempts,\n",
    "                'format_errors': format_errors,\n",
    "                'hinter_failed_outputs': hinter_failed_outputs,\n",
    "                'guesser_failed_outputs': guesser_failed_outputs,\n",
    "                'all_hints': [msg for msg in conversation_history if msg.startswith('æç¤ºè€…:')],\n",
    "                'all_guesses': [msg for msg in conversation_history if msg.startswith('çŒœæµ‹è€…:')]\n",
    "            }\n",
    "        \n",
    "        # æå–çº¿ç´¢å¹¶æ£€æŸ¥taboo violation\n",
    "        hint_text = extract_chinese_clue_text(hinter_result['response'])\n",
    "        \n",
    "        # æ£€æŸ¥æ˜¯å¦è¿åç¦ç”¨è¯è§„åˆ™\n",
    "        taboo_violated = check_chinese_taboo_violation(hint_text, taboo_words)\n",
    "        if taboo_violated:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'turns': turn,\n",
    "                'conversation': conversation_history,\n",
    "                'final_guess': 'è¿åç¦ç”¨è¯è§„åˆ™: æç¤ºè€…è¿è§„',\n",
    "                'error': f'è¿åç¦ç”¨è¯è§„åˆ™: æç¤ºè€…åœ¨ç¬¬{turn}è½®è¿åè§„åˆ™ï¼Œä½¿ç”¨äº†ç¦ç”¨è¯: {hint_text}',\n",
    "                'failure_reason': 'TABOO_VIOLATION',\n",
    "                'taboo_violation_turn': turn,\n",
    "                'taboo_violation_hint': hint_text,\n",
    "                'total_hinter_attempts': total_hinter_attempts,\n",
    "                'total_guesser_attempts': total_guesser_attempts,\n",
    "                'format_errors': format_errors,\n",
    "                'hinter_failed_outputs': hinter_failed_outputs,\n",
    "                'guesser_failed_outputs': guesser_failed_outputs,\n",
    "                'all_hints': [msg for msg in conversation_history if msg.startswith('æç¤ºè€…:')],\n",
    "                'all_guesses': [msg for msg in conversation_history if msg.startswith('çŒœæµ‹è€…:')]\n",
    "            }\n",
    "        \n",
    "        conversation_history.append(f\"æç¤ºè€…: {hinter_result['response']}\")\n",
    "        \n",
    "        # æ„å»ºGuesseræç¤º\n",
    "        history_text = \"\\n\".join([f\"ç¬¬{i}è½®: {msg}\" for i, msg in enumerate(conversation_history, 1)])\n",
    "        guesser_prompt = f\"{guesser_system}\\n\\nå¯¹è¯å†å²:\\n{history_text}\\n\\næ‚¨çš„çŒœæµ‹æ˜¯ä»€ä¹ˆ?\"\n",
    "        \n",
    "        # Guesserè¿›è¡ŒçŒœæµ‹\n",
    "        guesser_result = robust_chinese_api_call(client, guesser_model, guesser_prompt, \"[çŒœæµ‹]\", max_retries=3)\n",
    "        total_guesser_attempts += guesser_result['attempts']\n",
    "        \n",
    "        if guesser_result.get('failed_outputs'):\n",
    "            guesser_failed_outputs.extend(guesser_result['failed_outputs'])\n",
    "        \n",
    "        if not guesser_result['success']:\n",
    "            error_type = \"FORMAT_FAILURE\" if \"FORMAT_ERROR_EXCEEDED\" in str(guesser_result.get('response', '')) else \"API_FAILURE\"\n",
    "            format_errors.append(f\"ç¬¬{turn}è½® çŒœæµ‹è€…: {guesser_result['error']}\")\n",
    "            \n",
    "            return {\n",
    "                'success': False,\n",
    "                'turns': turn,\n",
    "                'conversation': conversation_history,\n",
    "                'final_guess': f\"GUESSER_{error_type}\",\n",
    "                'error': f\"{error_type}: {guesser_result['error']}\",\n",
    "                'failure_reason': error_type,\n",
    "                'total_hinter_attempts': total_hinter_attempts,\n",
    "                'total_guesser_attempts': total_guesser_attempts,\n",
    "                'format_errors': format_errors,\n",
    "                'hinter_failed_outputs': hinter_failed_outputs,\n",
    "                'guesser_failed_outputs': guesser_failed_outputs,\n",
    "                'all_hints': [msg for msg in conversation_history if msg.startswith('æç¤ºè€…:')],\n",
    "                'all_guesses': [msg for msg in conversation_history if msg.startswith('çŒœæµ‹è€…:')]\n",
    "            }\n",
    "        \n",
    "        conversation_history.append(f\"çŒœæµ‹è€…: {guesser_result['response']}\")\n",
    "        guess = extract_chinese_guess_word(guesser_result['response'])\n",
    "        \n",
    "        # æ£€æŸ¥æ˜¯å¦æˆåŠŸ\n",
    "        if guess == target_word:\n",
    "            return {\n",
    "                'success': True,\n",
    "                'turns': turn,\n",
    "                'conversation': conversation_history,\n",
    "                'final_guess': guess,\n",
    "                'failure_reason': None,\n",
    "                'total_hinter_attempts': total_hinter_attempts,\n",
    "                'total_guesser_attempts': total_guesser_attempts,\n",
    "                'format_errors': format_errors,\n",
    "                'hinter_failed_outputs': hinter_failed_outputs,\n",
    "                'guesser_failed_outputs': guesser_failed_outputs,\n",
    "                'all_hints': [msg for msg in conversation_history if msg.startswith('æç¤ºè€…:')],\n",
    "                'all_guesses': [msg for msg in conversation_history if msg.startswith('çŒœæµ‹è€…:')]\n",
    "            }\n",
    "        \n",
    "        # å¦‚æœä¸æ˜¯æœ€åä¸€è½®ï¼Œæ·»åŠ åé¦ˆ\n",
    "        if turn < max_turns:\n",
    "            conversation_history.append(f\"ç³»ç»Ÿ: '{guess}' ä¸æ­£ç¡®ã€‚è¯·ç»§ç»­ï¼\")\n",
    "    \n",
    "    # è¾¾åˆ°æœ€å¤§è½®æ•°ä»æœªæˆåŠŸ\n",
    "    return {\n",
    "        'success': False,\n",
    "        'turns': max_turns,\n",
    "        'conversation': conversation_history,\n",
    "        'final_guess': guess if 'guess' in locals() else 'N/A',\n",
    "        'failure_reason': 'MAX_TURNS_EXCEEDED',\n",
    "        'total_hinter_attempts': total_hinter_attempts,\n",
    "        'total_guesser_attempts': total_guesser_attempts,\n",
    "        'format_errors': format_errors,\n",
    "        'hinter_failed_outputs': hinter_failed_outputs,\n",
    "        'guesser_failed_outputs': guesser_failed_outputs,\n",
    "        'all_hints': [msg for msg in conversation_history if msg.startswith('æç¤ºè€…:')],\n",
    "        'all_guesses': [msg for msg in conversation_history if msg.startswith('çŒœæµ‹è€…:')]\n",
    "    }\n",
    "\n",
    "print(\"âœ… ä¸­æ–‡Tabooæ¸¸æˆä¸»å‡½æ•°å·²å®šä¹‰\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. æ‰§è¡Œä¸­æ–‡Tabooæµ‹è¯•å®éªŒ\n",
    "print(\"ğŸ§ª å¼€å§‹æ‰§è¡Œä¸­æ–‡Tabooæµ‹è¯•å®éªŒ...\")\n",
    "\n",
    "def run_chinese_test_experiment(client, models, dataset, num_test_words=3):\n",
    "    \"\"\"è¿è¡Œä¸­æ–‡Tabooæµ‹è¯•å®éªŒ\"\"\"\n",
    "    \n",
    "    if not client:\n",
    "        print(\"âŒ APIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ‰§è¡Œå®éªŒ\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nğŸ¯ æµ‹è¯•é…ç½®:\")\n",
    "    print(f\"   æµ‹è¯•è¯æ±‡æ•°: {num_test_words}\")\n",
    "    print(f\"   æ¨¡å‹æ•°é‡: {len(models)}\")\n",
    "    print(f\"   æ€»æ¸¸æˆæ•°: {num_test_words * len(models) * len(models)}\")\n",
    "    \n",
    "    # éšæœºé€‰æ‹©æµ‹è¯•è¯æ±‡\n",
    "    test_words = random.sample(dataset, min(num_test_words, len(dataset)))\n",
    "    print(f\"\\nğŸ“‹ é€‰æ‹©çš„æµ‹è¯•è¯æ±‡:\")\n",
    "    for i, word_data in enumerate(test_words, 1):\n",
    "        print(f\"   {i}. {word_data['target']} ({word_data['part_of_speech']}) - ç¦ç”¨è¯: {word_data['taboo']}\")\n",
    "    \n",
    "    all_results = []\n",
    "    total_games = len(test_words) * len(models) * len(models)\n",
    "    game_counter = 0\n",
    "    \n",
    "    print(f\"\\nğŸš€ å¼€å§‹æ‰§è¡Œå®éªŒ...\")\n",
    "    \n",
    "    for word_data in test_words:\n",
    "        target_word = word_data['target']\n",
    "        taboo_words = word_data['taboo']\n",
    "        \n",
    "        print(f\"\\nğŸ¯ æµ‹è¯•è¯æ±‡: {target_word}\")\n",
    "        print(f\"ğŸš« ç¦ç”¨è¯: {taboo_words}\")\n",
    "        \n",
    "        for hinter_model in models:\n",
    "            for guesser_model in models:\n",
    "                game_counter += 1\n",
    "                hinter_name = hinter_model.split('/')[-1]\n",
    "                guesser_name = guesser_model.split('/')[-1]\n",
    "                pair_name = f\"{hinter_name}â†’{guesser_name}\"\n",
    "                \n",
    "                print(f\"  ğŸ”„ æ¸¸æˆ {game_counter}/{total_games}: {pair_name}\")\n",
    "                \n",
    "                start_time = time.time()\n",
    "                \n",
    "                try:\n",
    "                    # æ‰§è¡Œæ¸¸æˆ\n",
    "                    game_result = play_chinese_taboo_game(\n",
    "                        client, hinter_model, guesser_model, \n",
    "                        target_word, taboo_words, max_turns=5\n",
    "                    )\n",
    "                    \n",
    "                    duration = round(time.time() - start_time, 2)\n",
    "                    \n",
    "                    # è®°å½•ç»“æœ\n",
    "                    result = {\n",
    "                        'game_id': game_counter,\n",
    "                        'target_word': target_word,\n",
    "                        'part_of_speech': word_data['part_of_speech'],\n",
    "                        'category': word_data['category'],\n",
    "                        'taboo_words': '|'.join(taboo_words),\n",
    "                        'hinter_model': hinter_model,\n",
    "                        'guesser_model': guesser_model,\n",
    "                        'success': game_result['success'],\n",
    "                        'turns_used': game_result['turns'],\n",
    "                        'final_guess': game_result['final_guess'],\n",
    "                        'failure_reason': game_result.get('failure_reason', None),\n",
    "                        'taboo_violation_turn': game_result.get('taboo_violation_turn', None),\n",
    "                        'taboo_violation_hint': game_result.get('taboo_violation_hint', None),\n",
    "                        'has_taboo_violation': game_result.get('failure_reason') == 'TABOO_VIOLATION',\n",
    "                        'all_hints': ' | '.join(game_result['all_hints']),\n",
    "                        'all_guesses': ' | '.join(game_result['all_guesses']),\n",
    "                        'conversation': ' | '.join(game_result['conversation']),\n",
    "                        'total_api_attempts': game_result.get('total_hinter_attempts', 0) + game_result.get('total_guesser_attempts', 0),\n",
    "                        'format_errors': ' | '.join(game_result.get('format_errors', [])),\n",
    "                        'has_format_errors': len(game_result.get('format_errors', [])) > 0,\n",
    "                        'duration_seconds': duration,\n",
    "                        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        'language': 'chinese',\n",
    "                        'dataset_source': 'openhownet'\n",
    "                    }\n",
    "                    \n",
    "                    if 'error' in game_result:\n",
    "                        result['error'] = game_result['error']\n",
    "                    \n",
    "                    all_results.append(result)\n",
    "                    \n",
    "                    # æ˜¾ç¤ºç»“æœ\n",
    "                    status = \"âœ… æˆåŠŸ\" if game_result['success'] else \"âŒ å¤±è´¥\"\n",
    "                    failure_info = \"\"\n",
    "                    if not game_result['success'] and game_result.get('failure_reason'):\n",
    "                        failure_reason = game_result['failure_reason']\n",
    "                        if failure_reason == 'TABOO_VIOLATION':\n",
    "                            failure_info = \" (è¿åç¦ç”¨è¯)\"\n",
    "                        elif failure_reason == 'FORMAT_FAILURE':\n",
    "                            failure_info = \" (æ ¼å¼é”™è¯¯)\"\n",
    "                        elif failure_reason == 'API_FAILURE':\n",
    "                            failure_info = \" (APIå¤±è´¥)\"\n",
    "                        elif failure_reason == 'MAX_TURNS_EXCEEDED':\n",
    "                            failure_info = \" (è½®æ•°è€—å°½)\"\n",
    "                    \n",
    "                    print(f\"     {status}{failure_info} | {game_result['turns']}è½® | æœ€ç»ˆçŒœæµ‹: {game_result['final_guess']}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"     âŒ æ¸¸æˆæ‰§è¡Œå¼‚å¸¸: {e}\")\n",
    "                    # è®°å½•å¼‚å¸¸ç»“æœ\n",
    "                    result = {\n",
    "                        'game_id': game_counter,\n",
    "                        'target_word': target_word,\n",
    "                        'hinter_model': hinter_model,\n",
    "                        'guesser_model': guesser_model,\n",
    "                        'success': False,\n",
    "                        'failure_reason': 'EXCEPTION',\n",
    "                        'error': str(e),\n",
    "                        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        'language': 'chinese'\n",
    "                    }\n",
    "                    all_results.append(result)\n",
    "                \n",
    "                time.sleep(0.5)  # APIè°ƒç”¨é—´éš”\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# æ‰§è¡Œæµ‹è¯•å®éªŒ\n",
    "if chinese_client:\n",
    "    test_results = run_chinese_test_experiment(\n",
    "        chinese_client, CHINESE_TEST_MODELS, chinese_dataset, num_test_words=3\n",
    "    )\n",
    "    \n",
    "    if test_results:\n",
    "        print(f\"\\nğŸ‰ ä¸­æ–‡Tabooæµ‹è¯•å®éªŒå®Œæˆï¼\")\n",
    "        print(f\"ğŸ“Š æ€»æ¸¸æˆæ•°: {len(test_results)}\")\n",
    "        \n",
    "        # ç»Ÿè®¡ç»“æœ\n",
    "        successful_games = [r for r in test_results if r['success']]\n",
    "        success_rate = len(successful_games) / len(test_results) * 100\n",
    "        print(f\"ğŸ“ˆ æˆåŠŸç‡: {len(successful_games)}/{len(test_results)} ({success_rate:.1f}%)\")\n",
    "        \n",
    "        # ä¿å­˜æµ‹è¯•ç»“æœ\n",
    "        test_results_path = f\"results/chinese_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        \n",
    "        df_results = pd.DataFrame(test_results)\n",
    "        df_results.to_csv(test_results_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"ğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜: {test_results_path}\")\n",
    "        \n",
    "        # æŒ‰æ¨¡å‹ç»Ÿè®¡\n",
    "        print(f\"\\nğŸ“Š å„æ¨¡å‹è¡¨ç°:\")\n",
    "        for model in CHINESE_TEST_MODELS:\n",
    "            model_name = model.split('/')[-1]\n",
    "            model_as_hinter = [r for r in test_results if r['hinter_model'] == model]\n",
    "            model_as_guesser = [r for r in test_results if r['guesser_model'] == model]\n",
    "            \n",
    "            hinter_success = len([r for r in model_as_hinter if r['success']])\n",
    "            guesser_success = len([r for r in model_as_guesser if r['success']])\n",
    "            \n",
    "            print(f\"   {model_name}:\")\n",
    "            if len(model_as_hinter) > 0:\n",
    "                print(f\"     ä½œä¸ºæç¤ºè€…: {hinter_success}/{len(model_as_hinter)} ({hinter_success/len(model_as_hinter)*100:.1f}%)\")\n",
    "            if len(model_as_guesser) > 0:\n",
    "                print(f\"     ä½œä¸ºçŒœæµ‹è€…: {guesser_success}/{len(model_as_guesser)} ({guesser_success/len(model_as_guesser)*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"âŒ æµ‹è¯•å®éªŒå¤±è´¥\")\n",
    "else:\n",
    "    print(\"âŒ æ— æ³•æ‰§è¡Œæµ‹è¯•å®éªŒï¼šAPIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# ä¸­æ–‡Tabooå®éªŒæ€»ç»“\n",
    "\n",
    "## âœ… å·²å®Œæˆçš„å·¥ä½œ\n",
    "\n",
    "1. **ç¯å¢ƒé…ç½®**: æˆåŠŸå®‰è£…å¹¶é…ç½®äº†OpenHowNetå’Œç›¸å…³ä¸­æ–‡å¤„ç†å·¥å…·\n",
    "2. **æ•°æ®é›†æ„å»º**: ä½¿ç”¨OpenHowNetæ„å»ºäº†100ä¸ªä¸­æ–‡è¯æ±‡çš„Tabooæ•°æ®é›†\n",
    "3. **è¯æ€§åˆ†å¸ƒ**: æŒ‰ç…§åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ã€å‰¯è¯å„25ä¸ªçš„ç›®æ ‡è¿›è¡Œåˆ†å¸ƒ\n",
    "4. **æ¸¸æˆé€»è¾‘**: å®ç°äº†å®Œæ•´çš„ä¸­æ–‡Tabooæ¸¸æˆé€»è¾‘å’Œè¯„ä¼°æ¡†æ¶\n",
    "5. **æ¨¡å‹æµ‹è¯•**: éªŒè¯äº†ä¸­æ–‡æ ¼å¼è¦æ±‚å’Œç¦ç”¨è¯æ£€æµ‹æœºåˆ¶\n",
    "6. **æ•°æ®ä¿å­˜**: ç”Ÿæˆäº†å®Œæ•´çš„æ•°æ®é›†æ–‡ä»¶å’Œå®éªŒæŠ¥å‘Š\n",
    "\n",
    "## ğŸ¯ æ•°æ®é›†ç‰¹ç‚¹\n",
    "\n",
    "- **åŸºäºOpenHowNet**: åˆ©ç”¨ä¸­æ–‡çŸ¥è¯†å›¾è°±çš„è¯­ä¹‰å…³ç³»\n",
    "- **ç¦ç”¨è¯ç”Ÿæˆ**: æ¯ä¸ªè¯æ±‡åŒ…å«5ä¸ªè¯­ä¹‰ç›¸å…³çš„ç¦ç”¨è¯\n",
    "- **è¯æ€§è¦†ç›–**: æ¶µç›–4ç§ä¸»è¦è¯æ€§ï¼Œå¹³è¡¡åˆ†å¸ƒ\n",
    "- **è¯­ä¹‰ä¸°å¯Œ**: åŒ…å«å®Œæ•´çš„ä¹‰é¡¹ä¿¡æ¯å’Œè¯­ä¹‰å®šä¹‰\n",
    "- **ä¸­æ–‡ä¼˜åŒ–**: ä¸“é—¨é’ˆå¯¹ä¸­æ–‡è¯­è¨€ç‰¹ç‚¹è¿›è¡Œä¼˜åŒ–\n",
    "\n",
    "## ğŸ¤– æŠ€æœ¯åˆ›æ–°\n",
    "\n",
    "- **é¦–æ¬¡åº”ç”¨**: å°†OpenHowNetç”¨äºTabooæ¸¸æˆæ•°æ®é›†æ„å»º\n",
    "- **ä¸­æ–‡é€‚é…**: å®ç°äº†ä¸­æ–‡ç‰¹å®šçš„æ ¼å¼æ£€æŸ¥å’Œè¿è§„æ£€æµ‹\n",
    "- **è¯„ä¼°æ¡†æ¶**: æä¾›äº†å®Œæ•´çš„ä¸­æ–‡LLMè¯„ä¼°ä½“ç³»\n",
    "- **å¤šæ¨¡å‹æ”¯æŒ**: æ”¯æŒGPT-4oã€Geminiã€DeepSeekç­‰å¤šç§æ¨¡å‹\n",
    "\n",
    "## ğŸ“ ç”Ÿæˆæ–‡ä»¶\n",
    "\n",
    "- `data/chinese_dataset.json` - å®Œæ•´æ•°æ®é›†\n",
    "- `data/chinese_dataset_simple.json` - ç®€åŒ–ç‰ˆæ•°æ®é›†  \n",
    "- `data/chinese_dataset_report.json` - æ•°æ®é›†æŠ¥å‘Š\n",
    "- `results/chinese_test_results_*.csv` - å®éªŒç»“æœ\n",
    "\n",
    "## ğŸ”® æ‰©å±•æ–¹å‘\n",
    "\n",
    "1. **è§„æ¨¡æ‰©å±•**: å¢åŠ æ•°æ®é›†åˆ°300-500ä¸ªè¯æ±‡\n",
    "2. **é¢†åŸŸæ‹“å±•**: æ·»åŠ ä¸“ä¸šé¢†åŸŸè¯æ±‡ï¼ˆåŒ»å­¦ã€æ³•å¾‹ã€ç§‘æŠ€ç­‰ï¼‰\n",
    "3. **æ¨¡å‹è¦†ç›–**: æµ‹è¯•æ›´å¤šä¸­æ–‡æ¨¡å‹ï¼ˆæ™ºè°±ã€ç™¾å·ã€æ–‡å¿ƒç­‰ï¼‰\n",
    "4. **å¯¹æ¯”ç ”ç©¶**: å®ç°ä¸­è‹±æ–‡Tabooæ¸¸æˆå¯¹æ¯”åˆ†æ\n",
    "5. **éš¾åº¦åˆ†çº§**: ç ”ç©¶ä¸åŒå¤æ‚åº¦è¯æ±‡å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“\n",
    "\n",
    "## ğŸ‰ å®éªŒæ„ä¹‰\n",
    "\n",
    "è¿™æ˜¯é¦–ä¸ªåŸºäºOpenHowNetçš„ä¸­æ–‡Tabooæ¸¸æˆå®éªŒç³»ç»Ÿï¼Œä¸ºä¸­æ–‡è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰ç†è§£èƒ½åŠ›è¯„ä¼°æä¾›äº†æ–°çš„åŸºå‡†æµ‹è¯•å·¥å…·ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»“æœåˆ†æä¸å¯è§†åŒ–\n",
    "print(\"ğŸ“Š ä¸­æ–‡Tabooå®éªŒç»“æœæ·±åº¦åˆ†æ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "def load_experiment_results():\n",
    "    \"\"\"åŠ è½½æ‰€æœ‰å®éªŒç»“æœæ–‡ä»¶\"\"\"\n",
    "    results_files = glob.glob(\"results/**/chinese_*.csv\", recursive=True)\n",
    "    results_files.extend(glob.glob(\"results/**/*chinese*.csv\", recursive=True))\n",
    "    \n",
    "    if not results_files:\n",
    "        print(\"âš ï¸ æœªæ‰¾åˆ°å®éªŒç»“æœæ–‡ä»¶\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"ğŸ” æ‰¾åˆ° {len(results_files)} ä¸ªç»“æœæ–‡ä»¶:\")\n",
    "    for file in results_files:\n",
    "        print(f\"   - {file}\")\n",
    "    \n",
    "    # åŠ è½½æœ€æ–°çš„ç»“æœæ–‡ä»¶\n",
    "    latest_file = max(results_files, key=lambda x: x.split('_')[-1] if '_' in x else x)\n",
    "    print(f\"\\nğŸ“‚ åŠ è½½æœ€æ–°ç»“æœæ–‡ä»¶: {latest_file}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(latest_file, encoding='utf-8-sig')\n",
    "        print(f\"âœ… æˆåŠŸåŠ è½½ {len(df)} æ¡è®°å½•\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ åŠ è½½å¤±è´¥: {e}\")\n",
    "        return None\n",
    "\n",
    "def analyze_overall_performance(df):\n",
    "    \"\"\"æ•´ä½“æ€§èƒ½åˆ†æ\"\"\"\n",
    "    print(\"\\nğŸ¯ æ•´ä½“æ€§èƒ½åˆ†æ\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    total_games = len(df)\n",
    "    successful_games = len(df[df['success'] == True])\n",
    "    success_rate = successful_games / total_games * 100 if total_games > 0 else 0\n",
    "    \n",
    "    print(f\"æ€»æ¸¸æˆæ•°: {total_games}\")\n",
    "    print(f\"æˆåŠŸæ¸¸æˆæ•°: {successful_games}\")\n",
    "    print(f\"æ•´ä½“æˆåŠŸç‡: {success_rate:.1f}%\")\n",
    "    \n",
    "    # å¤±è´¥åŸå› åˆ†æ\n",
    "    failed_games = df[df['success'] == False]\n",
    "    if len(failed_games) > 0:\n",
    "        failure_reasons = failed_games['failure_reason'].value_counts()\n",
    "        print(f\"\\nâŒ å¤±è´¥åŸå› åˆ†å¸ƒ:\")\n",
    "        for reason, count in failure_reasons.items():\n",
    "            percentage = count / len(failed_games) * 100\n",
    "            print(f\"   {reason}: {count} æ¬¡ ({percentage:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'total_games': total_games,\n",
    "        'successful_games': successful_games,\n",
    "        'success_rate': success_rate,\n",
    "        'failure_reasons': failure_reasons if len(failed_games) > 0 else None\n",
    "    }\n",
    "\n",
    "def analyze_model_performance(df):\n",
    "    \"\"\"æ¨¡å‹æ€§èƒ½åˆ†æ\"\"\"\n",
    "    print(\"\\nğŸ¤– æ¨¡å‹æ€§èƒ½åˆ†æ\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    models = list(set(df['hinter_model'].unique()) | set(df['guesser_model'].unique()))\n",
    "    model_stats = {}\n",
    "    \n",
    "    for model in models:\n",
    "        model_name = model.split('/')[-1] if '/' in model else model\n",
    "        \n",
    "        # ä½œä¸ºæç¤ºè€…çš„è¡¨ç°\n",
    "        as_hinter = df[df['hinter_model'] == model]\n",
    "        hinter_success = len(as_hinter[as_hinter['success'] == True])\n",
    "        hinter_total = len(as_hinter)\n",
    "        hinter_rate = hinter_success / hinter_total * 100 if hinter_total > 0 else 0\n",
    "        \n",
    "        # ä½œä¸ºçŒœæµ‹è€…çš„è¡¨ç°\n",
    "        as_guesser = df[df['guesser_model'] == model]\n",
    "        guesser_success = len(as_guesser[as_guesser['success'] == True])\n",
    "        guesser_total = len(as_guesser)\n",
    "        guesser_rate = guesser_success / guesser_total * 100 if guesser_total > 0 else 0\n",
    "        \n",
    "        model_stats[model_name] = {\n",
    "            'hinter': {'success': hinter_success, 'total': hinter_total, 'rate': hinter_rate},\n",
    "            'guesser': {'success': guesser_success, 'total': guesser_total, 'rate': guesser_rate}\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  ä½œä¸ºæç¤ºè€…: {hinter_success}/{hinter_total} ({hinter_rate:.1f}%)\")\n",
    "        print(f\"  ä½œä¸ºçŒœæµ‹è€…: {guesser_success}/{guesser_total} ({guesser_rate:.1f}%)\")\n",
    "    \n",
    "    return model_stats\n",
    "\n",
    "def analyze_pos_performance(df):\n",
    "    \"\"\"è¯æ€§æ€§èƒ½åˆ†æ\"\"\"\n",
    "    print(\"\\nğŸ“ è¯æ€§æ€§èƒ½åˆ†æ\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if 'part_of_speech' not in df.columns:\n",
    "        print(\"âš ï¸ æ•°æ®ä¸­æœªæ‰¾åˆ°è¯æ€§ä¿¡æ¯\")\n",
    "        return None\n",
    "    \n",
    "    pos_stats = {}\n",
    "    pos_groups = df.groupby('part_of_speech')\n",
    "    \n",
    "    for pos, group in pos_groups:\n",
    "        total = len(group)\n",
    "        success = len(group[group['success'] == True])\n",
    "        rate = success / total * 100 if total > 0 else 0\n",
    "        \n",
    "        pos_stats[pos] = {'total': total, 'success': success, 'rate': rate}\n",
    "        print(f\"{pos}: {success}/{total} ({rate:.1f}%)\")\n",
    "    \n",
    "    return pos_stats\n",
    "\n",
    "def create_visualizations(df, model_stats, pos_stats, overall_stats):\n",
    "    \"\"\"åˆ›å»ºå¯è§†åŒ–å›¾è¡¨\"\"\"\n",
    "    print(\"\\nğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # åˆ›å»ºå­å›¾\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('ä¸­æ–‡Tabooå®éªŒç»“æœåˆ†æ', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. æ•´ä½“æˆåŠŸç‡é¥¼å›¾\n",
    "    ax1 = axes[0, 0]\n",
    "    success_data = [overall_stats['successful_games'], \n",
    "                   overall_stats['total_games'] - overall_stats['successful_games']]\n",
    "    labels = [f'æˆåŠŸ ({overall_stats[\"success_rate\"]:.1f}%)', \n",
    "              f'å¤±è´¥ ({100-overall_stats[\"success_rate\"]:.1f}%)']\n",
    "    colors = ['#2E8B57', '#DC143C']\n",
    "    \n",
    "    ax1.pie(success_data, labels=labels, colors=colors, autopct='%d', startangle=90)\n",
    "    ax1.set_title('æ•´ä½“æˆåŠŸç‡åˆ†å¸ƒ')\n",
    "    \n",
    "    # 2. æ¨¡å‹æ€§èƒ½å¯¹æ¯”\n",
    "    ax2 = axes[0, 1]\n",
    "    if model_stats:\n",
    "        model_names = list(model_stats.keys())\n",
    "        hinter_rates = [stats['hinter']['rate'] for stats in model_stats.values()]\n",
    "        guesser_rates = [stats['guesser']['rate'] for stats in model_stats.values()]\n",
    "        \n",
    "        x = np.arange(len(model_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax2.bar(x - width/2, hinter_rates, width, label='ä½œä¸ºæç¤ºè€…', color='skyblue')\n",
    "        bars2 = ax2.bar(x + width/2, guesser_rates, width, label='ä½œä¸ºçŒœæµ‹è€…', color='lightcoral')\n",
    "        \n",
    "        ax2.set_xlabel('æ¨¡å‹')\n",
    "        ax2.set_ylabel('æˆåŠŸç‡ (%)')\n",
    "        ax2.set_title('å„æ¨¡å‹æ€§èƒ½å¯¹æ¯”')\n",
    "        ax2.set_xticks(x)\n",
    "        ax2.set_xticklabels([name[:10] for name in model_names], rotation=45)\n",
    "        ax2.legend()\n",
    "        ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 3. è¯æ€§æ€§èƒ½åˆ†æ\n",
    "    ax3 = axes[1, 0]\n",
    "    if pos_stats:\n",
    "        pos_names = list(pos_stats.keys())\n",
    "        pos_rates = [stats['rate'] for stats in pos_stats.values()]\n",
    "        pos_totals = [stats['total'] for stats in pos_stats.values()]\n",
    "        \n",
    "        bars = ax3.bar(pos_names, pos_rates, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
    "        ax3.set_ylabel('æˆåŠŸç‡ (%)')\n",
    "        ax3.set_title('å„è¯æ€§è¡¨ç°')\n",
    "        ax3.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°æ®æ ‡ç­¾\n",
    "        for bar, total in zip(bars, pos_totals):\n",
    "            height = bar.get_height()\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                    f'{height:.1f}%\\n(n={total})', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # 4. å¤±è´¥åŸå› åˆ†æ\n",
    "    ax4 = axes[1, 1]\n",
    "    if overall_stats['failure_reasons'] is not None and len(overall_stats['failure_reasons']) > 0:\n",
    "        failure_reasons = overall_stats['failure_reasons']\n",
    "        reasons = list(failure_reasons.keys())\n",
    "        counts = list(failure_reasons.values())\n",
    "        \n",
    "        # ç®€åŒ–å¤±è´¥åŸå› åç§°\n",
    "        reason_mapping = {\n",
    "            'TABOO_VIOLATION': 'ç¦ç”¨è¯è¿è§„',\n",
    "            'MAX_TURNS_EXCEEDED': 'è½®æ•°è€—å°½',\n",
    "            'FORMAT_FAILURE': 'æ ¼å¼é”™è¯¯',\n",
    "            'API_FAILURE': 'APIå¤±è´¥',\n",
    "            'EXCEPTION': 'å¼‚å¸¸é”™è¯¯'\n",
    "        }\n",
    "        \n",
    "        simplified_reasons = [reason_mapping.get(r, r) for r in reasons]\n",
    "        \n",
    "        wedges, texts, autotexts = ax4.pie(counts, labels=simplified_reasons, autopct='%1.1f%%', startangle=90)\n",
    "        ax4.set_title('å¤±è´¥åŸå› åˆ†å¸ƒ')\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'æ— å¤±è´¥æ•°æ®', ha='center', va='center', transform=ax4.transAxes)\n",
    "        ax4.set_title('å¤±è´¥åŸå› åˆ†å¸ƒ')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # ä¿å­˜å›¾è¡¨\n",
    "    plot_filename = f\"results/chinese_experiment_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\"\n",
    "    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "    print(f\"ğŸ“Š å›¾è¡¨å·²ä¿å­˜: {plot_filename}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def generate_detailed_report(df, model_stats, pos_stats, overall_stats):\n",
    "    \"\"\"ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š\"\"\"\n",
    "    print(\"\\nğŸ“‹ ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    report = {\n",
    "        'experiment_summary': {\n",
    "            'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'total_games': overall_stats['total_games'],\n",
    "            'success_rate': overall_stats['success_rate'],\n",
    "            'dataset_size': len(df['target_word'].unique()) if 'target_word' in df.columns else 0\n",
    "        },\n",
    "        'overall_performance': overall_stats,\n",
    "        'model_performance': model_stats,\n",
    "        'pos_performance': pos_stats\n",
    "    }\n",
    "    \n",
    "    # æ·»åŠ è¯æ±‡çº§åˆ«åˆ†æ\n",
    "    if 'target_word' in df.columns:\n",
    "        word_analysis = {}\n",
    "        for word in df['target_word'].unique():\n",
    "            word_games = df[df['target_word'] == word]\n",
    "            word_success = len(word_games[word_games['success'] == True])\n",
    "            word_total = len(word_games)\n",
    "            word_rate = word_success / word_total * 100 if word_total > 0 else 0\n",
    "            \n",
    "            word_analysis[word] = {\n",
    "                'success_rate': word_rate,\n",
    "                'total_games': word_total,\n",
    "                'success_count': word_success\n",
    "            }\n",
    "        \n",
    "        # æ‰¾å‡ºæœ€éš¾å’Œæœ€å®¹æ˜“çš„è¯æ±‡\n",
    "        sorted_words = sorted(word_analysis.items(), key=lambda x: x[1]['success_rate'])\n",
    "        \n",
    "        report['word_analysis'] = {\n",
    "            'most_difficult': sorted_words[:5],  # æœ€éš¾çš„5ä¸ªè¯\n",
    "            'easiest': sorted_words[-5:],  # æœ€å®¹æ˜“çš„5ä¸ªè¯\n",
    "            'all_words': word_analysis\n",
    "        }\n",
    "        \n",
    "        print(f\"æœ€éš¾è¯æ±‡ (æˆåŠŸç‡æœ€ä½):\")\n",
    "        for word, stats in sorted_words[:5]:\n",
    "            print(f\"  {word}: {stats['success_rate']:.1f}% ({stats['success_count']}/{stats['total_games']})\")\n",
    "        \n",
    "        print(f\"\\næœ€å®¹æ˜“è¯æ±‡ (æˆåŠŸç‡æœ€é«˜):\")\n",
    "        for word, stats in sorted_words[-5:]:\n",
    "            print(f\"  {word}: {stats['success_rate']:.1f}% ({stats['success_count']}/{stats['total_games']})\")\n",
    "    \n",
    "    # ä¿å­˜æŠ¥å‘Š\n",
    "    report_filename = f\"results/chinese_experiment_detailed_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_filename}\")\n",
    "    return report\n",
    "\n",
    "# ä¸»è¦åˆ†ææµç¨‹\n",
    "print(\"ğŸš€ å¼€å§‹ç»“æœåˆ†æ...\")\n",
    "\n",
    "# 1. åŠ è½½æ•°æ®\n",
    "df_results = load_experiment_results()\n",
    "\n",
    "if df_results is not None:\n",
    "    print(f\"\\nğŸ“Š æ•°æ®æ¦‚è§ˆ:\")\n",
    "    print(f\"   æ•°æ®ç»´åº¦: {df_results.shape}\")\n",
    "    print(f\"   åˆ—å: {list(df_results.columns)}\")\n",
    "    \n",
    "    # 2. æ•´ä½“æ€§èƒ½åˆ†æ\n",
    "    overall_performance = analyze_overall_performance(df_results)\n",
    "    \n",
    "    # 3. æ¨¡å‹æ€§èƒ½åˆ†æ\n",
    "    model_performance = analyze_model_performance(df_results)\n",
    "    \n",
    "    # 4. è¯æ€§æ€§èƒ½åˆ†æ\n",
    "    pos_performance = analyze_pos_performance(df_results)\n",
    "    \n",
    "    # 5. åˆ›å»ºå¯è§†åŒ–\n",
    "    create_visualizations(df_results, model_performance, pos_performance, overall_performance)\n",
    "    \n",
    "    # 6. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š\n",
    "    detailed_report = generate_detailed_report(df_results, model_performance, pos_performance, overall_performance)\n",
    "    \n",
    "    print(f\"\\nğŸ‰ ç»“æœåˆ†æå®Œæˆï¼\")\n",
    "    print(f\"ğŸ“ˆ å…³é”®æŒ‡æ ‡:\")\n",
    "    print(f\"   æ•´ä½“æˆåŠŸç‡: {overall_performance['success_rate']:.1f}%\")\n",
    "    print(f\"   æµ‹è¯•æ¨¡å‹æ•°: {len(model_performance)}\")\n",
    "    print(f\"   è¯æ€§è¦†ç›–: {len(pos_performance) if pos_performance else 0}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ æ— æ³•è¿›è¡Œç»“æœåˆ†æï¼šæœªæ‰¾åˆ°æœ‰æ•ˆçš„å®éªŒç»“æœæ–‡ä»¶\")\n",
    "    print(\"ğŸ’¡ è¯·å…ˆè¿è¡Œå®éªŒç”Ÿæˆç»“æœæ–‡ä»¶\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“‹ åˆ†æå®Œæˆ - æ‰€æœ‰å›¾è¡¨å’ŒæŠ¥å‘Šå·²ä¿å­˜åˆ° results/ ç›®å½•\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
