{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# 中文Taboo实验 - 基于OpenHowNet的数据集构建与测试\n",
    "# 仿照base_test.ipynb结构，专门针对中文词汇和语言模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: OpenHowNet in /opt/homebrew/lib/python3.11/site-packages (2.0)\n",
      "Requirement already satisfied: jieba in /opt/homebrew/lib/python3.11/site-packages (0.42.1)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (2.3.0)\n",
      "Requirement already satisfied: anytree in /opt/homebrew/lib/python3.11/site-packages (from OpenHowNet) (2.13.0)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/lib/python3.11/site-packages (from OpenHowNet) (78.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (from OpenHowNet) (4.66.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install OpenHowNet jieba requests pandas numpy\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "!pip3 install OpenHowNet\n",
    "!pip3 install jieba\n",
    "!pip3 install requests\n",
    "!pip3 install pandas\n",
    "!pip3 install numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OpenHowNet已导入\n",
      "🚀 中文Taboo实验环境初始化完成\n",
      "📋 实验目标: 使用OpenHowNet构建100个中文词汇的Taboo数据集\n",
      "🎯 词性分布: 名词、动词、形容词、副词各25个\n"
     ]
    }
   ],
   "source": [
    "# 1. 导入依赖和设置环境\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "import jieba\n",
    "import re\n",
    "from typing import Dict, List, Any, Tuple\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# 安装和导入OpenHowNet\n",
    "try:\n",
    "    import OpenHowNet\n",
    "    print(\"✅ OpenHowNet已导入\")\n",
    "except ImportError:\n",
    "    print(\"⚠️ 正在安装OpenHowNet...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"OpenHowNet\"])\n",
    "    import OpenHowNet\n",
    "    print(\"✅ OpenHowNet安装并导入成功\")\n",
    "\n",
    "print(\"🚀 中文Taboo实验环境初始化完成\")\n",
    "print(\"📋 实验目标: 使用OpenHowNet构建100个中文词汇的Taboo数据集\")\n",
    "print(\"🎯 词性分布: 名词、动词、形容词、副词各25个\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 深入测试OpenHowNet API结构...\n",
      "\n",
      "📝 测试词汇: 计算机\n",
      "   义项数量: 1\n",
      "   第一个义项类型: <class 'OpenHowNet.Sense.Sense'>\n",
      "   所有属性: ['No', 'en_word', 'en_grammar', 'zh_word', 'zh_grammar', 'Def', 'sememes']\n",
      "   中文词: 计算机\n",
      "   中文词性(zh_grammar): noun\n",
      "\n",
      "📝 测试词汇: 学习\n",
      "   义项数量: 5\n",
      "   第一个义项类型: <class 'OpenHowNet.Sense.Sense'>\n",
      "   所有属性: ['No', 'en_word', 'en_grammar', 'zh_word', 'zh_grammar', 'Def', 'sememes']\n",
      "   中文词: 学习\n",
      "   中文词性(zh_grammar): verb\n",
      "\n",
      "📝 测试词汇: 美丽\n",
      "   义项数量: 3\n",
      "   第一个义项类型: <class 'OpenHowNet.Sense.Sense'>\n",
      "   所有属性: ['No', 'en_word', 'en_grammar', 'zh_word', 'zh_grammar', 'Def', 'sememes']\n",
      "   中文词: 美丽\n",
      "   中文词性(zh_grammar): adj\n",
      "\n",
      "📝 测试词汇: 快速\n",
      "   义项数量: 4\n",
      "   第一个义项类型: <class 'OpenHowNet.Sense.Sense'>\n",
      "   所有属性: ['No', 'en_word', 'en_grammar', 'zh_word', 'zh_grammar', 'Def', 'sememes']\n",
      "   中文词: 快速\n",
      "   中文词性(zh_grammar): adj\n",
      "\n",
      "📊 词汇获取方法测试:\n",
      "   get_zh_words() 返回类型: <class 'list'>\n",
      "   词汇数量: 135009\n",
      "   前10个词汇: ['', '深圳乐家精品服务公寓', '临床表现为', '休息', '打小算盘', '湖底', '赏格', '超短裙', '襑', '岐']\n"
     ]
    }
   ],
   "source": [
    "# 深入测试OpenHowNet API\n",
    "if hownet_dict:\n",
    "    print(\"🔍 深入测试OpenHowNet API结构...\")\n",
    "    \n",
    "    # 测试词汇\n",
    "    test_words = [\"计算机\", \"学习\", \"美丽\", \"快速\"]\n",
    "    \n",
    "    for test_word in test_words:\n",
    "        print(f\"\\n📝 测试词汇: {test_word}\")\n",
    "        try:\n",
    "            senses = hownet_dict.get_sense(test_word)\n",
    "            if senses:\n",
    "                print(f\"   义项数量: {len(senses)}\")\n",
    "                sense = senses[0]\n",
    "                print(f\"   第一个义项类型: {type(sense)}\")\n",
    "                \n",
    "                # 检查所有可能的属性\n",
    "                if hasattr(sense, '__dict__'):\n",
    "                    attrs = list(sense.__dict__.keys())\n",
    "                    print(f\"   所有属性: {attrs}\")\n",
    "                else:\n",
    "                    # 尝试常见属性\n",
    "                    common_attrs = ['zh_word', 'en_word', 'pos', 'zh_grammar', 'definition', 'Def', 'def']\n",
    "                    available_attrs = []\n",
    "                    for attr in common_attrs:\n",
    "                        if hasattr(sense, attr):\n",
    "                            value = getattr(sense, attr)\n",
    "                            available_attrs.append(f\"{attr}={value}\")\n",
    "                    print(f\"   可用属性: {available_attrs}\")\n",
    "                \n",
    "                # 尝试调用一些方法\n",
    "                try:\n",
    "                    if hasattr(sense, 'zh_word'):\n",
    "                        print(f\"   中文词: {sense.zh_word}\")\n",
    "                    if hasattr(sense, 'pos'):\n",
    "                        print(f\"   词性(pos): {sense.pos}\")\n",
    "                    if hasattr(sense, 'zh_grammar'):\n",
    "                        print(f\"   中文词性(zh_grammar): {sense.zh_grammar}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   属性访问错误: {e}\")\n",
    "            else:\n",
    "                print(f\"   未找到义项\")\n",
    "        except Exception as e:\n",
    "            print(f\"   测试错误: {e}\")\n",
    "    \n",
    "    # 测试词汇列表方法\n",
    "    print(f\"\\n📊 词汇获取方法测试:\")\n",
    "    try:\n",
    "        zh_words = hownet_dict.get_zh_words()\n",
    "        print(f\"   get_zh_words() 返回类型: {type(zh_words)}\")\n",
    "        print(f\"   词汇数量: {len(zh_words)}\")\n",
    "        print(f\"   前10个词汇: {list(zh_words)[:10]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   get_zh_words() 错误: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ hownet_dict 为 None，跳过测试\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 正在初始化OpenHowNet和中文处理工具...\n",
      "📥 正在下载OpenHowNet数据...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "resources/resources.zip: 72948KB [00:06, 10870.59KB/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OpenHowNet数据下载完成\n",
      "Initializing OpenHowNet succeeded!\n",
      "✅ OpenHowNet词典加载成功\n",
      "📚 词典包含中文词汇数量: 135009 个\n",
      "✅ API测试成功，'计算机'有 1 个义项\n",
      "📋 第一个义项数据结构:\n",
      "   类型: <class 'OpenHowNet.Sense.Sense'>\n",
      "   内容: No.255809|computer|计算机\n",
      "   属性: ['No', 'en_word', 'en_grammar', 'zh_word', 'zh_grammar', 'Def', 'sememes']\n",
      "✅ jieba分词工具已配置\n",
      "🎲 随机种子已设置为42，确保实验可复现\n"
     ]
    }
   ],
   "source": [
    "# 2. 初始化OpenHowNet和中文处理工具\n",
    "print(\"🔧 正在初始化OpenHowNet和中文处理工具...\")\n",
    "\n",
    "# 初始化OpenHowNet实例\n",
    "try:\n",
    "    # 首先尝试下载数据\n",
    "    print(\"📥 正在下载OpenHowNet数据...\")\n",
    "    OpenHowNet.download()\n",
    "    print(\"✅ OpenHowNet数据下载完成\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ 下载过程中出现警告: {e}\")\n",
    "\n",
    "try:\n",
    "    hownet_dict = OpenHowNet.HowNetDict()\n",
    "    print(\"✅ OpenHowNet词典加载成功\")\n",
    "    \n",
    "    # 测试API方法\n",
    "    zh_words = hownet_dict.get_zh_words()\n",
    "    print(f\"📚 词典包含中文词汇数量: {len(zh_words)} 个\")\n",
    "    \n",
    "    # 测试get_sense方法\n",
    "    test_sense = hownet_dict.get_sense(\"计算机\")\n",
    "    if test_sense:\n",
    "        print(f\"✅ API测试成功，'计算机'有 {len(test_sense)} 个义项\")\n",
    "        print(f\"📋 第一个义项数据结构:\")\n",
    "        print(f\"   类型: {type(test_sense[0])}\")\n",
    "        print(f\"   内容: {test_sense[0]}\")\n",
    "        if hasattr(test_sense[0], '__dict__'):\n",
    "            print(f\"   属性: {list(test_sense[0].__dict__.keys())}\")\n",
    "    else:\n",
    "        print(\"⚠️ 测试词汇'计算机'未找到义项\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ OpenHowNet初始化失败: {e}\")\n",
    "    print(\"🔄 尝试重新初始化...\")\n",
    "    hownet_dict = None\n",
    "\n",
    "# 设置jieba分词\n",
    "jieba.setLogLevel(20)  # 减少jieba的日志输出\n",
    "print(\"✅ jieba分词工具已配置\")\n",
    "\n",
    "# 设置随机种子\n",
    "random.seed(42)\n",
    "print(\"🎲 随机种子已设置为42，确保实验可复现\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 检查 HowNet 词典状态...\n",
      "✅ hownet_dict 已存在且有效\n",
      "🎯 最终状态: hownet_dict = <class 'OpenHowNet.HowNetDict.HowNetDict'>\n"
     ]
    }
   ],
   "source": [
    "# 检查并重新初始化 HowNet（如果需要）\n",
    "print(\"🔍 检查 HowNet 词典状态...\")\n",
    "\n",
    "try:\n",
    "    # 检查 hownet_dict 是否已定义且有效\n",
    "    if 'hownet_dict' not in globals():\n",
    "        print(\"⚠️ hownet_dict 未定义，正在初始化...\")\n",
    "        hownet_dict = None\n",
    "    elif hownet_dict is None:\n",
    "        print(\"⚠️ hownet_dict 为 None，正在重新初始化...\")\n",
    "    else:\n",
    "        print(\"✅ hownet_dict 已存在且有效\")\n",
    "        \n",
    "    # 如果需要，重新初始化\n",
    "    if hownet_dict is None:\n",
    "        try:\n",
    "            import OpenHowNet\n",
    "            print(\"📥 正在初始化 OpenHowNet...\")\n",
    "            hownet_dict = OpenHowNet.HowNetDict()\n",
    "            print(\"✅ OpenHowNet 词典重新初始化成功\")\n",
    "            \n",
    "            # 测试功能\n",
    "            test_words = hownet_dict.get_zh_words()\n",
    "            print(f\"📚 词典包含 {len(test_words)} 个中文词汇\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ OpenHowNet 初始化失败: {e}\")\n",
    "            hownet_dict = None\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"❌ 检查过程出错: {e}\")\n",
    "    hownet_dict = None\n",
    "\n",
    "print(f\"🎯 最终状态: hownet_dict = {type(hownet_dict) if hownet_dict else 'None'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 中文词汇处理工具函数已定义\n"
     ]
    }
   ],
   "source": [
    "# 3. 中文词汇数据集构建工具函数\n",
    "\n",
    "def get_pos_mapping():\n",
    "    \"\"\"HowNet词性到标准词性的映射\"\"\"\n",
    "    return {\n",
    "        # 名词类\n",
    "        'N': 'noun', 'noun': 'noun',\n",
    "        # 动词类  \n",
    "        'V': 'verb', 'verb': 'verb',\n",
    "        # 形容词类\n",
    "        'A': 'adj', 'adj': 'adj', 'a': 'adj',\n",
    "        # 副词类\n",
    "        'D': 'adv', 'adv': 'adv', 'd': 'adv'\n",
    "    }\n",
    "\n",
    "def is_valid_chinese_word(word: str) -> bool:\n",
    "    \"\"\"检查是否为有效的中文词汇\"\"\"\n",
    "    if not word or len(word) < 1:\n",
    "        return False\n",
    "    \n",
    "    # 检查是否包含中文字符\n",
    "    chinese_pattern = re.compile(r'[\\u4e00-\\u9fff]+')\n",
    "    if not chinese_pattern.search(word):\n",
    "        return False\n",
    "    \n",
    "    # 过滤过长或过短的词\n",
    "    if len(word) > 6 or len(word) < 1:\n",
    "        return False\n",
    "    \n",
    "    # 过滤包含特殊字符的词\n",
    "    special_chars = ['·', '—', '…', '〈', '〉', '《', '》', '「', '」']\n",
    "    if any(char in word for char in special_chars):\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def extract_similar_words_from_hownet(target_word: str, target_pos: str, hownet_dict, max_count: int = 10) -> List[str]:\n",
    "    \"\"\"从HowNet中提取与目标词相似的词汇作为禁用词候选\"\"\"\n",
    "    similar_words = set()\n",
    "    \n",
    "    try:\n",
    "        # 获取目标词的义项\n",
    "        word_senses = hownet_dict.get_sense(target_word)\n",
    "        if not word_senses:\n",
    "            return []\n",
    "        \n",
    "        # 从第一个义项开始提取相似词\n",
    "        primary_sense = word_senses[0]\n",
    "        \n",
    "        # 方法1: 从义项中提取相关词汇\n",
    "        try:\n",
    "            # 获取所有包含该词的相关义项信息\n",
    "            for sense in word_senses[:3]:  # 取前3个义项\n",
    "                # 提取中文词 - 使用属性而不是字典访问\n",
    "                if hasattr(sense, 'zh_word') and sense.zh_word and is_valid_chinese_word(sense.zh_word) and sense.zh_word != target_word:\n",
    "                    similar_words.add(sense.zh_word)\n",
    "                    \n",
    "                # 尝试获取同义词 - 不同的属性名称\n",
    "                syn_attrs = ['syn', 'synonyms', 'similar_words']\n",
    "                for syn_attr in syn_attrs:\n",
    "                    if hasattr(sense, syn_attr):\n",
    "                        syn_data = getattr(sense, syn_attr)\n",
    "                        if syn_data:\n",
    "                            # 处理不同的同义词数据格式\n",
    "                            if isinstance(syn_data, list):\n",
    "                                for syn_item in syn_data[:5]:\n",
    "                                    if isinstance(syn_item, str):\n",
    "                                        syn_word = syn_item\n",
    "                                    elif hasattr(syn_item, 'text'):\n",
    "                                        syn_word = syn_item.text\n",
    "                                    elif hasattr(syn_item, 'word'):\n",
    "                                        syn_word = syn_item.word\n",
    "                                    else:\n",
    "                                        continue\n",
    "                                    \n",
    "                                    if is_valid_chinese_word(syn_word) and syn_word != target_word:\n",
    "                                        similar_words.add(syn_word)\n",
    "                        break\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 提取同义词时出错: {e}\")\n",
    "            pass\n",
    "        \n",
    "        # 方法2: 从义原定义中提取关键词\n",
    "        try:\n",
    "            for sense in word_senses[:2]:  # 取前2个义项\n",
    "                # 尝试不同的定义属性名称\n",
    "                definition = \"\"\n",
    "                def_attrs = ['Def', 'definition', 'def', 'meaning']\n",
    "                for def_attr in def_attrs:\n",
    "                    if hasattr(sense, def_attr):\n",
    "                        definition = getattr(sense, def_attr)\n",
    "                        if definition:\n",
    "                            break\n",
    "                \n",
    "                if definition:\n",
    "                    # 使用jieba分词提取定义中的中文词汇\n",
    "                    words_in_def = jieba.lcut(definition)\n",
    "                    for word in words_in_def:\n",
    "                        if is_valid_chinese_word(word) and word != target_word and len(word) >= 2:\n",
    "                            similar_words.add(word)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 提取定义关键词时出错: {e}\")\n",
    "            pass\n",
    "        \n",
    "        # 方法3: 从相关词汇中提取（基于词性）\n",
    "        try:\n",
    "            # 获取相同词性的相关词汇\n",
    "            target_pos = \"\"\n",
    "            pos_attrs = ['zh_grammar', 'pos', 'part_of_speech']\n",
    "            for pos_attr in pos_attrs:\n",
    "                if hasattr(primary_sense, pos_attr):\n",
    "                    target_pos = getattr(primary_sense, pos_attr)\n",
    "                    if target_pos:\n",
    "                        break\n",
    "            \n",
    "            if target_pos:\n",
    "                # 从已获得的相关词汇中进一步筛选（限制数量避免过度搜索）\n",
    "                similar_words_list = list(similar_words)[:10]  # 减少搜索范围\n",
    "                for word in similar_words_list:\n",
    "                    try:\n",
    "                        related_senses = hownet_dict.get_sense(word)\n",
    "                        if related_senses:\n",
    "                            for related_sense in related_senses[:1]:  # 只检查第一个义项\n",
    "                                related_pos = \"\"\n",
    "                                for pos_attr in pos_attrs:\n",
    "                                    if hasattr(related_sense, pos_attr):\n",
    "                                        related_pos = getattr(related_sense, pos_attr)\n",
    "                                        if related_pos:\n",
    "                                            break\n",
    "                                \n",
    "                                if related_pos == target_pos:\n",
    "                                    # 尝试获取相关词汇\n",
    "                                    if hasattr(related_sense, 'zh_word') and related_sense.zh_word:\n",
    "                                        related_word = related_sense.zh_word\n",
    "                                        if is_valid_chinese_word(related_word) and related_word != target_word:\n",
    "                                            similar_words.add(related_word)\n",
    "                    except:\n",
    "                        continue\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 基于词性提取相关词时出错: {e}\")\n",
    "            pass\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 提取 {target_word} 的相似词时出错: {e}\")\n",
    "    \n",
    "    # 过滤并返回结果\n",
    "    result = [word for word in similar_words if is_valid_chinese_word(word)][:max_count]\n",
    "    return result\n",
    "\n",
    "print(\"✅ 中文词汇处理工具函数已定义\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏗️ 开始构建中文Taboo数据集...\n",
      "📊 目标: 每个词性 25 个词，总计 100 个词\n",
      "🔍 正在获取HowNet中文词汇...\n",
      "📚 HowNet中文词汇总数: 130347 个\n",
      "🔍 正在分析词汇词性...\n",
      "   已处理 500/5000 个词汇\n",
      "   已处理 1000/5000 个词汇\n",
      "   已处理 1500/5000 个词汇\n",
      "   已处理 2000/5000 个词汇\n",
      "   已处理 2500/5000 个词汇\n",
      "   已处理 3000/5000 个词汇\n",
      "   已处理 3500/5000 个词汇\n",
      "   已处理 4000/5000 个词汇\n",
      "   已处理 4500/5000 个词汇\n",
      "   已处理 5000/5000 个词汇\n",
      "\n",
      "📈 词性分布统计:\n",
      "   noun: 2528 个候选词\n",
      "   verb: 1257 个候选词\n",
      "   adj: 490 个候选词\n",
      "   adv: 77 个候选词\n",
      "\n",
      "🎯 开始选择目标词汇并生成禁用词...\n",
      "\n",
      "🔄 正在处理 noun 类词汇 (25 个)...\n",
      "   处理 1/25: 人工岛\n",
      "   处理 2/25: 靳\n",
      "   处理 3/25: 近几个月来\n",
      "   处理 4/25: 虚数\n",
      "   处理 5/25: 供需矛盾\n",
      "   处理 6/25: 苦竹\n",
      "   处理 7/25: 长发\n",
      "   处理 8/25: 多米诺骨牌\n",
      "   处理 9/25: 单位\n",
      "   处理 10/25: 协奏曲\n",
      "   处理 11/25: 盐浓度\n",
      "   处理 12/25: 冤假错案\n",
      "   处理 13/25: 饧\n",
      "   处理 14/25: 军博\n",
      "   处理 15/25: 球果\n",
      "   处理 16/25: 日光节约时间\n",
      "   处理 17/25: 海伦娜\n",
      "   处理 18/25: 康马县\n",
      "   处理 19/25: 入场券\n",
      "   处理 20/25: 咬翼片\n",
      "   处理 21/25: 双轨\n",
      "   处理 22/25: 冰镩\n",
      "   处理 23/25: 送信人\n",
      "   处理 24/25: 海商法\n",
      "   处理 25/25: 生命迹象\n",
      "\n",
      "🔄 正在处理 verb 类词汇 (25 个)...\n",
      "   处理 1/25: 虚掷\n",
      "   处理 2/25: 解除\n",
      "   处理 3/25: 矍\n",
      "   处理 4/25: 打八折\n",
      "   处理 5/25: 猛涨\n",
      "   处理 6/25: 老羞成怒\n",
      "   处理 7/25: 身心交瘁\n",
      "   处理 8/25: 言归正传\n",
      "   处理 9/25: 苦乐与共\n",
      "   处理 10/25: 渐弱\n",
      "   处理 11/25: 引申\n",
      "   处理 12/25: 将低于\n",
      "   处理 13/25: 图个吉利\n",
      "   处理 14/25: 溶解\n",
      "   处理 15/25: 代购\n",
      "   处理 16/25: 如坐针毡\n",
      "   处理 17/25: 玩不转\n",
      "   处理 18/25: 致电\n",
      "   处理 19/25: 崇洋迷外\n",
      "   处理 20/25: 踹开\n",
      "   处理 21/25: 铭记\n",
      "   处理 22/25: 决一胜负\n",
      "   处理 23/25: 压货\n",
      "   处理 24/25: 齐唱\n",
      "   处理 25/25: 隐退\n",
      "\n",
      "🔄 正在处理 adj 类词汇 (25 个)...\n",
      "   处理 1/25: 应受谴责\n",
      "   处理 2/25: 犹豫不前\n",
      "   处理 3/25: 可鄙\n",
      "   处理 4/25: 可望不可即\n",
      "   处理 5/25: 低端\n",
      "   处理 6/25: 庞杂\n",
      "   处理 7/25: 齐声\n",
      "   处理 8/25: 缛\n",
      "   处理 9/25: 不虔诚\n",
      "   处理 10/25: 星际\n",
      "   处理 11/25: 众\n",
      "   处理 12/25: 直观\n",
      "   处理 13/25: 忸忸怩怩\n",
      "   处理 14/25: 净\n",
      "   处理 15/25: 小件\n",
      "   处理 16/25: 没意思\n",
      "   处理 17/25: 不知死活\n",
      "   处理 18/25: 不可估量\n",
      "   处理 19/25: 谈吐文雅\n",
      "   处理 20/25: 黏糊糊\n",
      "   处理 21/25: 灰不溜秋\n",
      "   处理 22/25: 低产\n",
      "   处理 23/25: 浓厚\n",
      "   处理 24/25: 更高\n",
      "   处理 25/25: 外挂\n",
      "\n",
      "🔄 正在处理 adv 类词汇 (25 个)...\n",
      "   处理 1/25: 恰到好处\n",
      "   处理 2/25: 照例\n",
      "   处理 3/25: 看样子\n",
      "   处理 4/25: 不可避免地\n",
      "   处理 5/25: 时有\n",
      "   处理 6/25: 全都\n",
      "   处理 7/25: 嗣后\n",
      "   处理 8/25: 残暴地\n",
      "   处理 9/25: 偶尔\n",
      "   处理 10/25: 往前\n",
      "   处理 11/25: 整体上\n",
      "   处理 12/25: 在此之前\n",
      "   处理 13/25: 总归\n",
      "   处理 14/25: 从那时起\n",
      "   处理 15/25: 由东而西\n",
      "   处理 16/25: 一点一滴地\n",
      "   处理 17/25: 怪不得\n",
      "   处理 18/25: 不争地\n",
      "   处理 19/25: 肃然\n",
      "   处理 20/25: 从始至终\n",
      "   处理 21/25: 有顷\n",
      "   处理 22/25: 早早晚晚\n",
      "   处理 23/25: 要不\n",
      "   处理 24/25: 最大限度地\n",
      "   处理 25/25: 诚然\n",
      "\n",
      "✅ 中文Taboo数据集构建完成！\n",
      "📊 总词汇数: 100 个\n"
     ]
    }
   ],
   "source": [
    "# 修复的中文Taboo数据集构建（基于OpenHowNet）\n",
    "print(\"🏗️ 开始构建中文Taboo数据集...\")\n",
    "\n",
    "def build_chinese_taboo_dataset_corrected(hownet_dict, target_count_per_pos: int = 25):\n",
    "    \"\"\"构建中文Taboo数据集 - 修复版\"\"\"\n",
    "    \n",
    "    if hownet_dict is None:\n",
    "        print(\"❌ HowNet词典未初始化，无法构建数据集\")\n",
    "        return []\n",
    "    \n",
    "    pos_mapping = get_pos_mapping()\n",
    "    target_pos_list = ['noun', 'verb', 'adj', 'adv']\n",
    "    dataset = []\n",
    "    \n",
    "    print(f\"📊 目标: 每个词性 {target_count_per_pos} 个词，总计 {target_count_per_pos * 4} 个词\")\n",
    "    \n",
    "    # 获取HowNet中文词汇表 - 使用正确的API方法\n",
    "    try:\n",
    "        print(\"🔍 正在获取HowNet中文词汇...\")\n",
    "        zh_words = hownet_dict.get_zh_words()\n",
    "        chinese_vocab = [word for word in zh_words if is_valid_chinese_word(word)]\n",
    "        print(f\"📚 HowNet中文词汇总数: {len(chinese_vocab)} 个\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 获取中文词汇失败: {e}\")\n",
    "        print(\"💡 提示: 确保OpenHowNet数据已正确下载\")\n",
    "        return []\n",
    "    \n",
    "    # 按词性分组收集词汇\n",
    "    words_by_pos = {pos: [] for pos in target_pos_list}\n",
    "    \n",
    "    print(\"🔍 正在分析词汇词性...\")\n",
    "    progress_count = 0\n",
    "    \n",
    "    for word in chinese_vocab[:5000]:  # 限制处理前5000个词汇以节省时间\n",
    "        progress_count += 1\n",
    "        if progress_count % 500 == 0:\n",
    "            print(f\"   已处理 {progress_count}/{min(5000, len(chinese_vocab))} 个词汇\")\n",
    "        \n",
    "        try:\n",
    "            # 获取词汇的义项信息\n",
    "            senses = hownet_dict.get_sense(word)\n",
    "            if not senses:\n",
    "                continue\n",
    "            \n",
    "            # 获取主要词性\n",
    "            primary_sense = senses[0]\n",
    "            \n",
    "            # 尝试不同的词性获取方法\n",
    "            pos_info = None\n",
    "            for attr in ['zh_grammar', 'pos', 'part_of_speech']:\n",
    "                if hasattr(primary_sense, attr):\n",
    "                    pos_info = getattr(primary_sense, attr)\n",
    "                    if pos_info:\n",
    "                        break\n",
    "            \n",
    "            if not pos_info:\n",
    "                continue\n",
    "                \n",
    "            # 映射到标准词性\n",
    "            standard_pos = pos_mapping.get(pos_info, None)\n",
    "            if standard_pos and standard_pos in target_pos_list:\n",
    "                words_by_pos[standard_pos].append({\n",
    "                    'word': word,\n",
    "                    'senses': senses,\n",
    "                    'primary_pos': standard_pos\n",
    "                })\n",
    "        \n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n📈 词性分布统计:\")\n",
    "    for pos, words in words_by_pos.items():\n",
    "        print(f\"   {pos}: {len(words)} 个候选词\")\n",
    "    \n",
    "    # 为每个词性随机选择指定数量的词汇\n",
    "    print(f\"\\n🎯 开始选择目标词汇并生成禁用词...\")\n",
    "    \n",
    "    for pos in target_pos_list:\n",
    "        available_words = words_by_pos[pos]\n",
    "        if len(available_words) == 0:\n",
    "            print(f\"⚠️ {pos} 词性无可用词汇，跳过\")\n",
    "            continue\n",
    "            \n",
    "        selected_count = min(target_count_per_pos, len(available_words))\n",
    "        \n",
    "        # 随机选择词汇\n",
    "        selected_words = random.sample(available_words, selected_count)\n",
    "        print(f\"\\n🔄 正在处理 {pos} 类词汇 ({selected_count} 个)...\")\n",
    "        \n",
    "        for i, word_info in enumerate(selected_words):\n",
    "            target_word = word_info['word']\n",
    "            senses = word_info['senses']\n",
    "            \n",
    "            print(f\"   处理 {i+1}/{selected_count}: {target_word}\")\n",
    "            \n",
    "            # 生成禁用词\n",
    "            taboo_words = extract_similar_words_from_hownet(\n",
    "                target_word, pos, hownet_dict, max_count=8\n",
    "            )\n",
    "            \n",
    "            # 确保至少有5个禁用词\n",
    "            if len(taboo_words) < 5:\n",
    "                # 根据词性添加通用禁用词\n",
    "                generic_mapping = {\n",
    "                    'noun': ['东西', '物品', '事物', '对象', '名词'],\n",
    "                    'verb': ['动作', '行为', '做', '进行', '活动'],\n",
    "                    'adj': ['特征', '性质', '状态', '形容', '描述'],\n",
    "                    'adv': ['方式', '程度', '如何', '状况', '修饰']\n",
    "                }\n",
    "                \n",
    "                generic_taboos = generic_mapping.get(pos, ['相关', '概念', '词汇', '内容', '意思'])\n",
    "                for generic in generic_taboos:\n",
    "                    if generic not in taboo_words and generic != target_word:\n",
    "                        taboo_words.append(generic)\n",
    "                        if len(taboo_words) >= 5:\n",
    "                            break\n",
    "            \n",
    "            # 将 Sense 对象转换为可序列化的字典格式\n",
    "            serializable_senses = []\n",
    "            for sense in senses:\n",
    "                sense_dict = {\n",
    "                    'zh_word': getattr(sense, 'zh_word', ''),\n",
    "                    'en_word': getattr(sense, 'en_word', ''),\n",
    "                    'zh_grammar': getattr(sense, 'zh_grammar', ''),\n",
    "                    'en_grammar': getattr(sense, 'en_grammar', ''),\n",
    "                    'Def': getattr(sense, 'Def', ''),\n",
    "                    'No': getattr(sense, 'No', ''),\n",
    "                    'sememes': str(getattr(sense, 'sememes', []))  # 转换为字符串\n",
    "                }\n",
    "                serializable_senses.append(sense_dict)\n",
    "            \n",
    "            # 构建数据集条目\n",
    "            entry = {\n",
    "                'target': target_word,\n",
    "                'part_of_speech': pos,\n",
    "                'taboo': taboo_words[:5],  # 确保正好5个禁用词\n",
    "                'category': 'chinese_hownet',\n",
    "                'senses': serializable_senses,  # 使用可序列化的版本\n",
    "                'metadata': {\n",
    "                    'sense_count': len(senses),\n",
    "                    'taboo_count': len(taboo_words[:5]),\n",
    "                    'source': 'openhownet_corrected'\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            dataset.append(entry)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# 构建数据集\n",
    "if hownet_dict:\n",
    "    chinese_dataset = build_chinese_taboo_dataset_corrected(hownet_dict, target_count_per_pos=25)\n",
    "    print(f\"\\n✅ 中文Taboo数据集构建完成！\")\n",
    "    print(f\"📊 总词汇数: {len(chinese_dataset)} 个\")\n",
    "else:\n",
    "    print(\"❌ OpenHowNet未正确初始化，无法构建数据集\")\n",
    "    chinese_dataset = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏗️ 开始构建中文Taboo数据集...\n",
      "📊 目标: 每个词性 25 个词，总计 100 个词\n",
      "📚 HowNet中文词汇总数: 130347 个\n",
      "🔍 正在分析词汇词性...\n",
      "   已处理 1000/130347 个词汇\n",
      "   已处理 2000/130347 个词汇\n",
      "   已处理 3000/130347 个词汇\n",
      "   已处理 4000/130347 个词汇\n",
      "   已处理 5000/130347 个词汇\n",
      "   已处理 6000/130347 个词汇\n",
      "   已处理 7000/130347 个词汇\n",
      "   已处理 8000/130347 个词汇\n",
      "   已处理 9000/130347 个词汇\n",
      "   已处理 10000/130347 个词汇\n",
      "   已处理 11000/130347 个词汇\n",
      "   已处理 12000/130347 个词汇\n",
      "   已处理 13000/130347 个词汇\n",
      "   已处理 14000/130347 个词汇\n",
      "   已处理 15000/130347 个词汇\n",
      "   已处理 16000/130347 个词汇\n",
      "   已处理 17000/130347 个词汇\n",
      "   已处理 18000/130347 个词汇\n",
      "   已处理 19000/130347 个词汇\n",
      "   已处理 20000/130347 个词汇\n",
      "   已处理 21000/130347 个词汇\n",
      "   已处理 22000/130347 个词汇\n",
      "   已处理 23000/130347 个词汇\n",
      "   已处理 24000/130347 个词汇\n",
      "   已处理 25000/130347 个词汇\n",
      "   已处理 26000/130347 个词汇\n",
      "   已处理 27000/130347 个词汇\n",
      "   已处理 28000/130347 个词汇\n",
      "   已处理 29000/130347 个词汇\n",
      "   已处理 30000/130347 个词汇\n",
      "   已处理 31000/130347 个词汇\n",
      "   已处理 32000/130347 个词汇\n",
      "   已处理 33000/130347 个词汇\n",
      "   已处理 34000/130347 个词汇\n",
      "   已处理 35000/130347 个词汇\n",
      "   已处理 36000/130347 个词汇\n",
      "   已处理 37000/130347 个词汇\n",
      "   已处理 38000/130347 个词汇\n",
      "   已处理 39000/130347 个词汇\n",
      "   已处理 40000/130347 个词汇\n",
      "   已处理 41000/130347 个词汇\n",
      "   已处理 42000/130347 个词汇\n",
      "   已处理 43000/130347 个词汇\n",
      "   已处理 44000/130347 个词汇\n",
      "   已处理 45000/130347 个词汇\n",
      "   已处理 46000/130347 个词汇\n",
      "   已处理 47000/130347 个词汇\n",
      "   已处理 48000/130347 个词汇\n",
      "   已处理 49000/130347 个词汇\n",
      "   已处理 50000/130347 个词汇\n",
      "   已处理 51000/130347 个词汇\n",
      "   已处理 52000/130347 个词汇\n",
      "   已处理 53000/130347 个词汇\n",
      "   已处理 54000/130347 个词汇\n",
      "   已处理 55000/130347 个词汇\n",
      "   已处理 56000/130347 个词汇\n",
      "   已处理 57000/130347 个词汇\n",
      "   已处理 58000/130347 个词汇\n",
      "   已处理 59000/130347 个词汇\n",
      "   已处理 60000/130347 个词汇\n",
      "   已处理 61000/130347 个词汇\n",
      "   已处理 62000/130347 个词汇\n",
      "   已处理 63000/130347 个词汇\n",
      "   已处理 64000/130347 个词汇\n",
      "   已处理 65000/130347 个词汇\n",
      "   已处理 66000/130347 个词汇\n",
      "   已处理 67000/130347 个词汇\n",
      "   已处理 68000/130347 个词汇\n",
      "   已处理 69000/130347 个词汇\n",
      "   已处理 70000/130347 个词汇\n",
      "   已处理 71000/130347 个词汇\n",
      "   已处理 72000/130347 个词汇\n",
      "   已处理 73000/130347 个词汇\n",
      "   已处理 74000/130347 个词汇\n",
      "   已处理 75000/130347 个词汇\n",
      "   已处理 76000/130347 个词汇\n",
      "   已处理 77000/130347 个词汇\n",
      "   已处理 78000/130347 个词汇\n",
      "   已处理 79000/130347 个词汇\n",
      "   已处理 80000/130347 个词汇\n",
      "   已处理 81000/130347 个词汇\n",
      "   已处理 82000/130347 个词汇\n",
      "   已处理 83000/130347 个词汇\n",
      "   已处理 84000/130347 个词汇\n",
      "   已处理 85000/130347 个词汇\n",
      "   已处理 86000/130347 个词汇\n",
      "   已处理 87000/130347 个词汇\n",
      "   已处理 88000/130347 个词汇\n",
      "   已处理 89000/130347 个词汇\n",
      "   已处理 90000/130347 个词汇\n",
      "   已处理 91000/130347 个词汇\n",
      "   已处理 92000/130347 个词汇\n",
      "   已处理 93000/130347 个词汇\n",
      "   已处理 94000/130347 个词汇\n",
      "   已处理 95000/130347 个词汇\n",
      "   已处理 96000/130347 个词汇\n",
      "   已处理 97000/130347 个词汇\n",
      "   已处理 98000/130347 个词汇\n",
      "   已处理 99000/130347 个词汇\n",
      "   已处理 100000/130347 个词汇\n",
      "   已处理 101000/130347 个词汇\n",
      "   已处理 102000/130347 个词汇\n",
      "   已处理 103000/130347 个词汇\n",
      "   已处理 104000/130347 个词汇\n",
      "   已处理 105000/130347 个词汇\n",
      "   已处理 106000/130347 个词汇\n",
      "   已处理 107000/130347 个词汇\n",
      "   已处理 108000/130347 个词汇\n",
      "   已处理 109000/130347 个词汇\n",
      "   已处理 110000/130347 个词汇\n",
      "   已处理 111000/130347 个词汇\n",
      "   已处理 112000/130347 个词汇\n",
      "   已处理 113000/130347 个词汇\n",
      "   已处理 114000/130347 个词汇\n",
      "   已处理 115000/130347 个词汇\n",
      "   已处理 116000/130347 个词汇\n",
      "   已处理 117000/130347 个词汇\n",
      "   已处理 118000/130347 个词汇\n",
      "   已处理 119000/130347 个词汇\n",
      "   已处理 120000/130347 个词汇\n",
      "   已处理 121000/130347 个词汇\n",
      "   已处理 122000/130347 个词汇\n",
      "   已处理 123000/130347 个词汇\n",
      "   已处理 124000/130347 个词汇\n",
      "   已处理 125000/130347 个词汇\n",
      "   已处理 126000/130347 个词汇\n",
      "   已处理 127000/130347 个词汇\n",
      "   已处理 128000/130347 个词汇\n",
      "   已处理 129000/130347 个词汇\n",
      "   已处理 130000/130347 个词汇\n",
      "\n",
      "📈 词性分布统计:\n",
      "   noun: 66456 个候选词\n",
      "   verb: 31994 个候选词\n",
      "   adj: 12641 个候选词\n",
      "   adv: 2217 个候选词\n",
      "\n",
      "🎯 开始选择目标词汇并生成禁用词...\n",
      "\n",
      "🔄 正在处理 noun 类词汇 (25 个)...\n",
      "   处理 1/25: 滤液\n",
      "   处理 2/25: 岁时\n",
      "   处理 3/25: 联络员\n",
      "   处理 4/25: 赌城\n",
      "   处理 5/25: 评委会\n",
      "   处理 6/25: 红烧鲴鱼\n",
      "   处理 7/25: 佛手\n",
      "   处理 8/25: 减速剂\n",
      "   处理 9/25: 评论员文章\n",
      "   处理 10/25: 开局\n",
      "   处理 11/25: 脊锯\n",
      "   处理 12/25: 录取分数线\n",
      "   处理 13/25: 千里鹅毛\n",
      "   处理 14/25: 品牌\n",
      "   处理 15/25: 全场\n",
      "   处理 16/25: 过山车\n",
      "   处理 17/25: 仁兄\n",
      "   处理 18/25: 物象\n",
      "   处理 19/25: 大烛台\n",
      "   处理 20/25: 绿豆糕\n",
      "   处理 21/25: 撑杆\n",
      "   处理 22/25: 药单\n",
      "   处理 23/25: 靶器官保护\n",
      "   处理 24/25: 布店\n",
      "   处理 25/25: 跳伞塔\n",
      "\n",
      "🔄 正在处理 verb 类词汇 (25 个)...\n",
      "   处理 1/25: 犯境\n",
      "   处理 2/25: 怒骂\n",
      "   处理 3/25: 脚踩两只船\n",
      "   处理 4/25: 拍报\n",
      "   处理 5/25: 屠城\n",
      "   处理 6/25: 走水\n",
      "   处理 7/25: 哀叹\n",
      "   处理 8/25: 奏参\n",
      "   处理 9/25: 落实\n",
      "   处理 10/25: 负气\n",
      "   处理 11/25: 势在必行\n",
      "   处理 12/25: 减少库存\n",
      "   处理 13/25: 攀新高\n",
      "   处理 14/25: 望见\n",
      "   处理 15/25: 怜贫惜老\n",
      "   处理 16/25: 疲劳\n",
      "   处理 17/25: 远足\n",
      "   处理 18/25: 投注\n",
      "   处理 19/25: 寄养\n",
      "   处理 20/25: 镗\n",
      "   处理 21/25: 过眼烟云\n",
      "   处理 22/25: 继续担任\n",
      "   处理 23/25: 引产\n",
      "   处理 24/25: 乏顿\n",
      "   处理 25/25: 止痒\n",
      "\n",
      "🔄 正在处理 adj 类词汇 (25 个)...\n",
      "   处理 1/25: 更加漂亮\n",
      "   处理 2/25: 贼溜溜\n",
      "   处理 3/25: 无边\n",
      "   处理 4/25: 睿哲\n",
      "   处理 5/25: 玫瑰紫\n",
      "   处理 6/25: 污秽\n",
      "   处理 7/25: 够味儿\n",
      "   处理 8/25: 神不知鬼不觉\n",
      "   处理 9/25: 多雨\n",
      "   处理 10/25: 信息丰富\n",
      "   处理 11/25: 无特色\n",
      "   处理 12/25: 一无可取\n",
      "   处理 13/25: 凊\n",
      "   处理 14/25: 壁立千仞\n",
      "   处理 15/25: 速成\n",
      "   处理 16/25: 洪都拉斯\n",
      "   处理 17/25: 淼\n",
      "   处理 18/25: 娴静\n",
      "   处理 19/25: 老幼\n",
      "   处理 20/25: 老气横秋\n",
      "   处理 21/25: 水渌渌\n",
      "   处理 22/25: 有能力\n",
      "   处理 23/25: 浅色\n",
      "   处理 24/25: 不流血\n",
      "   处理 25/25: 热门\n",
      "\n",
      "🔄 正在处理 adv 类词汇 (25 个)...\n",
      "   处理 1/25: 年代错误\n",
      "   处理 2/25: 远远\n",
      "   处理 3/25: 总算\n",
      "   处理 4/25: 很快\n",
      "   处理 5/25: 不公正地\n",
      "   处理 6/25: 在某些方面\n",
      "   处理 7/25: 可靠地\n",
      "   处理 8/25: 何其\n",
      "   处理 9/25: 恐怕\n",
      "   处理 10/25: 为之\n",
      "   处理 11/25: 后验地\n",
      "   处理 12/25: 固定地\n",
      "   处理 13/25: 挨肩\n",
      "   处理 14/25: 遗传学上\n",
      "   处理 15/25: 于此\n",
      "   处理 16/25: 刚刚\n",
      "   处理 17/25: 为什么不\n",
      "   处理 18/25: 谁知道\n",
      "   处理 19/25: 公然\n",
      "   处理 20/25: 挨边\n",
      "   处理 21/25: 即刻\n",
      "   处理 22/25: 起初\n",
      "   处理 23/25: 无奢望地\n",
      "   处理 24/25: 还没有\n",
      "   处理 25/25: 即是说\n",
      "\n",
      "✅ 中文Taboo数据集构建完成！\n",
      "📊 总词汇数: 100 个\n"
     ]
    }
   ],
   "source": [
    "# 4. 构建中文Taboo数据集\n",
    "print(\"🏗️ 开始构建中文Taboo数据集...\")\n",
    "\n",
    "def build_chinese_taboo_dataset(hownet_dict, target_count_per_pos: int = 25) -> List[Dict[str, Any]]:\n",
    "    \"\"\"构建中文Taboo数据集\"\"\"\n",
    "    \n",
    "    if hownet_dict is None:\n",
    "        print(\"❌ HowNet词典未初始化，无法构建数据集\")\n",
    "        return []\n",
    "    \n",
    "    pos_mapping = get_pos_mapping()\n",
    "    target_pos_list = ['noun', 'verb', 'adj', 'adv']\n",
    "    dataset = []\n",
    "    \n",
    "    print(f\"📊 目标: 每个词性 {target_count_per_pos} 个词，总计 {target_count_per_pos * 4} 个词\")\n",
    "    \n",
    "    # 获取HowNet中文词汇表\n",
    "    chinese_vocab = hownet_dict.get_zh_words()\n",
    "    chinese_vocab = [word for word in chinese_vocab if is_valid_chinese_word(word)]\n",
    "    print(f\"📚 HowNet中文词汇总数: {len(chinese_vocab)} 个\")\n",
    "    \n",
    "    # 按词性分组收集词汇\n",
    "    words_by_pos = {pos: [] for pos in target_pos_list}\n",
    "    \n",
    "    print(\"🔍 正在分析词汇词性...\")\n",
    "    progress_count = 0\n",
    "    \n",
    "    for word in chinese_vocab:\n",
    "        progress_count += 1\n",
    "        if progress_count % 1000 == 0:\n",
    "            print(f\"   已处理 {progress_count}/{len(chinese_vocab)} 个词汇\")\n",
    "        \n",
    "        try:\n",
    "            # 获取词汇的义项信息\n",
    "            senses = hownet_dict.get_sense(word)\n",
    "            if not senses:\n",
    "                continue\n",
    "            \n",
    "            # 获取主要词性 - OpenHowNet的Sense对象使用属性而不是字典\n",
    "            primary_sense = senses[0]\n",
    "            \n",
    "            # 尝试不同的词性获取方法\n",
    "            pos_info = None\n",
    "            if hasattr(primary_sense, 'zh_grammar'):\n",
    "                pos_info = primary_sense.zh_grammar\n",
    "            elif hasattr(primary_sense, 'pos'):\n",
    "                pos_info = primary_sense.pos\n",
    "            elif hasattr(primary_sense, 'part_of_speech'):\n",
    "                pos_info = primary_sense.part_of_speech\n",
    "            \n",
    "            if not pos_info:\n",
    "                continue\n",
    "                \n",
    "            # 映射到标准词性\n",
    "            standard_pos = pos_mapping.get(pos_info, None)\n",
    "            if standard_pos and standard_pos in target_pos_list:\n",
    "                words_by_pos[standard_pos].append({\n",
    "                    'word': word,\n",
    "                    'senses': senses,\n",
    "                    'primary_pos': standard_pos\n",
    "                })\n",
    "        \n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    print(\"\\n📈 词性分布统计:\")\n",
    "    for pos, words in words_by_pos.items():\n",
    "        print(f\"   {pos}: {len(words)} 个候选词\")\n",
    "    \n",
    "    # 为每个词性随机选择指定数量的词汇\n",
    "    print(\"\\n🎯 开始选择目标词汇并生成禁用词...\")\n",
    "    \n",
    "    for pos in target_pos_list:\n",
    "        available_words = words_by_pos[pos]\n",
    "        if len(available_words) < target_count_per_pos:\n",
    "            print(f\"⚠️ {pos} 词性可用词汇不足 ({len(available_words)} < {target_count_per_pos})\")\n",
    "            selected_count = len(available_words)\n",
    "        else:\n",
    "            selected_count = target_count_per_pos\n",
    "        \n",
    "        # 随机选择词汇\n",
    "        selected_words = random.sample(available_words, selected_count)\n",
    "        print(f\"\\n🔄 正在处理 {pos} 类词汇 ({selected_count} 个)...\")\n",
    "        \n",
    "        for i, word_info in enumerate(selected_words):\n",
    "            target_word = word_info['word']\n",
    "            senses = word_info['senses']\n",
    "            \n",
    "            print(f\"   处理 {i+1}/{selected_count}: {target_word}\")\n",
    "            \n",
    "            # 生成禁用词\n",
    "            taboo_words = extract_similar_words_from_hownet(\n",
    "                target_word, pos, hownet_dict, max_count=8\n",
    "            )\n",
    "            \n",
    "            # 如果禁用词不够，添加一些通用的相关词\n",
    "            if len(taboo_words) < 5:\n",
    "                # 使用jieba分词从定义中提取更多词汇\n",
    "                for sense in senses[:2]:  # 只取前两个义项\n",
    "                    # 正确使用属性访问而不是字典访问\n",
    "                    definition = getattr(sense, 'Def', '') if hasattr(sense, 'Def') else ''\n",
    "                    def_words = jieba.lcut(definition)\n",
    "                    for def_word in def_words:\n",
    "                        if (is_valid_chinese_word(def_word) and \n",
    "                            def_word != target_word and \n",
    "                            len(def_word) >= 2 and \n",
    "                            def_word not in taboo_words):\n",
    "                            taboo_words.append(def_word)\n",
    "                            if len(taboo_words) >= 5:\n",
    "                                break\n",
    "            \n",
    "            # 确保至少有5个禁用词\n",
    "            taboo_words = taboo_words[:5]  # 限制为5个\n",
    "            if len(taboo_words) < 5:\n",
    "                # 如果还是不够，添加一些通用词汇\n",
    "                generic_taboos = ['东西', '事物', '物品', '概念', '内容']\n",
    "                for generic in generic_taboos:\n",
    "                    if generic not in taboo_words and generic != target_word:\n",
    "                        taboo_words.append(generic)\n",
    "                        if len(taboo_words) >= 5:\n",
    "                            break\n",
    "            \n",
    "            # 将 Sense 对象转换为可序列化的字典格式\n",
    "            serializable_senses = []\n",
    "            for sense in senses:\n",
    "                sense_dict = {\n",
    "                    'zh_word': getattr(sense, 'zh_word', ''),\n",
    "                    'en_word': getattr(sense, 'en_word', ''),\n",
    "                    'zh_grammar': getattr(sense, 'zh_grammar', ''),\n",
    "                    'en_grammar': getattr(sense, 'en_grammar', ''),\n",
    "                    'Def': getattr(sense, 'Def', ''),\n",
    "                    'No': getattr(sense, 'No', ''),\n",
    "                    'sememes': str(getattr(sense, 'sememes', []))  # 转换为字符串\n",
    "                }\n",
    "                serializable_senses.append(sense_dict)\n",
    "            \n",
    "            # 构建数据集条目\n",
    "            entry = {\n",
    "                'target': target_word,\n",
    "                'part_of_speech': pos,\n",
    "                'taboo': taboo_words[:5],  # 确保正好5个禁用词\n",
    "                'category': 'chinese_general',\n",
    "                'senses': serializable_senses,  # 使用可序列化的版本\n",
    "                'metadata': {\n",
    "                    'sense_count': len(senses),\n",
    "                    'taboo_count': len(taboo_words[:5]),\n",
    "                    'source': 'openhownet'\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            dataset.append(entry)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# 构建数据集\n",
    "chinese_dataset = build_chinese_taboo_dataset(hownet_dict, target_count_per_pos=25)\n",
    "print(f\"\\n✅ 中文Taboo数据集构建完成！\")\n",
    "print(f\"📊 总词汇数: {len(chinese_dataset)} 个\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'taboo_benchmark/data/chinese_dataset.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 如果chinese_dataset还没加载，先加载\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtaboo_benchmark/data/chinese_dataset.json\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      6\u001b[39m     chinese_dataset = json.load(f)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 按词性分组\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/IPython/core/interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'taboo_benchmark/data/chinese_dataset.json'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "# 如果chinese_dataset还没加载，先加载\n",
    "with open('taboo_benchmark/data/chinese_dataset.json', 'r', encoding='utf-8') as f:\n",
    "    chinese_dataset = json.load(f)\n",
    "\n",
    "# 按词性分组\n",
    "pos_groups = {'noun': [], 'verb': [], 'adj': [], 'adv': []}\n",
    "for item in chinese_dataset:\n",
    "    pos = item.get('part_of_speech')\n",
    "    if pos in pos_groups:\n",
    "        pos_groups[pos].append(item)\n",
    "\n",
    "# 每类随机抽取10个\n",
    "sampled_dataset = []\n",
    "for pos, items in pos_groups.items():\n",
    "    sampled = random.sample(items, min(10, len(items)))\n",
    "    sampled_dataset.extend(sampled)\n",
    "\n",
    "# 检查结果\n",
    "for pos in pos_groups:\n",
    "    count = len([x for x in sampled_dataset if x['part_of_speech'] == pos])\n",
    "    print(f\"{pos}: {count} 个\")\n",
    "\n",
    "# 如需保存\n",
    "with open('taboo_benchmark/data/chinese_dataset_sample10.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(sampled_dataset, f, ensure_ascii=False, indent=2)\n",
    "print(\"已保存到 taboo_benchmark/data/chinese_dataset_sample10.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 中文Taboo数据集统计分析:\n",
      "==================================================\n",
      "📝 总词汇数: 100\n",
      "\n",
      "🏷️ 词性分布:\n",
      "   noun: 25 个 (25.0%)\n",
      "   verb: 25 个 (25.0%)\n",
      "   adj: 25 个 (25.0%)\n",
      "   adv: 25 个 (25.0%)\n",
      "\n",
      "🚫 禁用词统计:\n",
      "   平均数量: 5.0\n",
      "   范围: 5 - 5\n",
      "\n",
      "💭 义项统计:\n",
      "   平均数量: 1.8\n",
      "   范围: 1 - 12\n",
      "\n",
      "📋 数据样本 (随机5个):\n",
      "\n",
      "   样本 1:\n",
      "     目标词: 娴静\n",
      "     词性: adj\n",
      "     禁用词: ['东西', '事物', '物品', '概念', '内容']\n",
      "     定义: {gracious|雅}...\n",
      "\n",
      "   样本 2:\n",
      "     目标词: 总算\n",
      "     词性: adv\n",
      "     禁用词: ['功能', '时间', '特性', '东西', '事物']\n",
      "     定义: {FuncWord|功能词:comment={?}}...\n",
      "\n",
      "   样本 3:\n",
      "     目标词: 玫瑰紫\n",
      "     词性: adj\n",
      "     禁用词: ['东西', '事物', '物品', '概念', '内容']\n",
      "     定义: {red|红}...\n",
      "\n",
      "   样本 4:\n",
      "     目标词: 脚踩两只船\n",
      "     词性: verb\n",
      "     禁用词: ['得罪', '回避', '东西', '事物', '物品']\n",
      "     定义: {evade|回避:content={offend|得罪}}...\n",
      "\n",
      "   样本 5:\n",
      "     目标词: 老气横秋\n",
      "     词性: adj\n",
      "     禁用词: ['消极', '东西', '事物', '物品', '概念']\n",
      "     定义: {inactive|消极}...\n",
      "\n",
      "✅ 统计分析完成\n"
     ]
    }
   ],
   "source": [
    "# 5. 数据集统计分析\n",
    "print(\"📊 中文Taboo数据集统计分析:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 基本统计\n",
    "total_words = len(chinese_dataset)\n",
    "print(f\"📝 总词汇数: {total_words}\")\n",
    "\n",
    "# 词性分布\n",
    "pos_counts = {}\n",
    "taboo_counts = []\n",
    "sense_counts = []\n",
    "\n",
    "for item in chinese_dataset:\n",
    "    pos = item.get('part_of_speech', 'unknown')\n",
    "    pos_counts[pos] = pos_counts.get(pos, 0) + 1\n",
    "    taboo_counts.append(len(item.get('taboo', [])))\n",
    "    sense_counts.append(len(item.get('senses', [])))\n",
    "\n",
    "print(f\"\\n🏷️ 词性分布:\")\n",
    "for pos, count in sorted(pos_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    percentage = count / total_words * 100\n",
    "    print(f\"   {pos}: {count} 个 ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\n🚫 禁用词统计:\")\n",
    "print(f\"   平均数量: {sum(taboo_counts) / len(taboo_counts):.1f}\")\n",
    "print(f\"   范围: {min(taboo_counts)} - {max(taboo_counts)}\")\n",
    "\n",
    "print(f\"\\n💭 义项统计:\")\n",
    "print(f\"   平均数量: {sum(sense_counts) / len(sense_counts):.1f}\")\n",
    "print(f\"   范围: {min(sense_counts)} - {max(sense_counts)}\")\n",
    "\n",
    "# 显示数据样本\n",
    "print(f\"\\n📋 数据样本 (随机5个):\")\n",
    "sample_items = random.sample(chinese_dataset, min(5, len(chinese_dataset)))\n",
    "for i, item in enumerate(sample_items, 1):\n",
    "    print(f\"\\n   样本 {i}:\")\n",
    "    print(f\"     目标词: {item['target']}\")\n",
    "    print(f\"     词性: {item['part_of_speech']}\")\n",
    "    print(f\"     禁用词: {item['taboo']}\")\n",
    "    if item.get('senses') and len(item['senses']) > 0:\n",
    "        # senses 现在是字典列表，直接访问字典键\n",
    "        sense = item['senses'][0]\n",
    "        definition = sense.get('Def', '无定义')\n",
    "        if definition and definition != '无定义':\n",
    "            print(f\"     定义: {definition[:50]}...\")\n",
    "        else:\n",
    "            print(f\"     定义: 无定义\")\n",
    "\n",
    "print(f\"\\n✅ 统计分析完成\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (91919430.py, line 58)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mtry:\\n\",\u001b[39m\n         ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "# 完整的中文Taboo实验\n",
    "print(\"🧪 完整中文Taboo实验系统...\")\n",
    "\n",
    "def run_full_chinese_experiment(client, models, dataset, experiment_name=\"chinese_taboo\"):\n",
    "    \"\"\"运行完整的中文Taboo实验\"\"\"\n",
    "    \n",
    "    if not client:\n",
    "        print(\"❌ API客户端未初始化，无法执行实验\")\n",
    "        return None\n",
    "    \n",
    "    # 实验配置\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    experiment_id = f\"{experiment_name}_{timestamp}\"\n",
    "    \n",
    "    print(f\"\\n🎯 完整实验配置:\")\n",
    "    print(f\"   实验ID: {experiment_id}\")\n",
    "    print(f\"   词汇数量: {len(dataset)}\")\n",
    "    print(f\"   模型数量: {len(models)}\")\n",
    "    print(f\"   总游戏数: {len(dataset) * len(models) * len(models)}\")\n",
    "    \n",
    "    # 创建结果目录\n",
    "    results_dir = f\"results/{experiment_id}\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # 分批处理以避免过长时间运行\n",
    "    batch_size = 10  # 每批处理10个词汇\n",
    "    all_results = []\n",
    "    \n",
    "    print(f\"\\n🚀 开始完整实验（分批处理）...\")\n",
    "    \n",
    "    # 按批次处理数据集\n",
    "    for batch_start in range(0, len(dataset), batch_size):\n",
    "        batch_end = min(batch_start + batch_size, len(dataset))\n",
    "        batch_dataset = dataset[batch_start:batch_end]\n",
    "        batch_num = (batch_start // batch_size) + 1\n",
    "        total_batches = (len(dataset) + batch_size - 1) // batch_size\n",
    "        \n",
    "        print(f\"\\n📦 处理批次 {batch_num}/{total_batches} (词汇 {batch_start+1}-{batch_end})...\")\n",
    "        \n",
    "        batch_results = []\n",
    "        game_counter = batch_start * len(models) * len(models)\n",
    "        \n",
    "        for word_data in batch_dataset:\n",
    "            target_word = word_data['target']\n",
    "            taboo_words = word_data['taboo']\n",
    "            \n",
    "            print(f\"\\n🎯 词汇: {target_word} ({word_data['part_of_speech']})\\\")\")\n",
    "            print(f\"🚫 禁用词: {taboo_words}\")\n",
    "            \n",
    "            for hinter_model in models:\n",
    "                for guesser_model in models:\n",
    "                    game_counter += 1\n",
    "                    hinter_name = hinter_model.split('/')[-1]\n",
    "                    guesser_name = guesser_model.split('/')[-1]\n",
    "                    \n",
    "                    print(f\"  🔄 游戏 {game_counter}: {hinter_name} → {guesser_name}\")\n",
    "                    \n",
    "                    try:\\n\",\n",
    "                        start_time = time.time()\n",
    "                        \n",
    "                        # 执行游戏\n",
    "                        game_result = play_chinese_taboo_game(\n",
    "                            client, hinter_model, guesser_model, \n",
    "                            target_word, taboo_words, max_turns=5\n",
    "                        )\n",
    "                        \n",
    "                        duration = round(time.time() - start_time, 2)\n",
    "                        \n",
    "                        # 构建详细结果记录\n",
    "                        result = {\n",
    "                            'experiment_id': experiment_id,\n",
    "                            'batch_num': batch_num,\n",
    "                            'game_id': game_counter,\n",
    "                            'target_word': target_word,\n",
    "                            'part_of_speech': word_data['part_of_speech'],\n",
    "                            'category': word_data['category'],\n",
    "                            'taboo_words': '|'.join(taboo_words),\n",
    "                            'hinter_model': hinter_model,\n",
    "                            'guesser_model': guesser_model,\n",
    "                            'model_pair': f\\\"{hinter_name}_vs_{guesser_name}\\\",\n",
    "                            'success': game_result['success'],\\n\",\n",
    "                            'turns_used': game_result['turns'],\n",
    "                            'final_guess': game_result['final_guess'],\n",
    "                            'failure_reason': game_result.get('failure_reason', None),\n",
    "                            'taboo_violation_turn': game_result.get('taboo_violation_turn', None),\n",
    "                            'taboo_violation_hint': game_result.get('taboo_violation_hint', None),\n",
    "                            'has_taboo_violation': game_result.get('failure_reason') == 'TABOO_VIOLATION',\n",
    "                            'has_format_errors': len(game_result.get('format_errors', [])) > 0,\n",
    "                            'all_hints': ' | '.join(game_result.get('all_hints', [])),\n",
    "                            'all_guesses': ' | '.join(game_result.get('all_guesses', [])),\n",
    "                            'conversation': ' | '.join(game_result.get('conversation', [])),\n",
    "                            'total_api_attempts': game_result.get('total_hinter_attempts', 0) + game_result.get('total_guesser_attempts', 0),\n",
    "                            'hinter_attempts': game_result.get('total_hinter_attempts', 0),\n",
    "                            'guesser_attempts': game_result.get('total_guesser_attempts', 0),\n",
    "                            'format_errors': ' | '.join(game_result.get('format_errors', [])),\n",
    "                            'hinter_failed_outputs': ' | '.join(game_result.get('hinter_failed_outputs', [])),\n",
    "                            'guesser_failed_outputs': ' | '.join(game_result.get('guesser_failed_outputs', [])),\n",
    "                            'duration_seconds': duration,\n",
    "                            'timestamp': datetime.now().strftime(\\\"%Y-%m-%d %H:%M:%S\\\"),\n",
    "                            'language': 'chinese',\n",
    "                            'dataset_source': word_data.get('metadata', {}).get('source', 'unknown')\n",
    "                        }\n",
    "                        \n",
    "                        if 'error' in game_result:\n",
    "                            result['error'] = game_result['error']\n",
    "                        \n",
    "                        batch_results.append(result)\n",
    "                        \n",
    "                        # 显示结果\n",
    "                        status = \\\"✅ 成功\\\" if game_result['success'] else \\\"❌ 失败\\\"\n",
    "                        extra_info = \\\"\\\"\n",
    "                        if not game_result['success']:\n",
    "                            reason = game_result.get('failure_reason', 'unknown')\n",
    "                            if reason == 'TABOO_VIOLATION':\n",
    "                                extra_info = \\\" (违规)\\\"\n",
    "                            elif reason == 'FORMAT_FAILURE':\n",
    "                                extra_info = \\\" (格式)\\\"\n",
    "                            elif reason == 'MAX_TURNS_EXCEEDED':\n",
    "                                extra_info = \\\" (轮数)\\\"\n",
    "                        \n",
    "                        print(f\\\"     {status}{extra_info} | {game_result['turns']}轮 | {duration}s | {game_result['final_guess']}\\\")\\n\",\n",
    "                        \n",
    "                    except Exception as e:\\n\",\n",
    "                        print(f\\\"     ❌ 执行异常: {str(e)[:50]}...\\\")\n",
    "                        # 记录异常\n",
    "                        error_result = {\\n\",\n",
    "                            'experiment_id': experiment_id,\n",
    "                            'batch_num': batch_num,\n",
    "                            'game_id': game_counter,\n",
    "                            'target_word': target_word,\n",
    "                            'hinter_model': hinter_model,\\n\",\n",
    "                            'guesser_model': guesser_model,\\n\",\n",
    "                            'success': False,\\n\",\n",
    "                            'failure_reason': 'EXCEPTION',\\n\",\n",
    "                            'error': str(e),\\n\",\n",
    "                            'timestamp': datetime.now().strftime(\\\"%Y-%m-%d %H:%M:%S\\\"),\\n\",\n",
    "                            'language': 'chinese'\\n\",\n",
    "                        }\\n\",\n",
    "                        batch_results.append(error_result)\\n\",\n",
    "                    \\n\",\n",
    "                    time.sleep(0.3)  # API调用间隔\\n\",\n",
    "        \\n\",\n",
    "        # 保存批次结果\\n\",\n",
    "        batch_df = pd.DataFrame(batch_results)\\n\",\n",
    "        batch_file = f\\\"{results_dir}/batch_{batch_num:03d}.csv\\\"\\n\",\n",
    "        batch_df.to_csv(batch_file, index=False, encoding='utf-8-sig')\\n\",\n",
    "        print(f\\\"💾 批次 {batch_num} 结果已保存: {batch_file}\\\")\\n\",\n",
    "        \\n\",\n",
    "        all_results.extend(batch_results)\\n\",\n",
    "        \\n\",\n",
    "        # 显示批次统计\\n\",\n",
    "        batch_success = len([r for r in batch_results if r.get('success', False)])\\n\",\n",
    "        batch_total = len(batch_results)\\n\",\n",
    "        print(f\\\"📊 批次 {batch_num} 成功率: {batch_success}/{batch_total} ({batch_success/batch_total*100:.1f}%)\\\")\\n\",\n",
    "    \\n\",\n",
    "    # 保存完整结果\\n\",\n",
    "    complete_df = pd.DataFrame(all_results)\\n\",\n",
    "    complete_file = f\\\"{results_dir}/complete_experiment_results.csv\\\"\\n\",\n",
    "    complete_df.to_csv(complete_file, index=False, encoding='utf-8-sig')\\n\",\n",
    "    \\n\",\n",
    "    print(f\\\"\\\\n🎉 完整实验完成！\\\")\\n\",\n",
    "    print(f\\\"📁 结果目录: {results_dir}\\\")\\n\",\n",
    "    print(f\\\"📊 总游戏数: {len(all_results)}\\\")\\n\",\n",
    "    \\n\",\n",
    "    # 生成实验报告\\n\",\n",
    "    generate_experiment_report(all_results, results_dir, experiment_id)\\n\",\n",
    "    \\n\",\n",
    "    return all_results\\n\",\n",
    "\n",
    "def generate_experiment_report(results, results_dir, experiment_id):\n",
    "    \\\"\\\"\\\"生成详细的实验报告\\\"\\\"\\\"\n",
    "    print(f\\\"\\\\n📋 生成实验报告...\\\")\\n\",\n",
    "    \\n\",\n",
    "    total_games = len(results)\\n\",\n",
    "    successful_games = [r for r in results if r.get('success', False)]\\n\",\n",
    "    success_rate = len(successful_games) / total_games * 100 if total_games > 0 else 0\\n\",\n",
    "    \\n\",\n",
    "    # 按模型统计\\n\",\n",
    "    models_used = list(set([r.get('hinter_model', 'unknown') for r in results if 'hinter_model' in r]))\\n\",\n",
    "    model_stats = {}\\n\",\n",
    "    \\n\",\n",
    "    for model in models_used:\\n\",\n",
    "        model_name = model.split('/')[-1] if '/' in model else model\\n\",\n",
    "        \\n\",\n",
    "        # 作为hinter的表现\\n\",\n",
    "        as_hinter = [r for r in results if r.get('hinter_model') == model]\\n\",\n",
    "        hinter_success = len([r for r in as_hinter if r.get('success', False)])\\n\",\n",
    "        \\n\",\n",
    "        # 作为guesser的表现\\n\",\n",
    "        as_guesser = [r for r in results if r.get('guesser_model') == model]\\n\",\n",
    "        guesser_success = len([r for r in as_guesser if r.get('success', False)])\\n\",\n",
    "        \\n\",\n",
    "        model_stats[model_name] = {\\n\",\n",
    "            'as_hinter': {'total': len(as_hinter), 'success': hinter_success, 'rate': hinter_success/len(as_hinter)*100 if as_hinter else 0},\\n\",\n",
    "            'as_guesser': {'total': len(as_guesser), 'success': guesser_success, 'rate': guesser_success/len(as_guesser)*100 if as_guesser else 0}\\n\",\n",
    "        }\\n\",\n",
    "    \\n\",\n",
    "    # 失败原因统计\\n\",\n",
    "    failure_reasons = {}\\n\",\n",
    "    failed_games = [r for r in results if not r.get('success', True)]\\n\",\n",
    "    for game in failed_games:\\n\",\n",
    "        reason = game.get('failure_reason', 'unknown')\\n\",\n",
    "        failure_reasons[reason] = failure_reasons.get(reason, 0) + 1\\n\",\n",
    "    \\n\",\n",
    "    # 词性表现\\n\",\n",
    "    pos_stats = {}\\n\",\n",
    "    for result in results:\\n\",\n",
    "        pos = result.get('part_of_speech', 'unknown')\\n\",\n",
    "        if pos not in pos_stats:\\n\",\n",
    "            pos_stats[pos] = {'total': 0, 'success': 0}\\n\",\n",
    "        pos_stats[pos]['total'] += 1\\n\",\n",
    "        if result.get('success', False):\\n\",\n",
    "            pos_stats[pos]['success'] += 1\\n\",\n",
    "    \\n\",\n",
    "    # 生成报告\\n\",\n",
    "    report = {\\n\",\n",
    "        'experiment_info': {\\n\",\n",
    "            'experiment_id': experiment_id,\\n\",\n",
    "            'total_games': total_games,\\n\",\n",
    "            'successful_games': len(successful_games),\\n\",\n",
    "            'success_rate': round(success_rate, 2),\\n\",\n",
    "            'completion_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\\n\",\n",
    "            'models_tested': len(models_used),\\n\",\n",
    "            'language': 'chinese'\\n\",\n",
    "        },\\n\",\n",
    "        'model_performance': model_stats,\\n\",\n",
    "        'failure_analysis': failure_reasons,\\n\",\n",
    "        'pos_performance': {pos: {'total': stats['total'], 'success': stats['success'], 'rate': round(stats['success']/stats['total']*100, 1)} for pos, stats in pos_stats.items()}\\n\",\n",
    "    }\\n\",\n",
    "    \\n\",\n",
    "    # 保存报告\\n\",\n",
    "    report_file = f\\\"{results_dir}/experiment_report.json\\\"\\n\",\n",
    "    with open(report_file, 'w', encoding='utf-8') as f:\\n\",\n",
    "        json.dump(report, f, ensure_ascii=False, indent=2)\\n\",\n",
    "    \\n\",\n",
    "    # 显示报告摘要\\n\",\n",
    "    print(f\\\"\\\\n📈 实验结果摘要:\\\")\\n\",\n",
    "    print(f\\\"   总游戏数: {total_games}\\\")\\n\",\n",
    "    print(f\\\"   成功游戏: {len(successful_games)}\\\")\\n\",\n",
    "    print(f\\\"   整体成功率: {success_rate:.1f}%\\\")\\n\",\n",
    "    \\n\",\n",
    "    print(f\\\"\\\\n🤖 模型表现:\\\")\\n\",\n",
    "    for model_name, stats in model_stats.items():\\n\",\n",
    "        print(f\\\"   {model_name}:\\\")\\n\",\n",
    "        print(f\\\"     作为提示者: {stats['as_hinter']['success']}/{stats['as_hinter']['total']} ({stats['as_hinter']['rate']:.1f}%)\\\")\\n\",\n",
    "        print(f\\\"     作为猜测者: {stats['as_guesser']['success']}/{stats['as_guesser']['total']} ({stats['as_guesser']['rate']:.1f}%)\\\")\\n\",\n",
    "    \\n\",\n",
    "    if failure_reasons:\\n\",\n",
    "        print(f\\\"\\\\n❌ 失败原因分析:\\\")\\n\",\n",
    "        for reason, count in failure_reasons.items():\\n\",\n",
    "            print(f\\\"   {reason}: {count} 次 ({count/len(failed_games)*100:.1f}%)\\\")\\n\",\n",
    "    \\n\",\n",
    "    print(f\\\"\\\\n📝 报告已保存: {report_file}\\\")\\n\",\n",
    "\n",
    "print(\\\"✅ 完整中文Taboo实验系统已定义\\\")\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 保存中文Taboo数据集...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type Sense is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m chinese_dataset_path = os.path.join(data_dir, \u001b[33m\"\u001b[39m\u001b[33mchinese_dataset.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(chinese_dataset_path, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchinese_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ 完整数据集已保存: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchinese_dataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# 创建简化版数据集（用于快速测试）\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py:179\u001b[39m, in \u001b[36mdump\u001b[39m\u001b[34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[39m\n\u001b[32m    173\u001b[39m     iterable = \u001b[38;5;28mcls\u001b[39m(skipkeys=skipkeys, ensure_ascii=ensure_ascii,\n\u001b[32m    174\u001b[39m         check_circular=check_circular, allow_nan=allow_nan, indent=indent,\n\u001b[32m    175\u001b[39m         separators=separators,\n\u001b[32m    176\u001b[39m         default=default, sort_keys=sort_keys, **kw).iterencode(obj)\n\u001b[32m    177\u001b[39m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:430\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode\u001b[39m\u001b[34m(o, _current_indent_level)\u001b[39m\n\u001b[32m    428\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m _floatstr(o)\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    432\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:326\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_list\u001b[39m\u001b[34m(lst, _current_indent_level)\u001b[39m\n\u001b[32m    324\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    325\u001b[39m             chunks = _iterencode(value, _current_indent_level)\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    328\u001b[39m     _current_indent_level -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:406\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_dict\u001b[39m\u001b[34m(dct, _current_indent_level)\u001b[39m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    405\u001b[39m             chunks = _iterencode(value, _current_indent_level)\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    408\u001b[39m     _current_indent_level -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:326\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_list\u001b[39m\u001b[34m(lst, _current_indent_level)\u001b[39m\n\u001b[32m    324\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    325\u001b[39m             chunks = _iterencode(value, _current_indent_level)\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    328\u001b[39m     _current_indent_level -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:439\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode\u001b[39m\u001b[34m(o, _current_indent_level)\u001b[39m\n\u001b[32m    437\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCircular reference detected\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    438\u001b[39m     markers[markerid] = o\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m o = \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.11/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:180\u001b[39m, in \u001b[36mJSONEncoder.default\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[32m    162\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[33;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[33;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    178\u001b[39m \n\u001b[32m    179\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    181\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mis not JSON serializable\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Object of type Sense is not JSON serializable"
     ]
    }
   ],
   "source": [
    "# 6. 保存中文数据集\n",
    "print(\"💾 保存中文Taboo数据集...\")\n",
    "\n",
    "# 创建数据目录\n",
    "data_dir = \"data\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "    print(f\"📁 创建数据目录: {data_dir}\")\n",
    "\n",
    "# 保存完整数据集\n",
    "chinese_dataset_path = os.path.join(data_dir, \"chinese_dataset.json\")\n",
    "with open(chinese_dataset_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(chinese_dataset, f, ensure_ascii=False, indent=2)\n",
    "print(f\"✅ 完整数据集已保存: {chinese_dataset_path}\")\n",
    "\n",
    "# 创建简化版数据集（用于快速测试）\n",
    "simplified_dataset = []\n",
    "for item in chinese_dataset:\n",
    "    simplified_item = {\n",
    "        'target': item['target'],\n",
    "        'part_of_speech': item['part_of_speech'],\n",
    "        'taboo': item['taboo'],\n",
    "        'category': item['category']\n",
    "    }\n",
    "    simplified_dataset.append(simplified_item)\n",
    "\n",
    "simplified_path = os.path.join(data_dir, \"chinese_dataset_simple.json\")\n",
    "with open(simplified_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(simplified_dataset, f, ensure_ascii=False, indent=2)\n",
    "print(f\"✅ 简化数据集已保存: {simplified_path}\")\n",
    "\n",
    "# 创建安全的样本数据（移除可能不可序列化的内容）\n",
    "safe_sample_items = []\n",
    "for item in sample_items:\n",
    "    safe_item = {\n",
    "        'target': item['target'],\n",
    "        'part_of_speech': item['part_of_speech'],\n",
    "        'taboo': item['taboo'],\n",
    "        'category': item['category'],\n",
    "        'sense_count': len(item.get('senses', [])),\n",
    "        'first_definition': item['senses'][0].get('Def', '无定义')[:100] if item.get('senses') else '无定义'\n",
    "    }\n",
    "    safe_sample_items.append(safe_item)\n",
    "\n",
    "# 生成数据集报告\n",
    "report = {\n",
    "    'dataset_info': {\n",
    "        'total_words': len(chinese_dataset),\n",
    "        'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'source': 'OpenHowNet',\n",
    "        'language': 'Chinese',\n",
    "        'pos_distribution': pos_counts,\n",
    "        'avg_taboo_count': sum(taboo_counts) / len(taboo_counts),\n",
    "        'avg_sense_count': sum(sense_counts) / len(sense_counts)\n",
    "    },\n",
    "    'sample_data': safe_sample_items\n",
    "}\n",
    "\n",
    "report_path = os.path.join(data_dir, \"chinese_dataset_report.json\")\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "print(f\"✅ 数据集报告已保存: {report_path}\")\n",
    "\n",
    "print(f\"\\n🎉 中文Taboo数据集构建完成！\")\n",
    "print(f\"📁 数据文件位置:\")\n",
    "print(f\"   完整版: {chinese_dataset_path}\")\n",
    "print(f\"   简化版: {simplified_path}\")\n",
    "print(f\"   报告: {report_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. API客户端设置（支持中文模型）\n",
    "print(\"🔧 设置中文Taboo实验API客户端...\")\n",
    "\n",
    "def load_api_keys(keys_path: str = \"api_keys.json\") -> Dict[str, str]:\n",
    "    \"\"\"加载API密钥\"\"\"\n",
    "    with open(keys_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "class ChineseTabooClient:\n",
    "    \"\"\"中文Taboo游戏专用API客户端\"\"\"\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "        self.headers = {\n",
    "            \"Authorization\": f\"Bearer {api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "    \n",
    "    def call_model(self, model: str, messages: List[Dict[str, str]], temperature: float = 0.3) -> str:\n",
    "        \"\"\"调用模型API\"\"\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": 2000\n",
    "        }\n",
    "        \n",
    "        response = requests.post(self.base_url, headers=self.headers, json=payload, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        content = result['choices'][0]['message']['content'].strip()\n",
    "        \n",
    "        return content\n",
    "\n",
    "# 初始化API客户端\n",
    "try:\n",
    "    api_keys = load_api_keys()\n",
    "    chinese_client = ChineseTabooClient(api_keys[\"OPENROUTER_API_KEY\"])\n",
    "    print(\"✅ 中文Taboo API客户端初始化成功\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ API客户端初始化失败: {e}\")\n",
    "    chinese_client = None\n",
    "\n",
    "# 定义支持中文的测试模型\n",
    "CHINESE_TEST_MODELS = [\n",
    "    \"openai/gpt-4o\",  # GPT-4o 支持中文\n",
    "    \"google/gemini-2.5-flash\",  # Gemini 支持中文\n",
    "    \"deepseek/deepseek-chat-v3-0324\",  # DeepSeek 中文模型\n",
    "    \"anthropic/claude-sonnet-4\",  # Claude 支持中文\n",
    "    \"moonshotai/kimi-k2\",  # kimi-k2 的API路径\n",
    "]\n",
    "\n",
    "# 中文Taboo游戏逻辑\n",
    "def play_chinese_taboo_game(client, hinter_model, guesser_model, target_word, taboo_words, max_turns=5):\n",
    "    \"\"\"执行一场中文Taboo游戏\"\"\"\n",
    "    \n",
    "    conversation = []\n",
    "    all_hints = []\n",
    "    all_guesses = []\n",
    "    format_errors = []\n",
    "    hinter_failed_outputs = []\n",
    "    guesser_failed_outputs = []\n",
    "    total_hinter_attempts = 0\n",
    "    total_guesser_attempts = 0\n",
    "    \n",
    "    try:\n",
    "        for turn in range(1, max_turns + 1):\n",
    "            # Hinter给出提示\n",
    "            hinter_prompt = f\"\"\"你正在玩中文Taboo游戏。你需要用中文描述一个词汇，让对方猜出这个词。\n",
    "\n",
    "目标词汇: {target_word}\n",
    "禁用词汇: {', '.join(taboo_words)}\n",
    "\n",
    "规则:\n",
    "1. 你必须用中文描述目标词汇\n",
    "2. 你不能直接说出目标词汇或任何禁用词汇\n",
    "3. 你的回答只能包含描述文字，不要包含其他内容\n",
    "4. 保持描述简洁但有帮助\n",
    "\n",
    "请给出你的描述:\"\"\"\n",
    "\n",
    "            # 获取hinter的提示\n",
    "            hint = None\n",
    "            hinter_attempts = 0\n",
    "            while hint is None and hinter_attempts < 3:\n",
    "                hinter_attempts += 1\n",
    "                total_hinter_attempts += 1\n",
    "                try:\n",
    "                    hint_response = client.call_model(hinter_model, [{\"role\": \"user\", \"content\": hinter_prompt}])\n",
    "                    hint = hint_response.strip()\n",
    "                    \n",
    "                    # 检查是否违反禁用词\n",
    "                    hint_lower = hint.lower()\n",
    "                    target_lower = target_word.lower()\n",
    "                    \n",
    "                    if target_lower in hint_lower:\n",
    "                        return {\n",
    "                            'success': False,\n",
    "                            'failure_reason': 'TABOO_VIOLATION',\n",
    "                            'turns': turn,\n",
    "                            'final_guess': '',\n",
    "                            'taboo_violation_turn': turn,\n",
    "                            'taboo_violation_hint': hint,\n",
    "                            'conversation': conversation,\n",
    "                            'all_hints': all_hints,\n",
    "                            'all_guesses': all_guesses,\n",
    "                            'format_errors': format_errors,\n",
    "                            'hinter_failed_outputs': hinter_failed_outputs,\n",
    "                            'guesser_failed_outputs': guesser_failed_outputs,\n",
    "                            'total_hinter_attempts': total_hinter_attempts,\n",
    "                            'total_guesser_attempts': total_guesser_attempts\n",
    "                        }\n",
    "                    \n",
    "                    for taboo_word in taboo_words:\n",
    "                        if taboo_word.lower() in hint_lower:\n",
    "                            return {\n",
    "                                'success': False,\n",
    "                                'failure_reason': 'TABOO_VIOLATION',\n",
    "                                'turns': turn,\n",
    "                                'final_guess': '',\n",
    "                                'taboo_violation_turn': turn,\n",
    "                                'taboo_violation_hint': hint,\n",
    "                                'conversation': conversation,\n",
    "                                'all_hints': all_hints,\n",
    "                                'all_guesses': all_guesses,\n",
    "                                'format_errors': format_errors,\n",
    "                                'hinter_failed_outputs': hinter_failed_outputs,\n",
    "                                'guesser_failed_outputs': guesser_failed_outputs,\n",
    "                                'total_hinter_attempts': total_hinter_attempts,\n",
    "                                'total_guesser_attempts': total_guesser_attempts\n",
    "                            }\n",
    "                    \n",
    "                    break\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    if hinter_attempts == 3:\n",
    "                        return {\n",
    "                            'success': False,\n",
    "                            'failure_reason': 'API_FAILURE',\n",
    "                            'turns': turn,\n",
    "                            'final_guess': '',\n",
    "                            'error': f\"Hinter API failure: {e}\",\n",
    "                            'conversation': conversation,\n",
    "                            'all_hints': all_hints,\n",
    "                            'all_guesses': all_guesses,\n",
    "                            'format_errors': format_errors,\n",
    "                            'hinter_failed_outputs': hinter_failed_outputs,\n",
    "                            'guesser_failed_outputs': guesser_failed_outputs,\n",
    "                            'total_hinter_attempts': total_hinter_attempts,\n",
    "                            'total_guesser_attempts': total_guesser_attempts\n",
    "                        }\n",
    "                    hinter_failed_outputs.append(str(e))\n",
    "                    time.sleep(1)\n",
    "            \n",
    "            if hint is None:\n",
    "                return {\n",
    "                    'success': False,\n",
    "                    'failure_reason': 'FORMAT_FAILURE',\n",
    "                    'turns': turn,\n",
    "                    'final_guess': '',\n",
    "                    'conversation': conversation,\n",
    "                    'all_hints': all_hints,\n",
    "                    'all_guesses': all_guesses,\n",
    "                    'format_errors': format_errors,\n",
    "                    'hinter_failed_outputs': hinter_failed_outputs,\n",
    "                    'guesser_failed_outputs': guesser_failed_outputs,\n",
    "                    'total_hinter_attempts': total_hinter_attempts,\n",
    "                    'total_guesser_attempts': total_guesser_attempts\n",
    "                }\n",
    "            \n",
    "            all_hints.append(hint)\n",
    "            conversation.append(f\"第{turn}轮 提示: {hint}\")\n",
    "            \n",
    "            # Guesser猜测\n",
    "            conversation_history = \"\\n\".join(conversation)\n",
    "            guesser_prompt = f\"\"\"你正在玩中文Taboo游戏。根据对方的描述，你需要猜出他想让你猜的词汇。\n",
    "\n",
    "游戏历史:\n",
    "{conversation_history}\n",
    "\n",
    "请根据以上描述猜测目标词汇。你的回答只需要包含你猜测的词汇，不要包含其他解释。\n",
    "\n",
    "你的猜测:\"\"\"\n",
    "\n",
    "            # 获取guesser的猜测\n",
    "            guess = None\n",
    "            guesser_attempts = 0\n",
    "            while guess is None and guesser_attempts < 3:\n",
    "                guesser_attempts += 1\n",
    "                total_guesser_attempts += 1\n",
    "                try:\n",
    "                    guess_response = client.call_model(guesser_model, [{\"role\": \"user\", \"content\": guesser_prompt}])\n",
    "                    guess = guess_response.strip()\n",
    "                    \n",
    "                    # 简单的格式检查\n",
    "                    if len(guess) > 50:  # 回答太长，可能包含解释\n",
    "                        # 尝试提取最可能的词汇\n",
    "                        words = jieba.lcut(guess)\n",
    "                        chinese_words = [w for w in words if is_valid_chinese_word(w)]\n",
    "                        if chinese_words:\n",
    "                            guess = chinese_words[0]\n",
    "                        else:\n",
    "                            guess = guess[:10]  # 截取前10个字符\n",
    "                    \n",
    "                    break\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    if guesser_attempts == 3:\n",
    "                        return {\n",
    "                            'success': False,\n",
    "                            'failure_reason': 'API_FAILURE',\n",
    "                            'turns': turn,\n",
    "                            'final_guess': '',\n",
    "                            'error': f\"Guesser API failure: {e}\",\n",
    "                            'conversation': conversation,\n",
    "                            'all_hints': all_hints,\n",
    "                            'all_guesses': all_guesses,\n",
    "                            'format_errors': format_errors,\n",
    "                            'hinter_failed_outputs': hinter_failed_outputs,\n",
    "                            'guesser_failed_outputs': guesser_failed_outputs,\n",
    "                            'total_hinter_attempts': total_hinter_attempts,\n",
    "                            'total_guesser_attempts': total_guesser_attempts\n",
    "                        }\n",
    "                    guesser_failed_outputs.append(str(e))\n",
    "                    time.sleep(1)\n",
    "            \n",
    "            if guess is None:\n",
    "                return {\n",
    "                    'success': False,\n",
    "                    'failure_reason': 'FORMAT_FAILURE',\n",
    "                    'turns': turn,\n",
    "                    'final_guess': '',\n",
    "                    'conversation': conversation,\n",
    "                    'all_hints': all_hints,\n",
    "                    'all_guesses': all_guesses,\n",
    "                    'format_errors': format_errors,\n",
    "                    'hinter_failed_outputs': hinter_failed_outputs,\n",
    "                    'guesser_failed_outputs': guesser_failed_outputs,\n",
    "                    'total_hinter_attempts': total_hinter_attempts,\n",
    "                    'total_guesser_attempts': total_guesser_attempts\n",
    "                }\n",
    "            \n",
    "            all_guesses.append(guess)\n",
    "            conversation.append(f\"第{turn}轮 猜测: {guess}\")\n",
    "            \n",
    "            # 检查是否猜对\n",
    "            if guess.lower().strip() == target_word.lower().strip():\n",
    "                return {\n",
    "                    'success': True,\n",
    "                    'turns': turn,\n",
    "                    'final_guess': guess,\n",
    "                    'conversation': conversation,\n",
    "                    'all_hints': all_hints,\n",
    "                    'all_guesses': all_guesses,\n",
    "                    'format_errors': format_errors,\n",
    "                    'hinter_failed_outputs': hinter_failed_outputs,\n",
    "                    'guesser_failed_outputs': guesser_failed_outputs,\n",
    "                    'total_hinter_attempts': total_hinter_attempts,\n",
    "                    'total_guesser_attempts': total_guesser_attempts\n",
    "                }\n",
    "        \n",
    "        # 如果所有轮次都用完了还没猜对\n",
    "        return {\n",
    "            'success': False,\n",
    "            'failure_reason': 'MAX_TURNS_EXCEEDED',\n",
    "            'turns': max_turns,\n",
    "            'final_guess': all_guesses[-1] if all_guesses else '',\n",
    "            'conversation': conversation,\n",
    "            'all_hints': all_hints,\n",
    "            'all_guesses': all_guesses,\n",
    "            'format_errors': format_errors,\n",
    "            'hinter_failed_outputs': hinter_failed_outputs,\n",
    "            'guesser_failed_outputs': guesser_failed_outputs,\n",
    "            'total_hinter_attempts': total_hinter_attempts,\n",
    "            'total_guesser_attempts': total_guesser_attempts\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'failure_reason': 'EXCEPTION',\n",
    "            'turns': 0,\n",
    "            'final_guess': '',\n",
    "            'error': str(e),\n",
    "            'conversation': conversation,\n",
    "            'all_hints': all_hints,\n",
    "            'all_guesses': all_guesses,\n",
    "            'format_errors': format_errors,\n",
    "            'hinter_failed_outputs': hinter_failed_outputs,\n",
    "            'guesser_failed_outputs': guesser_failed_outputs,\n",
    "            'total_hinter_attempts': total_hinter_attempts,\n",
    "            'total_guesser_attempts': total_guesser_attempts\n",
    "        }\n",
    "\n",
    "print(\"✅ 中文Taboo游戏逻辑已定义\")\n",
    "\n",
    "print(f\"🤖 中文实验模型列表 ({len(CHINESE_TEST_MODELS)} 个):\")\n",
    "for i, model in enumerate(CHINESE_TEST_MODELS, 1):\n",
    "    print(f\"   {i}. {model}\")\n",
    "\n",
    "print(f\"\\n💡 选择较少模型进行测试以节省成本和时间\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. 运行完整中文Taboo实验\n",
    "print(\"🚀 开始运行完整中文Taboo实验...\")\n",
    "\n",
    "# 选择实验规模\n",
    "experiment_scales = {\n",
    "    'quick': {'count': 5, 'desc': '快速测试（5个词汇）'},\n",
    "    'medium': {'count': 20, 'desc': '中等规模（20个词汇）'},\n",
    "    'full': {'count': 100, 'desc': '完整实验（100个词汇）'}\n",
    "}\n",
    "\n",
    "# 设置实验规模 - 可以修改这里选择不同规模\n",
    "EXPERIMENT_SCALE = 'quick'  # 改为 'medium' 或 'full' 来运行更大规模实验\n",
    "\n",
    "scale_config = experiment_scales[EXPERIMENT_SCALE]\n",
    "experiment_dataset = chinese_dataset[:scale_config['count']]\n",
    "\n",
    "print(f\"📊 实验配置: {scale_config['desc']}\")\n",
    "print(f\"🎯 词汇数量: {len(experiment_dataset)}\")\n",
    "print(f\"🤖 模型数量: {len(CHINESE_TEST_MODELS)}\")\n",
    "print(f\"🎮 总游戏数: {len(experiment_dataset) * len(CHINESE_TEST_MODELS) * len(CHINESE_TEST_MODELS)}\")\n",
    "\n",
    "if chinese_client and chinese_dataset:\n",
    "    try:\n",
    "        # 运行实验\n",
    "        experiment_results = run_full_chinese_experiment(\n",
    "            chinese_client, \n",
    "            CHINESE_TEST_MODELS, \n",
    "            experiment_dataset,\n",
    "            f\"chinese_taboo_{EXPERIMENT_SCALE}\"\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n🎉 {scale_config['desc']}完成！\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 实验执行失败: {e}\")\n",
    "        print(\"💡 请检查API密钥和网络连接\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ 无法运行实验：API客户端或数据集未准备就绪\")\n",
    "    if not chinese_client:\n",
    "        print(\"   - API客户端未初始化\")\n",
    "    if not chinese_dataset:\n",
    "        print(\"   - 数据集未构建\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 启动中文Taboo完整实验\n",
    "print(\"🚀 启动中文Taboo完整实验...\")\n",
    "\n",
    "# 实验选项：用户可以选择实验规模\n",
    "EXPERIMENT_OPTIONS = {\n",
    "    'quick': {'size': 5, 'name': '快速测试'},\n",
    "    'medium': {'size': 20, 'name': '中等规模'}, \n",
    "    'full': {'size': 100, 'name': '完整实验'}\n",
    "}\n",
    "\n",
    "# 默认选择快速测试\n",
    "selected_option = 'quick'\n",
    "selected_size = EXPERIMENT_OPTIONS[selected_option]['size']\n",
    "\n",
    "print(f\"📊 实验规模: {EXPERIMENT_OPTIONS[selected_option]['name']} ({selected_size} 个词汇)\")\n",
    "print(f\"🤖 测试模型: {len(CHINESE_TEST_MODELS)} 个\")\n",
    "print(f\"🎯 预计游戏数: {selected_size * len(CHINESE_TEST_MODELS) * len(CHINESE_TEST_MODELS)}\")\n",
    "\n",
    "# 检查必要组件\n",
    "if 'chinese_client' in globals() and chinese_client and 'chinese_dataset' in globals() and chinese_dataset:\n",
    "    print(\"✅ 所有组件已就绪，可以开始实验\")\n",
    "    \n",
    "    # 选择数据集子集\n",
    "    test_dataset = chinese_dataset[:selected_size]\n",
    "    \n",
    "    print(f\"\\\\n🎯 选择的测试词汇:\")\n",
    "    for i, item in enumerate(test_dataset[:5], 1):  # 显示前5个\n",
    "        print(f\"   {i}. {item['target']} ({item['part_of_speech']}) - 禁用词: {item['taboo']}\")\n",
    "    if len(test_dataset) > 5:\n",
    "        print(f\"   ... 还有 {len(test_dataset) - 5} 个词汇\")\n",
    "    \n",
    "    print(f\"\\\\n💡 要执行完整实验，请运行下一个cell\")\n",
    "    print(f\"💡 要修改实验规模，请修改上面的 selected_option 变量\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ 缺少必要组件:\")\n",
    "    if 'chinese_client' not in globals() or not chinese_client:\n",
    "        print(\"   - API客户端未初始化\")\n",
    "    if 'chinese_dataset' not in globals() or not chinese_dataset:\n",
    "        print(\"   - 中文数据集未准备好\")\n",
    "    print(\"💡 请先运行前面的cell来初始化这些组件\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行中文Taboo完整实验\n",
    "print(\"🎮 执行中文Taboo完整实验...\")\n",
    "\n",
    "def execute_chinese_experiment(dataset, models, client):\n",
    "    \"\"\"执行中文Taboo实验\"\"\"\n",
    "    \n",
    "    # 创建结果目录\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    results_dir = f\"results/chinese_experiment_{timestamp}\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"📁 结果保存目录: {results_dir}\")\n",
    "    \n",
    "    all_results = []\n",
    "    game_counter = 0\n",
    "    total_games = len(dataset) * len(models) * len(models)\n",
    "    \n",
    "    print(f\"🚀 开始执行 {total_games} 个游戏...\")\n",
    "    \n",
    "    for word_idx, word_data in enumerate(dataset, 1):\n",
    "        target_word = word_data['target']\n",
    "        taboo_words = word_data['taboo']\n",
    "        pos = word_data['part_of_speech']\n",
    "        \n",
    "        print(f\"\\\\n🎯 词汇 {word_idx}/{len(dataset)}: {target_word} ({pos})\")\n",
    "        print(f\"🚫 禁用词: {taboo_words}\")\n",
    "        \n",
    "        word_success = 0\n",
    "        word_total = 0\n",
    "        \n",
    "        for hinter_model in models:\n",
    "            for guesser_model in models:\n",
    "                game_counter += 1\n",
    "                word_total += 1\n",
    "                \n",
    "                hinter_name = hinter_model.split('/')[-1]\n",
    "                guesser_name = guesser_model.split('/')[-1]\n",
    "                \n",
    "                print(f\"  🔄 {game_counter}/{total_games}: {hinter_name}→{guesser_name}\", end=\" \")\n",
    "                \n",
    "                try:\n",
    "                    start_time = time.time()\n",
    "                    \n",
    "                    # 执行游戏\n",
    "                    game_result = play_chinese_taboo_game(\n",
    "                        client, hinter_model, guesser_model, \n",
    "                        target_word, taboo_words, max_turns=5\n",
    "                    )\n",
    "                    \n",
    "                    duration = round(time.time() - start_time, 2)\n",
    "                    \n",
    "                    # 记录结果\n",
    "                    result = {\n",
    "                        'game_id': game_counter,\n",
    "                        'word_index': word_idx,\n",
    "                        'target_word': target_word,\n",
    "                        'part_of_speech': pos,\n",
    "                        'taboo_words': '|'.join(taboo_words),\n",
    "                        'hinter_model': hinter_model,\n",
    "                        'guesser_model': guesser_model,\n",
    "                        'success': game_result['success'],\n",
    "                        'turns_used': game_result['turns'],\n",
    "                        'final_guess': game_result['final_guess'],\n",
    "                        'failure_reason': game_result.get('failure_reason', ''),\n",
    "                        'duration_seconds': duration,\n",
    "                        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    }\n",
    "                    \n",
    "                    all_results.append(result)\n",
    "                    \n",
    "                    if game_result['success']:\n",
    "                        word_success += 1\n",
    "                        print(f\"✅ {game_result['turns']}轮 {duration}s\")\n",
    "                    else:\n",
    "                        reason = game_result.get('failure_reason', 'unknown')\n",
    "                        if reason == 'TABOO_VIOLATION':\n",
    "                            print(f\"❌ 违规 {duration}s\")\n",
    "                        elif reason == 'MAX_TURNS_EXCEEDED':\n",
    "                            print(f\"❌ 轮数 {duration}s\")\n",
    "                        else:\n",
    "                            print(f\"❌ {reason[:10]} {duration}s\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"❌ 异常: {str(e)[:20]}...\")\n",
    "                    result = {\n",
    "                        'game_id': game_counter,\n",
    "                        'target_word': target_word,\n",
    "                        'hinter_model': hinter_model,\n",
    "                        'guesser_model': guesser_model,\n",
    "                        'success': False,\n",
    "                        'failure_reason': 'EXCEPTION',\n",
    "                        'error': str(e),\n",
    "                        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    }\n",
    "                    all_results.append(result)\n",
    "                \n",
    "                time.sleep(0.1)  # 避免API限制\n",
    "        \n",
    "        # 显示词汇小结\n",
    "        word_rate = word_success / word_total * 100 if word_total > 0 else 0\n",
    "        print(f\"  📊 '{target_word}' 成功率: {word_success}/{word_total} ({word_rate:.1f}%)\")\n",
    "    \n",
    "    # 保存完整结果\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_file = f\"{results_dir}/complete_results.csv\"\n",
    "    results_df.to_csv(results_file, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    # 生成报告\n",
    "    total_success = len([r for r in all_results if r.get('success', False)])\n",
    "    overall_rate = total_success / len(all_results) * 100 if all_results else 0\n",
    "    \n",
    "    print(f\"\\\\n🎉 实验完成！\")\n",
    "    print(f\"📊 整体结果:\")\n",
    "    print(f\"   总游戏数: {len(all_results)}\")\n",
    "    print(f\"   成功数: {total_success}\")\n",
    "    print(f\"   成功率: {overall_rate:.1f}%\")\n",
    "    \n",
    "    # 按模型统计\n",
    "    print(f\"\\\\n🤖 模型表现:\")\n",
    "    for model in models:\n",
    "        model_name = model.split('/')[-1]\n",
    "        \n",
    "        # 作为提示者\n",
    "        as_hinter = [r for r in all_results if r.get('hinter_model') == model]\n",
    "        hinter_success = len([r for r in as_hinter if r.get('success', False)])\n",
    "        hinter_rate = hinter_success / len(as_hinter) * 100 if as_hinter else 0\n",
    "        \n",
    "        # 作为猜测者  \n",
    "        as_guesser = [r for r in all_results if r.get('guesser_model') == model]\n",
    "        guesser_success = len([r for r in as_guesser if r.get('success', False)])\n",
    "        guesser_rate = guesser_success / len(as_guesser) * 100 if as_guesser else 0\n",
    "        \n",
    "        print(f\"   {model_name}:\")\n",
    "        print(f\"     提示者: {hinter_success}/{len(as_hinter)} ({hinter_rate:.1f}%)\")\n",
    "        print(f\"     猜测者: {guesser_success}/{len(as_guesser)} ({guesser_rate:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\\\n📁 详细结果已保存: {results_file}\")\n",
    "    return all_results, results_dir\n",
    "\n",
    "# 执行实验\n",
    "if 'test_dataset' in globals() and test_dataset and chinese_client:\n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"🧪 开始中文Taboo完整实验\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    experiment_results, results_directory = execute_chinese_experiment(\n",
    "        test_dataset, CHINESE_TEST_MODELS, chinese_client\n",
    "    )\n",
    "    \n",
    "    print(f\"\\\\n✅ 实验完成！结果保存在: {results_directory}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ 无法执行实验：\")\n",
    "    if 'test_dataset' not in globals():\n",
    "        print(\"   - 测试数据集未定义，请先运行上一个cell\")\n",
    "    if 'chinese_client' not in globals() or not chinese_client:\n",
    "        print(\"   - API客户端未初始化\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. 中文Taboo游戏核心逻辑和工具函数\n",
    "print(\"🎮 定义中文Taboo游戏核心逻辑...\")\n",
    "\n",
    "def safe_chinese_text_cleanup(text: str, max_length: int = 300) -> str:\n",
    "    \"\"\"安全清理中文文本\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # 保留中文字符、英文字符、数字和常用标点\n",
    "    import re\n",
    "    cleaned = re.sub(r'[^\\u4e00-\\u9fff\\w\\s\\.,!?;:\"\\'()[\\]{}\\-]', '', str(text))\n",
    "    \n",
    "    if len(cleaned) > max_length:\n",
    "        cleaned = cleaned[:max_length] + \"...\"\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def extract_chinese_clue_text(response: str) -> str:\n",
    "    \"\"\"从响应中提取中文线索文本\"\"\"\n",
    "    if \"FORMAT_ERROR_EXCEEDED\" in response:\n",
    "        return \"FORMAT_ERROR\"\n",
    "    \n",
    "    # 检查中文格式标记\n",
    "    if '[线索]' in response or '[CLUE]' in response.upper():\n",
    "        import re\n",
    "        # 优先匹配中文标记\n",
    "        match = re.search(r'\\[线索\\]\\s*(.+)', response, re.DOTALL)\n",
    "        if not match:\n",
    "            match = re.search(r'\\[CLUE\\]\\s*(.+)', response, re.IGNORECASE | re.DOTALL)\n",
    "        \n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "    \n",
    "    # 备用格式\n",
    "    if '线索:' in response or 'Clue:' in response:\n",
    "        if '线索:' in response:\n",
    "            return response.split('线索:')[1].strip()\n",
    "        else:\n",
    "            return response.split('Clue:')[1].strip()\n",
    "    \n",
    "    return \"INVALID_FORMAT\"\n",
    "\n",
    "def extract_chinese_guess_word(response: str) -> str:\n",
    "    \"\"\"从响应中提取中文猜测词\"\"\"\n",
    "    if \"FORMAT_ERROR_EXCEEDED\" in response:\n",
    "        return \"FORMAT_ERROR\"\n",
    "    \n",
    "    # 检查中文格式标记\n",
    "    if '[猜测]' in response or '[GUESS]' in response.upper():\n",
    "        import re\n",
    "        # 优先匹配中文标记\n",
    "        match = re.search(r'\\[猜测\\]\\s*(.+)', response)\n",
    "        if not match:\n",
    "            match = re.search(r'\\[GUESS\\]\\s*(.+)', response, re.IGNORECASE)\n",
    "        \n",
    "        if match:\n",
    "            guess_part = match.group(1).strip()\n",
    "            # 提取第一个中文词汇\n",
    "            chinese_words = re.findall(r'[\\u4e00-\\u9fff]+', guess_part)\n",
    "            if chinese_words:\n",
    "                return chinese_words[0]\n",
    "    \n",
    "    # 备用格式\n",
    "    if '猜测:' in response or 'Guess:' in response:\n",
    "        if '猜测:' in response:\n",
    "            guess_part = response.split('猜测:')[1].strip()\n",
    "        else:\n",
    "            guess_part = response.split('Guess:')[1].strip()\n",
    "        \n",
    "        chinese_words = re.findall(r'[\\u4e00-\\u9fff]+', guess_part)\n",
    "        if chinese_words:\n",
    "            return chinese_words[0]\n",
    "    \n",
    "    return \"INVALID_FORMAT\"\n",
    "\n",
    "def check_chinese_taboo_violation(hint: str, taboo_words: List[str]) -> bool:\n",
    "    \"\"\"检查中文线索是否违反禁用词规则\"\"\"\n",
    "    hint_cleaned = re.sub(r'[^\\u4e00-\\u9fff]', '', hint.lower())\n",
    "    \n",
    "    for taboo in taboo_words:\n",
    "        taboo_cleaned = re.sub(r'[^\\u4e00-\\u9fff]', '', taboo.lower())\n",
    "        \n",
    "        # 检查完整匹配\n",
    "        if taboo_cleaned in hint_cleaned:\n",
    "            return True\n",
    "        \n",
    "        # 检查部分匹配（对于较长的词）\n",
    "        if len(taboo_cleaned) >= 2:\n",
    "            # 检查是否包含禁用词的主要部分\n",
    "            if len(taboo_cleaned) >= 3:\n",
    "                core_part = taboo_cleaned[:2]  # 取前两个字符作为核心\n",
    "                if core_part in hint_cleaned:\n",
    "                    return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def robust_chinese_api_call(client, model: str, base_prompt: str, expected_prefix: str, max_retries: int = 3):\n",
    "    \"\"\"健壮的中文API调用\"\"\"\n",
    "    failed_outputs = []\n",
    "    \n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            if attempt == 1:\n",
    "                prompt = base_prompt\n",
    "            else:\n",
    "                prev_output = failed_outputs[-1] if failed_outputs else \"未知\"\n",
    "                format_reminder = f\"\"\"\n",
    "\n",
    "⚠️ 格式错误 ⚠️\n",
    "您之前的回复是: \"{prev_output}\"\n",
    "\n",
    "必需格式:\n",
    "- 您必须以 '{expected_prefix}' 开头（包括方括号）\n",
    "- 不要在 {expected_prefix} 前添加任何文字\n",
    "\n",
    "请使用正确格式重试:\"\"\"\n",
    "                prompt = base_prompt + format_reminder\n",
    "            \n",
    "            response = client.call_model(model, [{\"role\": \"user\", \"content\": prompt}])\n",
    "            \n",
    "            if (response.strip().startswith(expected_prefix) or \n",
    "                response.strip().upper().startswith(expected_prefix.upper())):\n",
    "                return {\n",
    "                    'success': True,\n",
    "                    'response': response,\n",
    "                    'attempts': attempt,\n",
    "                    'error': None,\n",
    "                    'failed_outputs': failed_outputs\n",
    "                }\n",
    "            else:\n",
    "                safe_response = safe_chinese_text_cleanup(response, max_length=150)\n",
    "                failed_outputs.append(safe_response)\n",
    "                \n",
    "                if attempt == max_retries:\n",
    "                    all_failed = \" | \".join(failed_outputs)\n",
    "                    return {\n",
    "                        'success': False,\n",
    "                        'response': f\"FORMAT_ERROR_EXCEEDED: {safe_response}\",\n",
    "                        'attempts': attempt,\n",
    "                        'error': f\"尝试 {max_retries} 次后失败。期望格式 '{expected_prefix}'。所有失败输出: {all_failed}\",\n",
    "                        'failed_outputs': failed_outputs\n",
    "                    }\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "        except Exception as e:\n",
    "            safe_error = safe_chinese_text_cleanup(str(e), max_length=150)\n",
    "            error_msg = f\"API错误 (尝试 {attempt}/{max_retries}): {safe_error}\"\n",
    "            \n",
    "            if attempt == max_retries:\n",
    "                return {\n",
    "                    'success': False,\n",
    "                    'response': None,\n",
    "                    'attempts': attempt,\n",
    "                    'error': error_msg,\n",
    "                    'failed_outputs': failed_outputs\n",
    "                }\n",
    "            time.sleep(1.0)\n",
    "    \n",
    "    return {\n",
    "        'success': False,\n",
    "        'response': None,\n",
    "        'attempts': max_retries,\n",
    "        'error': \"超出最大重试次数\",\n",
    "        'failed_outputs': failed_outputs\n",
    "    }\n",
    "\n",
    "print(\"✅ 中文Taboo游戏核心逻辑已定义\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. 中文Taboo游戏主函数\n",
    "def play_chinese_taboo_game(client, hinter_model, guesser_model, target_word, taboo_words, max_turns=5):\n",
    "    \"\"\"中文Taboo游戏主函数\"\"\"\n",
    "    \n",
    "    conversation_history = []\n",
    "    taboo_list = '、'.join(taboo_words)\n",
    "    \n",
    "    # 中文系统提示 - Hinter\n",
    "    hinter_system = f\"\"\"您正在玩中文Taboo游戏，担任提示者角色。\n",
    "\n",
    "关键格式要求:\n",
    "- 您必须以 '[线索]' 开头回复（包括方括号）\n",
    "- 不要在 [线索] 前添加任何文字\n",
    "- 示例: [线索] 这是一种有长鼻子的大型哺乳动物\n",
    "\n",
    "游戏规则:\n",
    "1. 绝对不能使用这些禁用词: {taboo_list}\n",
    "2. 给出有用的线索帮助猜测者找到目标词\n",
    "3. 可以看到对话历史，在之前线索基础上补充\n",
    "4. 要有创意但严格遵循格式\n",
    "5. 使用标准中文表达\n",
    "\n",
    "目标词: {target_word}\n",
    "禁用词（绝对禁止）: {taboo_list}\"\"\"\n",
    "\n",
    "    # 中文系统提示 - Guesser\n",
    "    guesser_system = \"\"\"您正在玩中文Taboo游戏，担任猜测者角色。\n",
    "\n",
    "关键格式要求:\n",
    "- 您必须以 '[猜测]' 开头回复（包括方括号）\n",
    "- 不要在 [猜测] 前添加任何文字\n",
    "- 示例: [猜测] 大象\n",
    "\n",
    "游戏规则:\n",
    "1. 根据收到的所有线索进行最佳猜测\n",
    "2. 可以看到对话历史\n",
    "3. 在 [猜测] 后只给出一个中文词汇作为答案\n",
    "4. 使用标准中文词汇\"\"\"\n",
    "\n",
    "    # 记录统计信息\n",
    "    total_hinter_attempts = 0\n",
    "    total_guesser_attempts = 0\n",
    "    format_errors = []\n",
    "    hinter_failed_outputs = []\n",
    "    guesser_failed_outputs = []\n",
    "\n",
    "    for turn in range(1, max_turns + 1):\n",
    "        # 构建Hinter提示\n",
    "        if turn == 1:\n",
    "            hinter_prompt = f\"{hinter_system}\\n\\n请提供您的第一个线索:\"\n",
    "        else:\n",
    "            history_text = \"\\n\".join([f\"第{i}轮: {msg}\" for i, msg in enumerate(conversation_history, 1)])\n",
    "            hinter_prompt = f\"{hinter_system}\\n\\n对话历史:\\n{history_text}\\n\\n猜测者还没有找到答案。请提供下一个线索:\"\n",
    "        \n",
    "        # Hinter给出线索\n",
    "        hinter_result = robust_chinese_api_call(client, hinter_model, hinter_prompt, \"[线索]\", max_retries=3)\n",
    "        total_hinter_attempts += hinter_result['attempts']\n",
    "        \n",
    "        if hinter_result.get('failed_outputs'):\n",
    "            hinter_failed_outputs.extend(hinter_result['failed_outputs'])\n",
    "        \n",
    "        if not hinter_result['success']:\n",
    "            error_type = \"FORMAT_FAILURE\" if \"FORMAT_ERROR_EXCEEDED\" in str(hinter_result.get('response', '')) else \"API_FAILURE\"\n",
    "            format_errors.append(f\"第{turn}轮 提示者: {hinter_result['error']}\")\n",
    "            \n",
    "            return {\n",
    "                'success': False,\n",
    "                'turns': turn,\n",
    "                'conversation': conversation_history,\n",
    "                'final_guess': f\"HINTER_{error_type}\",\n",
    "                'error': f\"{error_type}: {hinter_result['error']}\",\n",
    "                'failure_reason': error_type,\n",
    "                'total_hinter_attempts': total_hinter_attempts,\n",
    "                'total_guesser_attempts': total_guesser_attempts,\n",
    "                'format_errors': format_errors,\n",
    "                'hinter_failed_outputs': hinter_failed_outputs,\n",
    "                'guesser_failed_outputs': guesser_failed_outputs,\n",
    "                'all_hints': [msg for msg in conversation_history if msg.startswith('提示者:')],\n",
    "                'all_guesses': [msg for msg in conversation_history if msg.startswith('猜测者:')]\n",
    "            }\n",
    "        \n",
    "        # 提取线索并检查taboo violation\n",
    "        hint_text = extract_chinese_clue_text(hinter_result['response'])\n",
    "        \n",
    "        # 检查是否违反禁用词规则\n",
    "        taboo_violated = check_chinese_taboo_violation(hint_text, taboo_words)\n",
    "        if taboo_violated:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'turns': turn,\n",
    "                'conversation': conversation_history,\n",
    "                'final_guess': '违反禁用词规则: 提示者违规',\n",
    "                'error': f'违反禁用词规则: 提示者在第{turn}轮违反规则，使用了禁用词: {hint_text}',\n",
    "                'failure_reason': 'TABOO_VIOLATION',\n",
    "                'taboo_violation_turn': turn,\n",
    "                'taboo_violation_hint': hint_text,\n",
    "                'total_hinter_attempts': total_hinter_attempts,\n",
    "                'total_guesser_attempts': total_guesser_attempts,\n",
    "                'format_errors': format_errors,\n",
    "                'hinter_failed_outputs': hinter_failed_outputs,\n",
    "                'guesser_failed_outputs': guesser_failed_outputs,\n",
    "                'all_hints': [msg for msg in conversation_history if msg.startswith('提示者:')],\n",
    "                'all_guesses': [msg for msg in conversation_history if msg.startswith('猜测者:')]\n",
    "            }\n",
    "        \n",
    "        conversation_history.append(f\"提示者: {hinter_result['response']}\")\n",
    "        \n",
    "        # 构建Guesser提示\n",
    "        history_text = \"\\n\".join([f\"第{i}轮: {msg}\" for i, msg in enumerate(conversation_history, 1)])\n",
    "        guesser_prompt = f\"{guesser_system}\\n\\n对话历史:\\n{history_text}\\n\\n您的猜测是什么?\"\n",
    "        \n",
    "        # Guesser进行猜测\n",
    "        guesser_result = robust_chinese_api_call(client, guesser_model, guesser_prompt, \"[猜测]\", max_retries=3)\n",
    "        total_guesser_attempts += guesser_result['attempts']\n",
    "        \n",
    "        if guesser_result.get('failed_outputs'):\n",
    "            guesser_failed_outputs.extend(guesser_result['failed_outputs'])\n",
    "        \n",
    "        if not guesser_result['success']:\n",
    "            error_type = \"FORMAT_FAILURE\" if \"FORMAT_ERROR_EXCEEDED\" in str(guesser_result.get('response', '')) else \"API_FAILURE\"\n",
    "            format_errors.append(f\"第{turn}轮 猜测者: {guesser_result['error']}\")\n",
    "            \n",
    "            return {\n",
    "                'success': False,\n",
    "                'turns': turn,\n",
    "                'conversation': conversation_history,\n",
    "                'final_guess': f\"GUESSER_{error_type}\",\n",
    "                'error': f\"{error_type}: {guesser_result['error']}\",\n",
    "                'failure_reason': error_type,\n",
    "                'total_hinter_attempts': total_hinter_attempts,\n",
    "                'total_guesser_attempts': total_guesser_attempts,\n",
    "                'format_errors': format_errors,\n",
    "                'hinter_failed_outputs': hinter_failed_outputs,\n",
    "                'guesser_failed_outputs': guesser_failed_outputs,\n",
    "                'all_hints': [msg for msg in conversation_history if msg.startswith('提示者:')],\n",
    "                'all_guesses': [msg for msg in conversation_history if msg.startswith('猜测者:')]\n",
    "            }\n",
    "        \n",
    "        conversation_history.append(f\"猜测者: {guesser_result['response']}\")\n",
    "        guess = extract_chinese_guess_word(guesser_result['response'])\n",
    "        \n",
    "        # 检查是否成功\n",
    "        if guess == target_word:\n",
    "            return {\n",
    "                'success': True,\n",
    "                'turns': turn,\n",
    "                'conversation': conversation_history,\n",
    "                'final_guess': guess,\n",
    "                'failure_reason': None,\n",
    "                'total_hinter_attempts': total_hinter_attempts,\n",
    "                'total_guesser_attempts': total_guesser_attempts,\n",
    "                'format_errors': format_errors,\n",
    "                'hinter_failed_outputs': hinter_failed_outputs,\n",
    "                'guesser_failed_outputs': guesser_failed_outputs,\n",
    "                'all_hints': [msg for msg in conversation_history if msg.startswith('提示者:')],\n",
    "                'all_guesses': [msg for msg in conversation_history if msg.startswith('猜测者:')]\n",
    "            }\n",
    "        \n",
    "        # 如果不是最后一轮，添加反馈\n",
    "        if turn < max_turns:\n",
    "            conversation_history.append(f\"系统: '{guess}' 不正确。请继续！\")\n",
    "    \n",
    "    # 达到最大轮数仍未成功\n",
    "    return {\n",
    "        'success': False,\n",
    "        'turns': max_turns,\n",
    "        'conversation': conversation_history,\n",
    "        'final_guess': guess if 'guess' in locals() else 'N/A',\n",
    "        'failure_reason': 'MAX_TURNS_EXCEEDED',\n",
    "        'total_hinter_attempts': total_hinter_attempts,\n",
    "        'total_guesser_attempts': total_guesser_attempts,\n",
    "        'format_errors': format_errors,\n",
    "        'hinter_failed_outputs': hinter_failed_outputs,\n",
    "        'guesser_failed_outputs': guesser_failed_outputs,\n",
    "        'all_hints': [msg for msg in conversation_history if msg.startswith('提示者:')],\n",
    "        'all_guesses': [msg for msg in conversation_history if msg.startswith('猜测者:')]\n",
    "    }\n",
    "\n",
    "print(\"✅ 中文Taboo游戏主函数已定义\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. 执行中文Taboo测试实验\n",
    "print(\"🧪 开始执行中文Taboo测试实验...\")\n",
    "\n",
    "def run_chinese_test_experiment(client, models, dataset, num_test_words=3):\n",
    "    \"\"\"运行中文Taboo测试实验\"\"\"\n",
    "    \n",
    "    if not client:\n",
    "        print(\"❌ API客户端未初始化，无法执行实验\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n🎯 测试配置:\")\n",
    "    print(f\"   测试词汇数: {num_test_words}\")\n",
    "    print(f\"   模型数量: {len(models)}\")\n",
    "    print(f\"   总游戏数: {num_test_words * len(models) * len(models)}\")\n",
    "    \n",
    "    # 随机选择测试词汇\n",
    "    test_words = random.sample(dataset, min(num_test_words, len(dataset)))\n",
    "    print(f\"\\n📋 选择的测试词汇:\")\n",
    "    for i, word_data in enumerate(test_words, 1):\n",
    "        print(f\"   {i}. {word_data['target']} ({word_data['part_of_speech']}) - 禁用词: {word_data['taboo']}\")\n",
    "    \n",
    "    all_results = []\n",
    "    total_games = len(test_words) * len(models) * len(models)\n",
    "    game_counter = 0\n",
    "    \n",
    "    print(f\"\\n🚀 开始执行实验...\")\n",
    "    \n",
    "    for word_data in test_words:\n",
    "        target_word = word_data['target']\n",
    "        taboo_words = word_data['taboo']\n",
    "        \n",
    "        print(f\"\\n🎯 测试词汇: {target_word}\")\n",
    "        print(f\"🚫 禁用词: {taboo_words}\")\n",
    "        \n",
    "        for hinter_model in models:\n",
    "            for guesser_model in models:\n",
    "                game_counter += 1\n",
    "                hinter_name = hinter_model.split('/')[-1]\n",
    "                guesser_name = guesser_model.split('/')[-1]\n",
    "                pair_name = f\"{hinter_name}→{guesser_name}\"\n",
    "                \n",
    "                print(f\"  🔄 游戏 {game_counter}/{total_games}: {pair_name}\")\n",
    "                \n",
    "                start_time = time.time()\n",
    "                \n",
    "                try:\n",
    "                    # 执行游戏\n",
    "                    game_result = play_chinese_taboo_game(\n",
    "                        client, hinter_model, guesser_model, \n",
    "                        target_word, taboo_words, max_turns=5\n",
    "                    )\n",
    "                    \n",
    "                    duration = round(time.time() - start_time, 2)\n",
    "                    \n",
    "                    # 记录结果\n",
    "                    result = {\n",
    "                        'game_id': game_counter,\n",
    "                        'target_word': target_word,\n",
    "                        'part_of_speech': word_data['part_of_speech'],\n",
    "                        'category': word_data['category'],\n",
    "                        'taboo_words': '|'.join(taboo_words),\n",
    "                        'hinter_model': hinter_model,\n",
    "                        'guesser_model': guesser_model,\n",
    "                        'success': game_result['success'],\n",
    "                        'turns_used': game_result['turns'],\n",
    "                        'final_guess': game_result['final_guess'],\n",
    "                        'failure_reason': game_result.get('failure_reason', None),\n",
    "                        'taboo_violation_turn': game_result.get('taboo_violation_turn', None),\n",
    "                        'taboo_violation_hint': game_result.get('taboo_violation_hint', None),\n",
    "                        'has_taboo_violation': game_result.get('failure_reason') == 'TABOO_VIOLATION',\n",
    "                        'all_hints': ' | '.join(game_result['all_hints']),\n",
    "                        'all_guesses': ' | '.join(game_result['all_guesses']),\n",
    "                        'conversation': ' | '.join(game_result['conversation']),\n",
    "                        'total_api_attempts': game_result.get('total_hinter_attempts', 0) + game_result.get('total_guesser_attempts', 0),\n",
    "                        'format_errors': ' | '.join(game_result.get('format_errors', [])),\n",
    "                        'has_format_errors': len(game_result.get('format_errors', [])) > 0,\n",
    "                        'duration_seconds': duration,\n",
    "                        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        'language': 'chinese',\n",
    "                        'dataset_source': 'openhownet'\n",
    "                    }\n",
    "                    \n",
    "                    if 'error' in game_result:\n",
    "                        result['error'] = game_result['error']\n",
    "                    \n",
    "                    all_results.append(result)\n",
    "                    \n",
    "                    # 显示结果\n",
    "                    status = \"✅ 成功\" if game_result['success'] else \"❌ 失败\"\n",
    "                    failure_info = \"\"\n",
    "                    if not game_result['success'] and game_result.get('failure_reason'):\n",
    "                        failure_reason = game_result['failure_reason']\n",
    "                        if failure_reason == 'TABOO_VIOLATION':\n",
    "                            failure_info = \" (违反禁用词)\"\n",
    "                        elif failure_reason == 'FORMAT_FAILURE':\n",
    "                            failure_info = \" (格式错误)\"\n",
    "                        elif failure_reason == 'API_FAILURE':\n",
    "                            failure_info = \" (API失败)\"\n",
    "                        elif failure_reason == 'MAX_TURNS_EXCEEDED':\n",
    "                            failure_info = \" (轮数耗尽)\"\n",
    "                    \n",
    "                    print(f\"     {status}{failure_info} | {game_result['turns']}轮 | 最终猜测: {game_result['final_guess']}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"     ❌ 游戏执行异常: {e}\")\n",
    "                    # 记录异常结果\n",
    "                    result = {\n",
    "                        'game_id': game_counter,\n",
    "                        'target_word': target_word,\n",
    "                        'hinter_model': hinter_model,\n",
    "                        'guesser_model': guesser_model,\n",
    "                        'success': False,\n",
    "                        'failure_reason': 'EXCEPTION',\n",
    "                        'error': str(e),\n",
    "                        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        'language': 'chinese'\n",
    "                    }\n",
    "                    all_results.append(result)\n",
    "                \n",
    "                time.sleep(0.5)  # API调用间隔\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# 执行测试实验\n",
    "if chinese_client:\n",
    "    test_results = run_chinese_test_experiment(\n",
    "        chinese_client, CHINESE_TEST_MODELS, chinese_dataset, num_test_words=3\n",
    "    )\n",
    "    \n",
    "    if test_results:\n",
    "        print(f\"\\n🎉 中文Taboo测试实验完成！\")\n",
    "        print(f\"📊 总游戏数: {len(test_results)}\")\n",
    "        \n",
    "        # 统计结果\n",
    "        successful_games = [r for r in test_results if r['success']]\n",
    "        success_rate = len(successful_games) / len(test_results) * 100\n",
    "        print(f\"📈 成功率: {len(successful_games)}/{len(test_results)} ({success_rate:.1f}%)\")\n",
    "        \n",
    "        # 保存测试结果\n",
    "        test_results_path = f\"results/chinese_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        \n",
    "        df_results = pd.DataFrame(test_results)\n",
    "        df_results.to_csv(test_results_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"💾 测试结果已保存: {test_results_path}\")\n",
    "        \n",
    "        # 按模型统计\n",
    "        print(f\"\\n📊 各模型表现:\")\n",
    "        for model in CHINESE_TEST_MODELS:\n",
    "            model_name = model.split('/')[-1]\n",
    "            model_as_hinter = [r for r in test_results if r['hinter_model'] == model]\n",
    "            model_as_guesser = [r for r in test_results if r['guesser_model'] == model]\n",
    "            \n",
    "            hinter_success = len([r for r in model_as_hinter if r['success']])\n",
    "            guesser_success = len([r for r in model_as_guesser if r['success']])\n",
    "            \n",
    "            print(f\"   {model_name}:\")\n",
    "            if len(model_as_hinter) > 0:\n",
    "                print(f\"     作为提示者: {hinter_success}/{len(model_as_hinter)} ({hinter_success/len(model_as_hinter)*100:.1f}%)\")\n",
    "            if len(model_as_guesser) > 0:\n",
    "                print(f\"     作为猜测者: {guesser_success}/{len(model_as_guesser)} ({guesser_success/len(model_as_guesser)*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"❌ 测试实验失败\")\n",
    "else:\n",
    "    print(\"❌ 无法执行测试实验：API客户端未初始化\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# 中文Taboo实验总结\n",
    "\n",
    "## ✅ 已完成的工作\n",
    "\n",
    "1. **环境配置**: 成功安装并配置了OpenHowNet和相关中文处理工具\n",
    "2. **数据集构建**: 使用OpenHowNet构建了100个中文词汇的Taboo数据集\n",
    "3. **词性分布**: 按照名词、动词、形容词、副词各25个的目标进行分布\n",
    "4. **游戏逻辑**: 实现了完整的中文Taboo游戏逻辑和评估框架\n",
    "5. **模型测试**: 验证了中文格式要求和禁用词检测机制\n",
    "6. **数据保存**: 生成了完整的数据集文件和实验报告\n",
    "\n",
    "## 🎯 数据集特点\n",
    "\n",
    "- **基于OpenHowNet**: 利用中文知识图谱的语义关系\n",
    "- **禁用词生成**: 每个词汇包含5个语义相关的禁用词\n",
    "- **词性覆盖**: 涵盖4种主要词性，平衡分布\n",
    "- **语义丰富**: 包含完整的义项信息和语义定义\n",
    "- **中文优化**: 专门针对中文语言特点进行优化\n",
    "\n",
    "## 🤖 技术创新\n",
    "\n",
    "- **首次应用**: 将OpenHowNet用于Taboo游戏数据集构建\n",
    "- **中文适配**: 实现了中文特定的格式检查和违规检测\n",
    "- **评估框架**: 提供了完整的中文LLM评估体系\n",
    "- **多模型支持**: 支持GPT-4o、Gemini、DeepSeek等多种模型\n",
    "\n",
    "## 📁 生成文件\n",
    "\n",
    "- `data/chinese_dataset.json` - 完整数据集\n",
    "- `data/chinese_dataset_simple.json` - 简化版数据集  \n",
    "- `data/chinese_dataset_report.json` - 数据集报告\n",
    "- `results/chinese_test_results_*.csv` - 实验结果\n",
    "\n",
    "## 🔮 扩展方向\n",
    "\n",
    "1. **规模扩展**: 增加数据集到300-500个词汇\n",
    "2. **领域拓展**: 添加专业领域词汇（医学、法律、科技等）\n",
    "3. **模型覆盖**: 测试更多中文模型（智谱、百川、文心等）\n",
    "4. **对比研究**: 实现中英文Taboo游戏对比分析\n",
    "5. **难度分级**: 研究不同复杂度词汇对模型性能的影响\n",
    "\n",
    "## 🎉 实验意义\n",
    "\n",
    "这是首个基于OpenHowNet的中文Taboo游戏实验系统，为中文语言模型的语义理解能力评估提供了新的基准测试工具。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 结果分析与可视化\n",
    "print(\"📊 中文Taboo实验结果深度分析\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "\n",
    "# 设置中文字体支持\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "def load_experiment_results():\n",
    "    \"\"\"加载所有实验结果文件\"\"\"\n",
    "    results_files = glob.glob(\"results/**/chinese_*.csv\", recursive=True)\n",
    "    results_files.extend(glob.glob(\"results/**/*chinese*.csv\", recursive=True))\n",
    "    \n",
    "    if not results_files:\n",
    "        print(\"⚠️ 未找到实验结果文件\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"🔍 找到 {len(results_files)} 个结果文件:\")\n",
    "    for file in results_files:\n",
    "        print(f\"   - {file}\")\n",
    "    \n",
    "    # 加载最新的结果文件\n",
    "    latest_file = max(results_files, key=lambda x: x.split('_')[-1] if '_' in x else x)\n",
    "    print(f\"\\n📂 加载最新结果文件: {latest_file}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(latest_file, encoding='utf-8-sig')\n",
    "        print(f\"✅ 成功加载 {len(df)} 条记录\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 加载失败: {e}\")\n",
    "        return None\n",
    "\n",
    "def analyze_overall_performance(df):\n",
    "    \"\"\"整体性能分析\"\"\"\n",
    "    print(\"\\n🎯 整体性能分析\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    total_games = len(df)\n",
    "    successful_games = len(df[df['success'] == True])\n",
    "    success_rate = successful_games / total_games * 100 if total_games > 0 else 0\n",
    "    \n",
    "    print(f\"总游戏数: {total_games}\")\n",
    "    print(f\"成功游戏数: {successful_games}\")\n",
    "    print(f\"整体成功率: {success_rate:.1f}%\")\n",
    "    \n",
    "    # 失败原因分析\n",
    "    failed_games = df[df['success'] == False]\n",
    "    if len(failed_games) > 0:\n",
    "        failure_reasons = failed_games['failure_reason'].value_counts()\n",
    "        print(f\"\\n❌ 失败原因分布:\")\n",
    "        for reason, count in failure_reasons.items():\n",
    "            percentage = count / len(failed_games) * 100\n",
    "            print(f\"   {reason}: {count} 次 ({percentage:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'total_games': total_games,\n",
    "        'successful_games': successful_games,\n",
    "        'success_rate': success_rate,\n",
    "        'failure_reasons': failure_reasons if len(failed_games) > 0 else None\n",
    "    }\n",
    "\n",
    "def analyze_model_performance(df):\n",
    "    \"\"\"模型性能分析\"\"\"\n",
    "    print(\"\\n🤖 模型性能分析\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    models = list(set(df['hinter_model'].unique()) | set(df['guesser_model'].unique()))\n",
    "    model_stats = {}\n",
    "    \n",
    "    for model in models:\n",
    "        model_name = model.split('/')[-1] if '/' in model else model\n",
    "        \n",
    "        # 作为提示者的表现\n",
    "        as_hinter = df[df['hinter_model'] == model]\n",
    "        hinter_success = len(as_hinter[as_hinter['success'] == True])\n",
    "        hinter_total = len(as_hinter)\n",
    "        hinter_rate = hinter_success / hinter_total * 100 if hinter_total > 0 else 0\n",
    "        \n",
    "        # 作为猜测者的表现\n",
    "        as_guesser = df[df['guesser_model'] == model]\n",
    "        guesser_success = len(as_guesser[as_guesser['success'] == True])\n",
    "        guesser_total = len(as_guesser)\n",
    "        guesser_rate = guesser_success / guesser_total * 100 if guesser_total > 0 else 0\n",
    "        \n",
    "        model_stats[model_name] = {\n",
    "            'hinter': {'success': hinter_success, 'total': hinter_total, 'rate': hinter_rate},\n",
    "            'guesser': {'success': guesser_success, 'total': guesser_total, 'rate': guesser_rate}\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  作为提示者: {hinter_success}/{hinter_total} ({hinter_rate:.1f}%)\")\n",
    "        print(f\"  作为猜测者: {guesser_success}/{guesser_total} ({guesser_rate:.1f}%)\")\n",
    "    \n",
    "    return model_stats\n",
    "\n",
    "def analyze_pos_performance(df):\n",
    "    \"\"\"词性性能分析\"\"\"\n",
    "    print(\"\\n📝 词性性能分析\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if 'part_of_speech' not in df.columns:\n",
    "        print(\"⚠️ 数据中未找到词性信息\")\n",
    "        return None\n",
    "    \n",
    "    pos_stats = {}\n",
    "    pos_groups = df.groupby('part_of_speech')\n",
    "    \n",
    "    for pos, group in pos_groups:\n",
    "        total = len(group)\n",
    "        success = len(group[group['success'] == True])\n",
    "        rate = success / total * 100 if total > 0 else 0\n",
    "        \n",
    "        pos_stats[pos] = {'total': total, 'success': success, 'rate': rate}\n",
    "        print(f\"{pos}: {success}/{total} ({rate:.1f}%)\")\n",
    "    \n",
    "    return pos_stats\n",
    "\n",
    "def create_visualizations(df, model_stats, pos_stats, overall_stats):\n",
    "    \"\"\"创建可视化图表\"\"\"\n",
    "    print(\"\\n📈 生成可视化图表\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # 创建子图\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('中文Taboo实验结果分析', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. 整体成功率饼图\n",
    "    ax1 = axes[0, 0]\n",
    "    success_data = [overall_stats['successful_games'], \n",
    "                   overall_stats['total_games'] - overall_stats['successful_games']]\n",
    "    labels = [f'成功 ({overall_stats[\"success_rate\"]:.1f}%)', \n",
    "              f'失败 ({100-overall_stats[\"success_rate\"]:.1f}%)']\n",
    "    colors = ['#2E8B57', '#DC143C']\n",
    "    \n",
    "    ax1.pie(success_data, labels=labels, colors=colors, autopct='%d', startangle=90)\n",
    "    ax1.set_title('整体成功率分布')\n",
    "    \n",
    "    # 2. 模型性能对比\n",
    "    ax2 = axes[0, 1]\n",
    "    if model_stats:\n",
    "        model_names = list(model_stats.keys())\n",
    "        hinter_rates = [stats['hinter']['rate'] for stats in model_stats.values()]\n",
    "        guesser_rates = [stats['guesser']['rate'] for stats in model_stats.values()]\n",
    "        \n",
    "        x = np.arange(len(model_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax2.bar(x - width/2, hinter_rates, width, label='作为提示者', color='skyblue')\n",
    "        bars2 = ax2.bar(x + width/2, guesser_rates, width, label='作为猜测者', color='lightcoral')\n",
    "        \n",
    "        ax2.set_xlabel('模型')\n",
    "        ax2.set_ylabel('成功率 (%)')\n",
    "        ax2.set_title('各模型性能对比')\n",
    "        ax2.set_xticks(x)\n",
    "        ax2.set_xticklabels([name[:10] for name in model_names], rotation=45)\n",
    "        ax2.legend()\n",
    "        ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 3. 词性性能分析\n",
    "    ax3 = axes[1, 0]\n",
    "    if pos_stats:\n",
    "        pos_names = list(pos_stats.keys())\n",
    "        pos_rates = [stats['rate'] for stats in pos_stats.values()]\n",
    "        pos_totals = [stats['total'] for stats in pos_stats.values()]\n",
    "        \n",
    "        bars = ax3.bar(pos_names, pos_rates, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
    "        ax3.set_ylabel('成功率 (%)')\n",
    "        ax3.set_title('各词性表现')\n",
    "        ax3.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # 在柱子上添加数据标签\n",
    "        for bar, total in zip(bars, pos_totals):\n",
    "            height = bar.get_height()\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                    f'{height:.1f}%\\n(n={total})', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # 4. 失败原因分析\n",
    "    ax4 = axes[1, 1]\n",
    "    if overall_stats['failure_reasons'] is not None and len(overall_stats['failure_reasons']) > 0:\n",
    "        failure_reasons = overall_stats['failure_reasons']\n",
    "        reasons = list(failure_reasons.keys())\n",
    "        counts = list(failure_reasons.values())\n",
    "        \n",
    "        # 简化失败原因名称\n",
    "        reason_mapping = {\n",
    "            'TABOO_VIOLATION': '禁用词违规',\n",
    "            'MAX_TURNS_EXCEEDED': '轮数耗尽',\n",
    "            'FORMAT_FAILURE': '格式错误',\n",
    "            'API_FAILURE': 'API失败',\n",
    "            'EXCEPTION': '异常错误'\n",
    "        }\n",
    "        \n",
    "        simplified_reasons = [reason_mapping.get(r, r) for r in reasons]\n",
    "        \n",
    "        wedges, texts, autotexts = ax4.pie(counts, labels=simplified_reasons, autopct='%1.1f%%', startangle=90)\n",
    "        ax4.set_title('失败原因分布')\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, '无失败数据', ha='center', va='center', transform=ax4.transAxes)\n",
    "        ax4.set_title('失败原因分布')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 保存图表\n",
    "    plot_filename = f\"results/chinese_experiment_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\"\n",
    "    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "    print(f\"📊 图表已保存: {plot_filename}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def generate_detailed_report(df, model_stats, pos_stats, overall_stats):\n",
    "    \"\"\"生成详细分析报告\"\"\"\n",
    "    print(\"\\n📋 生成详细分析报告\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    report = {\n",
    "        'experiment_summary': {\n",
    "            'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'total_games': overall_stats['total_games'],\n",
    "            'success_rate': overall_stats['success_rate'],\n",
    "            'dataset_size': len(df['target_word'].unique()) if 'target_word' in df.columns else 0\n",
    "        },\n",
    "        'overall_performance': overall_stats,\n",
    "        'model_performance': model_stats,\n",
    "        'pos_performance': pos_stats\n",
    "    }\n",
    "    \n",
    "    # 添加词汇级别分析\n",
    "    if 'target_word' in df.columns:\n",
    "        word_analysis = {}\n",
    "        for word in df['target_word'].unique():\n",
    "            word_games = df[df['target_word'] == word]\n",
    "            word_success = len(word_games[word_games['success'] == True])\n",
    "            word_total = len(word_games)\n",
    "            word_rate = word_success / word_total * 100 if word_total > 0 else 0\n",
    "            \n",
    "            word_analysis[word] = {\n",
    "                'success_rate': word_rate,\n",
    "                'total_games': word_total,\n",
    "                'success_count': word_success\n",
    "            }\n",
    "        \n",
    "        # 找出最难和最容易的词汇\n",
    "        sorted_words = sorted(word_analysis.items(), key=lambda x: x[1]['success_rate'])\n",
    "        \n",
    "        report['word_analysis'] = {\n",
    "            'most_difficult': sorted_words[:5],  # 最难的5个词\n",
    "            'easiest': sorted_words[-5:],  # 最容易的5个词\n",
    "            'all_words': word_analysis\n",
    "        }\n",
    "        \n",
    "        print(f\"最难词汇 (成功率最低):\")\n",
    "        for word, stats in sorted_words[:5]:\n",
    "            print(f\"  {word}: {stats['success_rate']:.1f}% ({stats['success_count']}/{stats['total_games']})\")\n",
    "        \n",
    "        print(f\"\\n最容易词汇 (成功率最高):\")\n",
    "        for word, stats in sorted_words[-5:]:\n",
    "            print(f\"  {word}: {stats['success_rate']:.1f}% ({stats['success_count']}/{stats['total_games']})\")\n",
    "    \n",
    "    # 保存报告\n",
    "    report_filename = f\"results/chinese_experiment_detailed_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\n💾 详细报告已保存: {report_filename}\")\n",
    "    return report\n",
    "\n",
    "# 主要分析流程\n",
    "print(\"🚀 开始结果分析...\")\n",
    "\n",
    "# 1. 加载数据\n",
    "df_results = load_experiment_results()\n",
    "\n",
    "if df_results is not None:\n",
    "    print(f\"\\n📊 数据概览:\")\n",
    "    print(f\"   数据维度: {df_results.shape}\")\n",
    "    print(f\"   列名: {list(df_results.columns)}\")\n",
    "    \n",
    "    # 2. 整体性能分析\n",
    "    overall_performance = analyze_overall_performance(df_results)\n",
    "    \n",
    "    # 3. 模型性能分析\n",
    "    model_performance = analyze_model_performance(df_results)\n",
    "    \n",
    "    # 4. 词性性能分析\n",
    "    pos_performance = analyze_pos_performance(df_results)\n",
    "    \n",
    "    # 5. 创建可视化\n",
    "    create_visualizations(df_results, model_performance, pos_performance, overall_performance)\n",
    "    \n",
    "    # 6. 生成详细报告\n",
    "    detailed_report = generate_detailed_report(df_results, model_performance, pos_performance, overall_performance)\n",
    "    \n",
    "    print(f\"\\n🎉 结果分析完成！\")\n",
    "    print(f\"📈 关键指标:\")\n",
    "    print(f\"   整体成功率: {overall_performance['success_rate']:.1f}%\")\n",
    "    print(f\"   测试模型数: {len(model_performance)}\")\n",
    "    print(f\"   词性覆盖: {len(pos_performance) if pos_performance else 0}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ 无法进行结果分析：未找到有效的实验结果文件\")\n",
    "    print(\"💡 请先运行实验生成结果文件\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"📋 分析完成 - 所有图表和报告已保存到 results/ 目录\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
