{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š æ­£åœ¨åŠ è½½Quick80æ•°æ®é›†...\n",
      "âœ… æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…±80æ¡è®°å½•\n",
      "\n",
      "ğŸ“‹ æ•°æ®é›†æ ·æœ¬:\n",
      "   ç›®æ ‡è¯: atomize\n",
      "   ç±»åˆ«: chemistry\n",
      "   ç¦ç”¨è¯: ['atomise', 'perfume', 'spray', 'nuke', 'zap']\n",
      "   è¯ä¹‰æ•°: 3\n"
     ]
    }
   ],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "from typing import Dict, List, Any\n",
    "from datetime import datetime\n",
    "\n",
    "# åŠ è½½Quick80æ•°æ®é›†\n",
    "def load_dataset(dataset_path: str = \"quick80_dataset.json\") -> List[Dict]:\n",
    "    \"\"\"åŠ è½½Tabooæ•°æ®é›†\"\"\"\n",
    "    with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# åŠ è½½æ•°æ®é›†\n",
    "print(\"ğŸ“š æ­£åœ¨åŠ è½½Quick80æ•°æ®é›†...\")\n",
    "dataset = load_dataset()\n",
    "print(f\"âœ… æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…±{len(dataset)}æ¡è®°å½•\")\n",
    "\n",
    "# æ˜¾ç¤ºæ•°æ®é›†æ ·æœ¬\n",
    "print(\"\\nğŸ“‹ æ•°æ®é›†æ ·æœ¬:\")\n",
    "sample = random.choice(dataset)\n",
    "print(f\"   ç›®æ ‡è¯: {sample['target']}\")\n",
    "print(f\"   ç±»åˆ«: {sample.get('category', 'unknown')}\")\n",
    "print(f\"   ç¦ç”¨è¯: {sample['taboo']}\")\n",
    "print(f\"   è¯ä¹‰æ•°: {len(sample.get('senses', []))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Quick80æ•°æ®é›†åŸºæœ¬ç»Ÿè®¡:\n",
      "========================================\n",
      "\n",
      "ğŸ·ï¸ ç±»åˆ«åˆ†å¸ƒ:\n",
      "   1. general: 55 æ¡ (68.8%)\n",
      "   2. philosophy: 8 æ¡ (10.0%)\n",
      "   3. finance: 7 æ¡ (8.8%)\n",
      "   4. chemistry: 5 æ¡ (6.2%)\n",
      "   5. cs: 5 æ¡ (6.2%)\n",
      "\n",
      "ğŸš« ç¦ç”¨è¯ç»Ÿè®¡:\n",
      "   å¹³å‡æ•°é‡: 5.0\n",
      "   èŒƒå›´: 5 - 5\n",
      "\n",
      "ğŸ’­ è¯ä¹‰ç»Ÿè®¡:\n",
      "   å¹³å‡æ•°é‡: 1.9\n",
      "   èŒƒå›´: 1 - 23\n",
      "\n",
      "âœ… Quick80æ•°æ®é›†ç»Ÿè®¡å®Œæˆï¼Œé€‚åˆå®šé‡åˆ†æå®éªŒ\n",
      "\n",
      "ğŸ² éšæœºç§å­å·²è®¾ç½®ä¸º 240ï¼Œç¡®ä¿å®éªŒå¯å¤ç°\n"
     ]
    }
   ],
   "source": [
    "# Quick80æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯\n",
    "print(\"ğŸ“Š Quick80æ•°æ®é›†åŸºæœ¬ç»Ÿè®¡:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# ç±»åˆ«åˆ†å¸ƒç»Ÿè®¡\n",
    "categories = {}\n",
    "taboo_counts = []\n",
    "sense_counts = []\n",
    "\n",
    "for item in dataset:\n",
    "    # ç»Ÿè®¡ç±»åˆ«\n",
    "    category = item.get('category', 'unknown')\n",
    "    categories[category] = categories.get(category, 0) + 1\n",
    "    \n",
    "    # ç»Ÿè®¡ç¦ç”¨è¯æ•°é‡\n",
    "    taboo_counts.append(len(item.get('taboo', [])))\n",
    "    \n",
    "    # ç»Ÿè®¡è¯ä¹‰æ•°é‡\n",
    "    sense_counts.append(len(item.get('senses', [])))\n",
    "\n",
    "print(f\"\\nğŸ·ï¸ ç±»åˆ«åˆ†å¸ƒ:\")\n",
    "sorted_categories = sorted(categories.items(), key=lambda x: x[1], reverse=True)\n",
    "for i, (category, count) in enumerate(sorted_categories, 1):\n",
    "    percentage = count / len(dataset) * 100\n",
    "    print(f\"   {i}. {category}: {count} æ¡ ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nğŸš« ç¦ç”¨è¯ç»Ÿè®¡:\")\n",
    "print(f\"   å¹³å‡æ•°é‡: {sum(taboo_counts) / len(taboo_counts):.1f}\")\n",
    "print(f\"   èŒƒå›´: {min(taboo_counts)} - {max(taboo_counts)}\")\n",
    "\n",
    "print(f\"\\nğŸ’­ è¯ä¹‰ç»Ÿè®¡:\")\n",
    "print(f\"   å¹³å‡æ•°é‡: {sum(sense_counts) / len(sense_counts):.1f}\")\n",
    "print(f\"   èŒƒå›´: {min(sense_counts)} - {max(sense_counts)}\")\n",
    "\n",
    "print(f\"\\nâœ… Quick80æ•°æ®é›†ç»Ÿè®¡å®Œæˆï¼Œé€‚åˆå®šé‡åˆ†æå®éªŒ\")\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­ç”¨äºå®éªŒ\n",
    "random.seed(240)\n",
    "print(\"\\nğŸ² éšæœºç§å­å·²è®¾ç½®ä¸º 240ï¼Œç¡®ä¿å®éªŒå¯å¤ç°\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Quick80å®šé‡åˆ†æé…ç½®å·²åŠ è½½\n",
      "ğŸ¤– åˆ†ææ¨¡å‹: 2 ä¸ª\n",
      "   1. deepseek-chat-v3-0324\n",
      "   2. gemini-2.5-flash\n",
      "ğŸŒ¡ï¸ æ¸©åº¦å‚æ•°: [0.1, 0.3, 0.7]\n",
      "ğŸš« ç¦å¿Œè¯æ•°é‡: [1, 3, 5]\n",
      "ğŸ’¬ Hinteræç¤ºé•¿åº¦ï¼ˆå•è¯ä¸ªæ•°ï¼‰: [1, 5, 10]\n",
      "ğŸ“Š Quick80æ•°æ®é›†è¯æ±‡æ€»æ•°: 80\n"
     ]
    }
   ],
   "source": [
    "# å®šé‡åˆ†ææ¨¡å‹é…ç½®\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# å®šä¹‰å®šé‡åˆ†æä½¿ç”¨çš„æ¨¡å‹\n",
    "QUANTITATIVE_MODELS = [\n",
    "    \"deepseek/deepseek-chat-v3-0324\",  # deepseek-v3  \n",
    "    \"google/gemini-2.5-flash\"  # gemini-2.5flash\n",
    "]\n",
    "\n",
    "# æ¸©åº¦å‚æ•°è®¾ç½®\n",
    "TEMPERATURE_VALUES = [0.1, 0.3, 0.7]\n",
    "\n",
    "# ç¦å¿Œè¯æ•°é‡è®¾ç½® \n",
    "TABOO_COUNTS = [1, 3, 5]\n",
    "\n",
    "# Hinteræç¤ºé•¿åº¦è®¾ç½®ï¼ˆå•è¯ä¸ªæ•°ï¼‰\n",
    "HINT_WORD_COUNTS = [1, 5, 10]\n",
    "\n",
    "print(\"âœ… Quick80å®šé‡åˆ†æé…ç½®å·²åŠ è½½\")\n",
    "print(f\"ğŸ¤– åˆ†ææ¨¡å‹: {len(QUANTITATIVE_MODELS)} ä¸ª\")\n",
    "for i, model in enumerate(QUANTITATIVE_MODELS, 1):\n",
    "    model_name = model.split('/')[-1]\n",
    "    print(f\"   {i}. {model_name}\")\n",
    "\n",
    "print(f\"ğŸŒ¡ï¸ æ¸©åº¦å‚æ•°: {TEMPERATURE_VALUES}\")\n",
    "print(f\"ğŸš« ç¦å¿Œè¯æ•°é‡: {TABOO_COUNTS}\")\n",
    "print(f\"ğŸ’¬ Hinteræç¤ºé•¿åº¦ï¼ˆå•è¯ä¸ªæ•°ï¼‰: {HINT_WORD_COUNTS}\")\n",
    "print(f\"ğŸ“Š Quick80æ•°æ®é›†è¯æ±‡æ€»æ•°: {len(dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Quick80å®šé‡åˆ†æAPIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ\n"
     ]
    }
   ],
   "source": [
    "# APIå®¢æˆ·ç«¯è®¾ç½®\n",
    "def load_api_keys(keys_path: str = \"api_keys.json\") -> Dict[str, str]:\n",
    "    \"\"\"åŠ è½½APIå¯†é’¥\"\"\"\n",
    "    with open(keys_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "class QuantitativeOpenRouterClient:\n",
    "    \"\"\"å®šé‡åˆ†æä¸“ç”¨çš„APIå®¢æˆ·ç«¯ï¼Œæ”¯æŒæ¸©åº¦æ§åˆ¶å’Œé•¿åº¦éªŒè¯\"\"\"\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "        self.headers = {\n",
    "            \"Authorization\": f\"Bearer {api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "    \n",
    "    def call_model_with_temperature(self, model: str, messages: List[Dict[str, str]], \n",
    "                                  temperature: float = 0.3, max_tokens: int = 2000) -> str:\n",
    "        \"\"\"è°ƒç”¨æ¨¡å‹APIï¼Œæ”¯æŒè‡ªå®šä¹‰æ¸©åº¦å‚æ•°\"\"\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens\n",
    "        }\n",
    "        response = requests.post(self.base_url, headers=self.headers, json=payload, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        content = result['choices'][0]['message']['content'].strip()\n",
    "        \n",
    "        # é˜²æ­¢ä¹±ç ï¼šåªä¿ç•™ASCIIå¯æ‰“å°å­—ç¬¦\n",
    "        content = re.sub(r'[^\\\\x20-\\\\x7E]', '', content)\n",
    "        return content\n",
    "\n",
    "# åˆå§‹åŒ–APIå®¢æˆ·ç«¯\n",
    "try:\n",
    "    api_keys = load_api_keys()\n",
    "    quant_client = QuantitativeOpenRouterClient(api_keys[\"OPENROUTER_API_KEY\"])\n",
    "    print(\"âœ… Quick80å®šé‡åˆ†æAPIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ APIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}\")\n",
    "    quant_client = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š æ­£åœ¨å‡†å¤‡Quick80å®šé‡åˆ†ææ•°æ®é›†...\n",
      "âœ… Quick80å®šé‡åˆ†ææ•°æ®é›†å‡†å¤‡å®Œæˆ\n",
      "ğŸ“‹ åŸå§‹è¯æ±‡æ•°: 80\n",
      "ğŸ“Š åˆ†ææ¡ç›®æ•°: 240 (æ¯ä¸ªè¯æ±‡3ä¸ªç¦å¿Œè¯å˜ä½“)\n",
      "\n",
      "ğŸš« ç¦å¿Œè¯æ•°é‡åˆ†å¸ƒ:\n",
      "   1ä¸ªç¦å¿Œè¯: 80 æ¡\n",
      "   3ä¸ªç¦å¿Œè¯: 80 æ¡\n",
      "   5ä¸ªç¦å¿Œè¯: 80 æ¡\n",
      "\n",
      "ğŸ·ï¸ ç±»åˆ«åˆ†å¸ƒ:\n",
      "   general: 165 æ¡\n",
      "   philosophy: 24 æ¡\n",
      "   finance: 21 æ¡\n",
      "   cs: 15 æ¡\n",
      "   chemistry: 15 æ¡\n",
      "\n",
      "ğŸ“‹ æ•°æ®é›†ç¤ºä¾‹:\n",
      "   å®éªŒID: farm_1taboo\n",
      "   ç›®æ ‡è¯: farm\n",
      "   åŸå§‹ç¦å¿Œè¯æ•°: 5\n",
      "   å½“å‰ç¦å¿Œè¯: ['farmer'] (1ä¸ª)\n"
     ]
    }
   ],
   "source": [
    "# Quick80ç¦å¿Œè¯æ•°é‡æ§åˆ¶å’Œæ•°æ®é›†å¤„ç†\n",
    "def create_taboo_variants(word_data, target_counts=[1, 3, 5]):\n",
    "    \"\"\"ä¸ºå•ä¸ªè¯æ±‡åˆ›å»ºä¸åŒç¦å¿Œè¯æ•°é‡çš„å˜ä½“\"\"\"\n",
    "    variants = {}\n",
    "    original_taboo = word_data['taboo']\n",
    "    \n",
    "    for count in target_counts:\n",
    "        if count <= len(original_taboo):\n",
    "            # é€‰æ‹©å‰Nä¸ªç¦å¿Œè¯\n",
    "            selected_taboo = original_taboo[:count]\n",
    "            variants[count] = {\n",
    "                **word_data,\n",
    "                'taboo': selected_taboo,\n",
    "                'original_taboo_count': len(original_taboo),\n",
    "                'variant_taboo_count': count\n",
    "            }\n",
    "        else:\n",
    "            # å¦‚æœè¦æ±‚çš„æ•°é‡è¶…è¿‡åŸå§‹ç¦å¿Œè¯æ•°é‡ï¼Œåˆ™ä½¿ç”¨å…¨éƒ¨\n",
    "            variants[count] = {\n",
    "                **word_data,\n",
    "                'taboo': original_taboo,\n",
    "                'original_taboo_count': len(original_taboo),\n",
    "                'variant_taboo_count': len(original_taboo)\n",
    "            }\n",
    "    \n",
    "    return variants\n",
    "\n",
    "def prepare_quantitative_dataset(dataset, sample_size=80):\n",
    "    \"\"\"å‡†å¤‡å®šé‡åˆ†ææ•°æ®é›†ï¼Œä¸ºæ¯ä¸ªè¯åˆ›å»ºä¸åŒç¦å¿Œè¯æ•°é‡çš„å˜ä½“\"\"\"\n",
    "    # ä½¿ç”¨å…¨éƒ¨80ä¸ªè¯æ±‡æˆ–æŒ‡å®šæ•°é‡\n",
    "    sample_words = random.sample(dataset, min(sample_size, len(dataset)))\n",
    "    \n",
    "    quantitative_dataset = []\n",
    "    \n",
    "    for word_data in sample_words:\n",
    "        variants = create_taboo_variants(word_data, TABOO_COUNTS)\n",
    "        \n",
    "        for taboo_count, variant in variants.items():\n",
    "            variant['experiment_id'] = f\"{word_data['target']}_{taboo_count}taboo\"\n",
    "            quantitative_dataset.append(variant)\n",
    "    \n",
    "    return quantitative_dataset\n",
    "\n",
    "# åˆ›å»ºQuick80å®šé‡åˆ†ææ•°æ®é›†\n",
    "print(\"ğŸ“Š æ­£åœ¨å‡†å¤‡Quick80å®šé‡åˆ†ææ•°æ®é›†...\")\n",
    "quant_dataset = prepare_quantitative_dataset(dataset, sample_size=80)  # ä½¿ç”¨å…¨éƒ¨80ä¸ªè¯æ±‡\n",
    "\n",
    "print(f\"âœ… Quick80å®šé‡åˆ†ææ•°æ®é›†å‡†å¤‡å®Œæˆ\")\n",
    "print(f\"ğŸ“‹ åŸå§‹è¯æ±‡æ•°: 80\")\n",
    "print(f\"ğŸ“Š åˆ†ææ¡ç›®æ•°: {len(quant_dataset)} (æ¯ä¸ªè¯æ±‡3ä¸ªç¦å¿Œè¯å˜ä½“)\")\n",
    "\n",
    "# ç»Ÿè®¡å„ç±»åˆ«çš„åˆ†å¸ƒ\n",
    "taboo_distribution = {}\n",
    "category_distribution = {}\n",
    "\n",
    "for item in quant_dataset:\n",
    "    # ç¦å¿Œè¯æ•°é‡åˆ†å¸ƒ\n",
    "    taboo_count = item['variant_taboo_count']\n",
    "    taboo_distribution[taboo_count] = taboo_distribution.get(taboo_count, 0) + 1\n",
    "    \n",
    "    # ç±»åˆ«åˆ†å¸ƒ\n",
    "    category = item.get('category', 'unknown')\n",
    "    category_distribution[category] = category_distribution.get(category, 0) + 1\n",
    "\n",
    "print(f\"\\nğŸš« ç¦å¿Œè¯æ•°é‡åˆ†å¸ƒ:\")\n",
    "for count, num in sorted(taboo_distribution.items()):\n",
    "    print(f\"   {count}ä¸ªç¦å¿Œè¯: {num} æ¡\")\n",
    "\n",
    "print(f\"\\nğŸ·ï¸ ç±»åˆ«åˆ†å¸ƒ:\")\n",
    "for category, num in sorted(category_distribution.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"   {category}: {num} æ¡\")\n",
    "\n",
    "# æ˜¾ç¤ºç¤ºä¾‹\n",
    "print(f\"\\nğŸ“‹ æ•°æ®é›†ç¤ºä¾‹:\")\n",
    "sample_item = quant_dataset[0]\n",
    "print(f\"   å®éªŒID: {sample_item['experiment_id']}\")\n",
    "print(f\"   ç›®æ ‡è¯: {sample_item['target']}\")\n",
    "print(f\"   åŸå§‹ç¦å¿Œè¯æ•°: {sample_item['original_taboo_count']}\")\n",
    "print(f\"   å½“å‰ç¦å¿Œè¯: {sample_item['taboo']} ({sample_item['variant_taboo_count']}ä¸ª)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Quick80å®šé‡åˆ†æå®éªŒè§„æ¨¡:\n",
      "==================================================\n",
      "ğŸ“Š æ•°æ®é›†: Quick80 (80 ä¸ªåŸå§‹è¯æ±‡)\n",
      "ğŸ¤– æ¨¡å‹: 2 ä¸ª\n",
      "ğŸ”„ æ¨¡å‹ç»„åˆ: 4 ä¸ª (æ¯ä¸ªæ¨¡å‹æ—¢åšHinteråˆåšGuesser)\n",
      "ğŸŒ¡ï¸ æ¸©åº¦å‚æ•°: 3 ä¸ª\n",
      "ğŸ’¬ Hinteræç¤ºé•¿åº¦: 3 ä¸ª\n",
      "ğŸ“ æ•°æ®é›†å˜ä½“: 240 ä¸ª (80è¯æ±‡ Ã— 3ç§ç¦å¿Œè¯æ•°é‡)\n",
      "\n",
      "ğŸ® æ€»å®éªŒæ•°: 8,640 ä¸ª\n",
      "â±ï¸ é¢„è®¡è€—æ—¶: 1.2 å°æ—¶ (æŒ‰æ¯ä¸ªå®éªŒ0.5ç§’ä¼°ç®—)\n",
      "\n",
      "ğŸ’¡ å®éªŒè®¾è®¡ç‰¹ç‚¹:\n",
      "   âœ… æ¶µç›–Quick80æ•°æ®é›†çš„å…¨éƒ¨80ä¸ªè¯æ±‡\n",
      "   âœ… æµ‹è¯•3ä¸ªç›®æ ‡æ¨¡å‹çš„å®Œæ•´æ€§èƒ½\n",
      "   âœ… åˆ†æ4ä¸ªæ¸©åº¦å‚æ•°çš„å½±å“\n",
      "   âœ… éªŒè¯3ç§Hinteræç¤ºé•¿åº¦çš„æ•ˆæœ\n",
      "   âœ… æ¯”è¾ƒ3ç§ç¦å¿Œè¯æ•°é‡çš„éš¾åº¦\n",
      "   âœ… æ”¯æŒå°è§„æ¨¡æµ‹è¯•å’Œå®Œæ•´å®éªŒ\n",
      "\n",
      "ğŸ“‹ å…³é”®æ›´æ–°:\n",
      "   ğŸ”„ æ•°æ®é›†: ä»dataset.json (300è¯) æ”¹ä¸º quick80_dataset.json (80è¯)\n",
      "   ğŸ“Š å®éªŒè§„æ¨¡: æ›´åŠ èšç„¦ï¼Œé€‚åˆå¿«é€Ÿè¿­ä»£å’Œæ·±åº¦åˆ†æ\n",
      "   ğŸ’¾ æ•°æ®è´¨é‡: Quick80æ˜¯é«˜è´¨é‡ç²¾é€‰è¯æ±‡é›†\n"
     ]
    }
   ],
   "source": [
    "# å®éªŒæ€»è§ˆ\n",
    "# è®¡ç®—å®éªŒè§„æ¨¡\n",
    "total_models = len(QUANTITATIVE_MODELS)\n",
    "total_combinations = total_models * total_models  # hinter x guesser\n",
    "total_temperatures = len(TEMPERATURE_VALUES)\n",
    "total_hint_lengths = len(HINT_WORD_COUNTS)\n",
    "total_dataset_variants = len(quant_dataset)\n",
    "\n",
    "total_experiments = total_combinations * total_temperatures * total_hint_lengths * total_dataset_variants\n",
    "\n",
    "print(\"ğŸ¯ Quick80å®šé‡åˆ†æå®éªŒè§„æ¨¡:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ğŸ“Š æ•°æ®é›†: Quick80 ({len(dataset)} ä¸ªåŸå§‹è¯æ±‡)\")\n",
    "print(f\"ğŸ¤– æ¨¡å‹: {total_models} ä¸ª\")\n",
    "print(f\"ğŸ”„ æ¨¡å‹ç»„åˆ: {total_combinations} ä¸ª (æ¯ä¸ªæ¨¡å‹æ—¢åšHinteråˆåšGuesser)\")\n",
    "print(f\"ğŸŒ¡ï¸ æ¸©åº¦å‚æ•°: {total_temperatures} ä¸ª\")\n",
    "print(f\"ğŸ’¬ Hinteræç¤ºé•¿åº¦: {total_hint_lengths} ä¸ª\")\n",
    "print(f\"ğŸ“ æ•°æ®é›†å˜ä½“: {total_dataset_variants} ä¸ª (80è¯æ±‡ Ã— 3ç§ç¦å¿Œè¯æ•°é‡)\")\n",
    "print(f\"\\nğŸ® æ€»å®éªŒæ•°: {total_experiments:,} ä¸ª\")\n",
    "print(f\"â±ï¸ é¢„è®¡è€—æ—¶: {total_experiments * 0.5 / 3600:.1f} å°æ—¶ (æŒ‰æ¯ä¸ªå®éªŒ0.5ç§’ä¼°ç®—)\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ å®éªŒè®¾è®¡ç‰¹ç‚¹:\")\n",
    "print(f\"   âœ… æ¶µç›–Quick80æ•°æ®é›†çš„å…¨éƒ¨80ä¸ªè¯æ±‡\")\n",
    "print(f\"   âœ… æµ‹è¯•3ä¸ªç›®æ ‡æ¨¡å‹çš„å®Œæ•´æ€§èƒ½\")\n",
    "print(f\"   âœ… åˆ†æ4ä¸ªæ¸©åº¦å‚æ•°çš„å½±å“\")\n",
    "print(f\"   âœ… éªŒè¯3ç§Hinteræç¤ºé•¿åº¦çš„æ•ˆæœ\")\n",
    "print(f\"   âœ… æ¯”è¾ƒ3ç§ç¦å¿Œè¯æ•°é‡çš„éš¾åº¦\")\n",
    "print(f\"   âœ… æ”¯æŒå°è§„æ¨¡æµ‹è¯•å’Œå®Œæ•´å®éªŒ\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ å…³é”®æ›´æ–°:\")\n",
    "print(f\"   ğŸ”„ æ•°æ®é›†: ä»dataset.json (300è¯) æ”¹ä¸º quick80_dataset.json (80è¯)\")\n",
    "print(f\"   ğŸ“Š å®éªŒè§„æ¨¡: æ›´åŠ èšç„¦ï¼Œé€‚åˆå¿«é€Ÿè¿­ä»£å’Œæ·±åº¦åˆ†æ\")\n",
    "print(f\"   ğŸ’¾ æ•°æ®è´¨é‡: Quick80æ˜¯é«˜è´¨é‡ç²¾é€‰è¯æ±‡é›†\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª å¼€å§‹Quick80å°è§„æ¨¡æµ‹è¯•...\n",
      "ğŸ“Š æµ‹è¯•é…ç½®:\n",
      "   æ¨¡å‹: ['deepseek-chat-v3-0324', 'gemini-2.5-flash']\n",
      "   æ¸©åº¦: [0.1, 0.3]\n",
      "   ç¦å¿Œè¯æ•°é‡: [1, 3, 5]\n",
      "   æç¤ºé•¿åº¦: [1, 5]\n",
      "   æµ‹è¯•è¯æ±‡æ•°: 5\n",
      "   æµ‹è¯•æ•°æ®é›†: 15 æ¡è®°å½•\n",
      "   æ€»æµ‹è¯•å®éªŒæ•°: 240\n",
      "   é¢„è®¡è€—æ—¶: 2.0 åˆ†é’Ÿ\n",
      "\n",
      "âš ï¸ è¿™æ˜¯ä¸€ä¸ªå°è§„æ¨¡æµ‹è¯•ï¼Œç”¨äºéªŒè¯ç³»ç»ŸåŠŸèƒ½\n",
      "ğŸ’¡ å¦‚æœæµ‹è¯•æˆåŠŸï¼Œæ‚¨å¯ä»¥è¿è¡Œå®Œæ•´çš„Quick80å®šé‡åˆ†æå®éªŒ\n"
     ]
    }
   ],
   "source": [
    "# å°è§„æ¨¡æµ‹è¯•å®éªŒ\n",
    "print(\"ğŸ§ª å¼€å§‹Quick80å°è§„æ¨¡æµ‹è¯•...\")\n",
    "print(\"ğŸ“Š æµ‹è¯•é…ç½®:\")\n",
    "print(f\"   æ¨¡å‹: {[m.split('/')[-1] for m in QUANTITATIVE_MODELS]}\")\n",
    "print(f\"   æ¸©åº¦: {TEMPERATURE_VALUES[:2]}\")  # åªç”¨å‰2ä¸ªæ¸©åº¦\n",
    "print(f\"   ç¦å¿Œè¯æ•°é‡: {TABOO_COUNTS}\")\n",
    "print(f\"   æç¤ºé•¿åº¦: {HINT_WORD_COUNTS[:2]}\")  # åªç”¨å‰2ä¸ªé•¿åº¦\n",
    "print(f\"   æµ‹è¯•è¯æ±‡æ•°: 5\")\n",
    "\n",
    "# åˆ›å»ºå°è§„æ¨¡æµ‹è¯•æ•°æ®é›†\n",
    "test_dataset = prepare_quantitative_dataset(dataset, sample_size=5)  # åªç”¨5ä¸ªè¯æ±‡\n",
    "print(f\"   æµ‹è¯•æ•°æ®é›†: {len(test_dataset)} æ¡è®°å½•\")\n",
    "\n",
    "# è®¡ç®—æµ‹è¯•å®éªŒæ•°\n",
    "total_test_experiments = len(QUANTITATIVE_MODELS) * len(QUANTITATIVE_MODELS) * len(test_dataset) * 2 * 2  # 2æ¸©åº¦ x 2é•¿åº¦\n",
    "print(f\"   æ€»æµ‹è¯•å®éªŒæ•°: {total_test_experiments}\")\n",
    "print(f\"   é¢„è®¡è€—æ—¶: {total_test_experiments * 0.5 / 60:.1f} åˆ†é’Ÿ\")\n",
    "\n",
    "print(f\"\\nâš ï¸ è¿™æ˜¯ä¸€ä¸ªå°è§„æ¨¡æµ‹è¯•ï¼Œç”¨äºéªŒè¯ç³»ç»ŸåŠŸèƒ½\")\n",
    "print(f\"ğŸ’¡ å¦‚æœæµ‹è¯•æˆåŠŸï¼Œæ‚¨å¯ä»¥è¿è¡Œå®Œæ•´çš„Quick80å®šé‡åˆ†æå®éªŒ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ç®€åŒ–ç‰ˆTabooæ¸¸æˆå‡½æ•°å·²å®šä¹‰\n"
     ]
    }
   ],
   "source": [
    "# ç®€åŒ–ç‰ˆTabooæ¸¸æˆå‡½æ•° - ç”¨äºQuick80æµ‹è¯•\n",
    "def simple_taboo_game(client, hinter_model, guesser_model, target_word, taboo_words, \n",
    "                     temperature=0.3, hint_word_count=5, max_turns=5):\n",
    "    \"\"\"ç®€åŒ–ç‰ˆTabooæ¸¸æˆï¼Œç”¨äºQuick80å®šé‡åˆ†æ\"\"\"\n",
    "    \n",
    "    conversation_history = []\n",
    "    taboo_list = ', '.join(taboo_words)\n",
    "    \n",
    "    # ç³»ç»Ÿæç¤º\n",
    "    hinter_system = f\"\"\"You are playing a Taboo word game as the Hinter.\n",
    "\n",
    "CRITICAL FORMAT REQUIREMENT:\n",
    "- You MUST start your response with exactly '[CLUE]' (including square brackets)\n",
    "- Your clue must contain exactly {hint_word_count} words\n",
    "- Do NOT add any text before [CLUE]\n",
    "\n",
    "GAME RULES:\n",
    "1. NEVER use any of these taboo words: {taboo_list}\n",
    "2. Give helpful clues to help the guesser find the target word\n",
    "3. Use BRITISH ENGLISH spelling\n",
    "\n",
    "Target word: {target_word}\n",
    "Taboo words (ABSOLUTELY FORBIDDEN): {taboo_list}\"\"\"\n",
    "\n",
    "    guesser_system = \"\"\"You are playing a Taboo word game as the Guesser.\n",
    "\n",
    "CRITICAL FORMAT REQUIREMENT:\n",
    "- You MUST start your response with exactly '[GUESS]' (including square brackets)\n",
    "- Give only ONE word as your guess after [GUESS]\n",
    "\n",
    "GAME RULES:\n",
    "1. Make your best guess based on the clues\n",
    "2. Use BRITISH ENGLISH spelling\"\"\"\n",
    "\n",
    "    for turn in range(1, max_turns + 1):\n",
    "        try:\n",
    "            # Hinterç»™å‡ºçº¿ç´¢\n",
    "            if turn == 1:\n",
    "                hinter_prompt = f\"{hinter_system}\\n\\nProvide your first clue:\"\n",
    "            else:\n",
    "                history_text = \"\\n\".join(conversation_history)\n",
    "                hinter_prompt = f\"{hinter_system}\\n\\nConversation so far:\\n{history_text}\\n\\nProvide your next clue:\"\n",
    "            \n",
    "            hinter_response = client.call_model_with_temperature(\n",
    "                hinter_model, [{\"role\": \"user\", \"content\": hinter_prompt}], temperature\n",
    "            )\n",
    "            \n",
    "            # æ£€æŸ¥æ ¼å¼å’Œç¦å¿Œè¯\n",
    "            if not hinter_response.strip().upper().startswith('[CLUE]'):\n",
    "                return {\n",
    "                    'success': False, 'turns': turn, 'final_guess': 'FORMAT_ERROR',\n",
    "                    'failure_reason': 'HINTER_FORMAT_ERROR', 'temperature': temperature,\n",
    "                    'hint_word_count': hint_word_count, 'conversation': conversation_history\n",
    "                }\n",
    "            \n",
    "            # æå–çº¿ç´¢æ–‡æœ¬\n",
    "            import re\n",
    "            match = re.search(r'\\[CLUE\\]\\s*(.+)', hinter_response, re.IGNORECASE)\n",
    "            if match:\n",
    "                hint_text = match.group(1).strip()\n",
    "            else:\n",
    "                return {\n",
    "                    'success': False, 'turns': turn, 'final_guess': 'INVALID_CLUE',\n",
    "                    'failure_reason': 'CLUE_EXTRACTION_ERROR', 'temperature': temperature,\n",
    "                    'hint_word_count': hint_word_count, 'conversation': conversation_history\n",
    "                }\n",
    "            \n",
    "            # æ£€æŸ¥ç¦å¿Œè¯è¿è§„\n",
    "            hint_lower = hint_text.lower()\n",
    "            for taboo in taboo_words:\n",
    "                if taboo.lower() in hint_lower:\n",
    "                    return {\n",
    "                        'success': False, 'turns': turn, 'final_guess': 'TABOO_VIOLATION',\n",
    "                        'failure_reason': 'TABOO_VIOLATION', 'temperature': temperature,\n",
    "                        'hint_word_count': hint_word_count, 'conversation': conversation_history,\n",
    "                        'taboo_violation_hint': hint_text\n",
    "                    }\n",
    "            \n",
    "            # æ£€æŸ¥æç¤ºé•¿åº¦\n",
    "            word_count = len(hint_text.split())\n",
    "            if word_count != hint_word_count:\n",
    "                return {\n",
    "                    'success': False, 'turns': turn, 'final_guess': 'HINT_LENGTH_ERROR',\n",
    "                    'failure_reason': 'HINT_LENGTH_FAILURE', 'temperature': temperature,\n",
    "                    'hint_word_count': hint_word_count, 'conversation': conversation_history,\n",
    "                    'actual_word_count': word_count\n",
    "                }\n",
    "            \n",
    "            conversation_history.append(f\"Hinter: {hinter_response}\")\n",
    "            \n",
    "            # Guesserè¿›è¡ŒçŒœæµ‹\n",
    "            history_text = \"\\n\".join(conversation_history)\n",
    "            guesser_prompt = f\"{guesser_system}\\n\\nConversation so far:\\n{history_text}\\n\\nWhat is your guess?\"\n",
    "            \n",
    "            guesser_response = client.call_model_with_temperature(\n",
    "                guesser_model, [{\"role\": \"user\", \"content\": guesser_prompt}], temperature\n",
    "            )\n",
    "            \n",
    "            # æ£€æŸ¥guesseræ ¼å¼\n",
    "            if not guesser_response.strip().upper().startswith('[GUESS]'):\n",
    "                return {\n",
    "                    'success': False, 'turns': turn, 'final_guess': 'GUESSER_FORMAT_ERROR',\n",
    "                    'failure_reason': 'GUESSER_FORMAT_ERROR', 'temperature': temperature,\n",
    "                    'hint_word_count': hint_word_count, 'conversation': conversation_history\n",
    "                }\n",
    "            \n",
    "            conversation_history.append(f\"Guesser: {guesser_response}\")\n",
    "            \n",
    "            # æå–çŒœæµ‹\n",
    "            match = re.search(r'\\[GUESS\\]\\s*(.+)', guesser_response, re.IGNORECASE)\n",
    "            if match:\n",
    "                guess = match.group(1).strip().split()[0]  # å–ç¬¬ä¸€ä¸ªè¯\n",
    "                guess = guess.strip('.,!?;:\"\\'()[]{}')\n",
    "            else:\n",
    "                guess = 'INVALID_GUESS'\n",
    "            \n",
    "            # æ£€æŸ¥æ˜¯å¦æˆåŠŸ\n",
    "            if guess.lower() == target_word.lower():\n",
    "                return {\n",
    "                    'success': True, 'turns': turn, 'final_guess': guess,\n",
    "                    'failure_reason': None, 'temperature': temperature,\n",
    "                    'hint_word_count': hint_word_count, 'conversation': conversation_history\n",
    "                }\n",
    "            \n",
    "            # å¦‚æœä¸æ˜¯æœ€åä¸€è½®ï¼Œæ·»åŠ åé¦ˆ\n",
    "            if turn < max_turns:\n",
    "                conversation_history.append(f\"System: '{guess}' is not correct. Try again!\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'success': False, 'turns': turn, 'final_guess': 'API_ERROR',\n",
    "                'failure_reason': 'API_FAILURE', 'temperature': temperature,\n",
    "                'hint_word_count': hint_word_count, 'conversation': conversation_history,\n",
    "                'error': str(e)[:200]\n",
    "            }\n",
    "    \n",
    "    # è¾¾åˆ°æœ€å¤§è½®æ•°ä»æœªæˆåŠŸ\n",
    "    return {\n",
    "        'success': False, 'turns': max_turns, 'final_guess': guess if 'guess' in locals() else 'N/A',\n",
    "        'failure_reason': 'MAX_TURNS_EXCEEDED', 'temperature': temperature,\n",
    "        'hint_word_count': hint_word_count, 'conversation': conversation_history\n",
    "    }\n",
    "\n",
    "print(\"âœ… ç®€åŒ–ç‰ˆTabooæ¸¸æˆå‡½æ•°å·²å®šä¹‰\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 204)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mFile \u001b[39m\u001b[32m<tokenize>:204\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m\"metadata\": {\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ è¿è¡ŒQuick80å°è§„æ¨¡å®šé‡åˆ†æå®éªŒ\n",
    "if quant_client is not None:\n",
    "    print(\"ğŸš€ å¼€å§‹è¿è¡ŒQuick80å°è§„æ¨¡å®šé‡åˆ†æå®éªŒ...\")\n",
    "    \n",
    "    # å®éªŒé…ç½®\n",
    "    test_models = QUANTITATIVE_MODELS\n",
    "    test_temperatures = [0.1, 0.3]  # åªç”¨2ä¸ªæ¸©åº¦è¿›è¡Œæµ‹è¯•\n",
    "    test_hint_lengths = [1, 5]      # åªç”¨2ä¸ªæç¤ºé•¿åº¦è¿›è¡Œæµ‹è¯•\n",
    "    test_sample = test_dataset[:15]  # åªç”¨å‰15ä¸ªæ•°æ®é¡¹ï¼ˆ5è¯æ±‡Ã—3ç¦å¿Œè¯å˜ä½“ï¼‰\n",
    "    \n",
    "    total_test_experiments = len(test_models) * len(test_models) * len(test_sample) * len(test_temperatures) * len(test_hint_lengths)\n",
    "    \n",
    "    print(f\"ğŸ“Š æµ‹è¯•é…ç½®:\")\n",
    "    print(f\"   æ¨¡å‹: {[m.split('/')[-1] for m in test_models]}\")\n",
    "    print(f\"   æ¸©åº¦: {test_temperatures}\")\n",
    "    print(f\"   æç¤ºé•¿åº¦: {test_hint_lengths}\")\n",
    "    print(f\"   æ•°æ®é¡¹: {len(test_sample)} ä¸ª\")\n",
    "    print(f\"   æ€»å®éªŒæ•°: {total_test_experiments}\")\n",
    "    print(f\"   é¢„è®¡è€—æ—¶: {total_test_experiments * 0.8 / 60:.1f} åˆ†é’Ÿ\")\n",
    "    \n",
    "    # è¿è¡Œå®éªŒ\n",
    "    results = []\n",
    "    experiment_counter = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for hinter_model in test_models:\n",
    "        for guesser_model in test_models:\n",
    "            for temperature in test_temperatures:\n",
    "                for hint_length in test_hint_lengths:\n",
    "                    for data_item in test_sample:\n",
    "                        experiment_counter += 1\n",
    "                        \n",
    "                        # è¿›åº¦æ˜¾ç¤º\n",
    "                        progress = (experiment_counter / total_test_experiments) * 100\n",
    "                        hinter_name = hinter_model.split('/')[-1]\n",
    "                        guesser_name = guesser_model.split('/')[-1]\n",
    "                        \n",
    "                        print(f\\\"\\\\rğŸ”„ å®éªŒ {experiment_counter}/{total_test_experiments} ({progress:.1f}%) | {hinter_name}â†’{guesser_name} | T={temperature} | L={hint_length} | {data_item['target']}\\\", end=\\\"\\\")\\n\",\n",
    "                        \\n\",\n",
    "                        # æ‰§è¡Œæ¸¸æˆ\\n\",\n",
    "                        game_start = time.time()\\n\",\n",
    "                        game_result = simple_taboo_game(\\n\",\n",
    "                            quant_client, hinter_model, guesser_model,\\n\",\n",
    "                            data_item['target'], data_item['taboo'],\\n\",\n",
    "                            temperature=temperature,\\n\",\n",
    "                            hint_word_count=hint_length,\\n\",\n",
    "                            max_turns=5\\n\",\n",
    "                        )\\n\",\n",
    "                        game_duration = round(time.time() - game_start, 2)\\n\",\n",
    "                        \\n\",\n",
    "                        # è®°å½•ç»“æœ\\n\",\n",
    "                        result = {\\n\",\n",
    "                            'experiment_id': f\\\"{experiment_counter:04d}\\\",\\n\",\n",
    "                            'hinter_model': hinter_model,\\n\",\n",
    "                            'guesser_model': guesser_model,\\n\",\n",
    "                            'target_word': data_item['target'],\\n\",\n",
    "                            'category': data_item.get('category', 'unknown'),\\n\",\n",
    "                            'taboo_words': '|'.join(data_item['taboo']),\\n\",\n",
    "                            'taboo_count': len(data_item['taboo']),\\n\",\n",
    "                            'variant_taboo_count': data_item.get('variant_taboo_count', len(data_item['taboo'])),\\n\",\n",
    "                            'temperature': temperature,\\n\",\n",
    "                            'hint_word_count': hint_length,\\n\",\n",
    "                            'success': game_result['success'],\\n\",\n",
    "                            'turns_used': game_result['turns'],\\n\",\n",
    "                            'final_guess': game_result['final_guess'],\\n\",\n",
    "                            'failure_reason': game_result.get('failure_reason', None),\\n\",\n",
    "                            'taboo_violation': game_result.get('failure_reason') == 'TABOO_VIOLATION',\\n\",\n",
    "                            'hint_length_violation': game_result.get('failure_reason') == 'HINT_LENGTH_FAILURE',\\n\",\n",
    "                            'actual_word_count': game_result.get('actual_word_count', hint_length),\\n\",\n",
    "                            'conversation_length': len(game_result.get('conversation', [])),\\n\",\n",
    "                            'duration_seconds': game_duration,\\n\",\n",
    "                            'timestamp': datetime.now().strftime(\\\"%Y-%m-%d %H:%M:%S\\\")\\n\",\n",
    "                        }\\n\",\n",
    "                        \\n\",\n",
    "                        if 'error' in game_result:\\n\",\n",
    "                            result['error'] = game_result['error']\\n\",\n",
    "                        if 'taboo_violation_hint' in game_result:\\n\",\n",
    "                            result['taboo_violation_hint'] = game_result['taboo_violation_hint']\\n\",\n",
    "                        \\n\",\n",
    "                        results.append(result)\\n\",\n",
    "                        \\n\",\n",
    "                        # æ¯10ä¸ªå®éªŒæ˜¾ç¤ºä¸€æ¬¡ä¸­é—´ç»Ÿè®¡\\n\",\n",
    "                        if experiment_counter % 10 == 0:\\n\",\n",
    "                            recent_results = results[-10:]\\n\",\n",
    "                            recent_success = sum(r['success'] for r in recent_results)\\n\",\n",
    "                            print(f\\\"\\\\n   ğŸ“Š æœ€è¿‘10ä¸ª: æˆåŠŸ{recent_success}/10 ({recent_success*10}%)\\\")\\n\",\n",
    "                        \\n\",\n",
    "                        time.sleep(0.5)  # APIè°ƒç”¨é—´éš”\\n\",\n",
    "    \\n\",\n",
    "    \"    total_duration = time.time() - start_time\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\n\\\\nâœ… Quick80å°è§„æ¨¡å®éªŒå®Œæˆï¼\\\")\\n\",\n",
    "    \"    print(f\\\"â±ï¸ æ€»è€—æ—¶: {total_duration/60:.1f} åˆ†é’Ÿ\\\")\\n\",\n",
    "    \"    print(f\\\"ğŸ“Š å®éªŒç»“æœ: {len(results)} ä¸ª\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # åŸºæœ¬ç»Ÿè®¡\\n\",\n",
    "    \"    if results:\\n\",\n",
    "    \"        df = pd.DataFrame(results)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        total_success = sum(r['success'] for r in results)\\n\",\n",
    "    \"        success_rate = total_success / len(results) * 100\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        print(f\\\"ğŸ“ˆ æ•´ä½“æˆåŠŸç‡: {success_rate:.1f}% ({total_success}/{len(results)})\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # æŒ‰æ¸©åº¦ç»Ÿè®¡\\n\",\n",
    "    \"        print(f\\\"\\\\nğŸŒ¡ï¸ æ¸©åº¦å½±å“:\\\")\\n\",\n",
    "    \"        for temp in test_temperatures:\\n\",\n",
    "    \"            temp_results = df[df['temperature'] == temp]\\n\",\n",
    "    \"            temp_success = temp_results['success'].sum()\\n\",\n",
    "    \"            temp_rate = temp_success / len(temp_results) * 100 if len(temp_results) > 0 else 0\\n\",\n",
    "    \"            print(f\\\"   T={temp}: {temp_success}/{len(temp_results)} ({temp_rate:.1f}%)\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # æŒ‰æç¤ºé•¿åº¦ç»Ÿè®¡\\n\",\n",
    "    \"        print(f\\\"\\\\nğŸ’¬ æç¤ºé•¿åº¦å½±å“:\\\")\\n\",\n",
    "    \"        for length in test_hint_lengths:\\n\",\n",
    "    \"            length_results = df[df['hint_word_count'] == length]\\n\",\n",
    "    \"            length_success = length_results['success'].sum()\\n\",\n",
    "    \"            length_rate = length_success / len(length_results) * 100 if len(length_results) > 0 else 0\\n\",\n",
    "    \"            print(f\\\"   {length}è¯: {length_success}/{len(length_results)} ({length_rate:.1f}%)\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # æŒ‰æ¨¡å‹ç»Ÿè®¡\\n\",\n",
    "    \"        print(f\\\"\\\\nğŸ¤– æ¨¡å‹æ€§èƒ½ (Hinter):\\\")\\n\",\n",
    "    \"        for model in test_models:\\n\",\n",
    "    \"            model_results = df[df['hinter_model'] == model]\\n\",\n",
    "    \"            model_success = model_results['success'].sum()\\n\",\n",
    "    \"            model_rate = model_success / len(model_results) * 100 if len(model_results) > 0 else 0\\n\",\n",
    "    \"            model_name = model.split('/')[-1]\\n\",\n",
    "    \"            print(f\\\"   {model_name}: {model_success}/{len(model_results)} ({model_rate:.1f}%)\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # å¤±è´¥åŸå› ç»Ÿè®¡\\n\",\n",
    "    \"        failed_results = df[df['success'] == False]\\n\",\n",
    "    \"        if len(failed_results) > 0:\\n\",\n",
    "    \"            print(f\\\"\\\\nâŒ å¤±è´¥åŸå› :\\\")\\n\",\n",
    "    \"            failure_counts = failed_results['failure_reason'].value_counts()\\n\",\n",
    "    \"            for reason, count in failure_counts.items():\\n\",\n",
    "    \"                percentage = count / len(failed_results) * 100\\n\",\n",
    "    \"                print(f\\\"   {reason}: {count} æ¬¡ ({percentage:.1f}%)\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # ä¿å­˜ç»“æœ\\n\",\n",
    "    \"        timestamp = datetime.now().strftime(\\\"%Y%m%d_%H%M%S\\\")\\n\",\n",
    "    \"        output_path = f\\\"results/quick80_test_{timestamp}.csv\\\"\\n\",\n",
    "    \"        os.makedirs(\\\"results\\\", exist_ok=True)\\n\",\n",
    "    \"        df.to_csv(output_path, index=False, encoding='utf-8')\\n\",\n",
    "    \"        print(f\\\"\\\\nğŸ’¾ ç»“æœå·²ä¿å­˜: {output_path}\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        print(f\\\"\\\\nğŸ‰ Quick80å°è§„æ¨¡å®éªŒæˆåŠŸå®Œæˆï¼\\\")\\n\",\n",
    "    \"        print(f\\\"ğŸ’¡ å¦‚æœç»“æœæ»¡æ„ï¼Œå¯ä»¥è¿è¡Œå®Œæ•´çš„Quick80å®šé‡åˆ†æå®éªŒ\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"âŒ APIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•è¿è¡Œå®éªŒ\\\")\\n\",\n",
    "    \"    print(\\\"ğŸ’¡ è¯·æ£€æŸ¥api_keys.jsonæ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”åŒ…å«æ­£ç¡®çš„APIå¯†é’¥\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"raw\",\n",
    "   \"metadata\": {\n",
    "    \"vscode\": {\n",
    "     \"languageId\": \"raw\"\n",
    "    }\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"# æ‰§è¡ŒQuick80å®Œæ•´å®šé‡åˆ†æå®éªŒï¼ˆå¯é€‰ï¼‰\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# ğŸš€ è¿è¡ŒQuick80å®Œæ•´å®šé‡åˆ†æå®éªŒï¼ˆå–æ¶ˆæ³¨é‡Šä»¥è¿è¡Œï¼‰\\n\",\n",
    "    \"\\\"\\\"\\\"\\n\",\n",
    "    \"if quant_client is not None and input(\\\"æ˜¯å¦è¿è¡Œå®Œæ•´å®éªŒï¼Ÿ(è¾“å…¥'yes'ç¡®è®¤): \\\") == 'yes':\\n\",\n",
    "    \"    print(\\\"ğŸš€ å¼€å§‹è¿è¡ŒQuick80å®Œæ•´å®šé‡åˆ†æå®éªŒ...\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # ä½¿ç”¨å…¨éƒ¨é…ç½®\\n\",\n",
    "    \"    full_models = QUANTITATIVE_MODELS\\n\",\n",
    "    \"    full_temperatures = TEMPERATURE_VALUES  # å…¨éƒ¨4ä¸ªæ¸©åº¦\\n\",\n",
    "    \"    full_hint_lengths = HINT_WORD_COUNTS    # å…¨éƒ¨3ä¸ªæç¤ºé•¿åº¦\\n\",\n",
    "    \"    full_dataset = quant_dataset            # å…¨éƒ¨240ä¸ªæ•°æ®é¡¹\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    total_full_experiments = len(full_models) * len(full_models) * len(full_dataset) * len(full_temperatures) * len(full_hint_lengths)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"ğŸ“Š å®Œæ•´å®éªŒé…ç½®:\\\")\\n\",\n",
    "    \"    print(f\\\"   æ¨¡å‹ç»„åˆ: {len(full_models)}Ã—{len(full_models)} = {len(full_models)**2}\\\")\\n\",\n",
    "    \"    print(f\\\"   æ¸©åº¦å€¼: {len(full_temperatures)} ä¸ª\\\")\\n\",\n",
    "    \"    print(f\\\"   æç¤ºé•¿åº¦: {len(full_hint_lengths)} ä¸ª\\\")\\n\",\n",
    "    \"    print(f\\\"   æ•°æ®é¡¹: {len(full_dataset)} ä¸ª\\\")\\n\",\n",
    "    \"    print(f\\\"   æ€»å®éªŒæ•°: {total_full_experiments:,}\\\")\\n\",\n",
    "    \"    print(f\\\"   é¢„è®¡è€—æ—¶: {total_full_experiments * 0.8 / 3600:.1f} å°æ—¶\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # è¿™é‡Œå¯ä»¥æ·»åŠ å®Œæ•´å®éªŒçš„ä»£ç ...\\n\",\n",
    "    \"    print(\\\"\\\\nğŸ’¡ å®Œæ•´å®éªŒä»£ç å¯ä»¥åŸºäºä¸Šé¢çš„å°è§„æ¨¡å®éªŒè¿›è¡Œæ‰©å±•\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"ğŸ’¡ å®Œæ•´å®éªŒæœªè¿è¡Œ\\\")\\n\",\n",
    "    \"\\\"\\\"\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"ğŸ’¡ å®Œæ•´å®éªŒä»£ç å·²å‡†å¤‡ï¼Œéœ€è¦æ—¶å¯ä»¥å–æ¶ˆæ³¨é‡Šè¿è¡Œ\\\")\\n\",\n",
    "    \"print(\\\"ğŸ“‹ å»ºè®®: å…ˆåˆ†æå°è§„æ¨¡å®éªŒç»“æœï¼Œç¡®è®¤é…ç½®æ— è¯¯åå†è¿è¡Œå®Œæ•´å®éªŒ\\\")\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.11.11\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
