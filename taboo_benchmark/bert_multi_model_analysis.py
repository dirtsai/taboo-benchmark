# BERTè¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ - å¤šæ¨¡å‹å¤šè½®æ¬¡ç‰ˆæœ¬
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from transformers import BertTokenizer, BertModel
import torch
from sklearn.metrics.pairwise import cosine_similarity
import json
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class BERTSimilarityAnalyzer:
    """BERTè¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æå™¨"""
    
    def __init__(self, model_name='bert-base-chinese'):
        """åˆå§‹åŒ–BERTæ¨¡å‹"""
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.tokenizer = BertTokenizer.from_pretrained(model_name)
        self.model = BertModel.from_pretrained(model_name)
        self.model.to(self.device)
        self.model.eval()
        print(f"âœ… BERTæ¨¡å‹å·²åŠ è½½åˆ° {self.device}")
    
    def get_embedding(self, text, max_length=10):
        """è·å–æ–‡æœ¬çš„BERTåµŒå…¥å‘é‡"""
        try:
            inputs = self.tokenizer(
                text, 
                return_tensors='pt', 
                truncation=True, 
                max_length=max_length,
                padding=True
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            
            return embedding
        except Exception as e:
            print(f"è·å–åµŒå…¥å‘é‡å¤±è´¥: {text}, é”™è¯¯: {e}")
            return np.zeros((1, 768))
    
    def calculate_similarity(self, text1, text2):
        """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
        emb1 = self.get_embedding(text1)
        emb2 = self.get_embedding(text2)
        similarity = cosine_similarity(emb1, emb2)[0][0]
        return similarity
    
    def analyze_game_similarities(self, game_data):
        """åˆ†æå•åœºæ¸¸æˆçš„ç›¸ä¼¼åº¦å˜åŒ–"""
        target_word = game_data['target_word']
        all_guesses = game_data['all_guesses']
        
        similarities = []
        for i, guess in enumerate(all_guesses):
            similarity = self.calculate_similarity(guess, target_word)
            similarities.append({
                'turn_number': i + 1,
                'guess': guess,
                'target': target_word,
                'similarity': similarity
            })
        
        return similarities

def load_experiment_data():
    """åŠ è½½å®éªŒæ•°æ®"""
    try:
        with open('data/dataset.json', 'r', encoding='utf-8') as f:
            dataset = json.load(f)
        
        results_df = pd.read_csv('results/taboo_experiment_20250712_004918/complete_experiment_results.csv')
        
        dataset_df = pd.DataFrame(dataset)
        dataset_info = dataset_df[['target', 'part_of_speech']].copy()
        dataset_info = dataset_info.rename(columns={'target': 'target_word'})
        
        merged_df = results_df.merge(dataset_info, on='target_word', how='left')
        
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {len(merged_df)} æ¡è®°å½•")
        return merged_df
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None

import re

def extract_guesses_from_all_guesses(all_guesses_str):
    """ä»all_guesseså­—æ®µæå–æ¯è½®çŒœæµ‹è¯"""
    if pd.isna(all_guesses_str):
        return []
    items = all_guesses_str.split('|')
    guesses = []
    for item in items:
        match = re.search(r'\[GUESS\]\s*(.+)', item)
        if match:
            guess = match.group(1).strip()
            if guess and guess not in ['INVALID_FORMAT', 'FORMAT_ERROR']:
                guesses.append(guess)
    return guesses

def run_bert_similarity_analysis():
    """è¿è¡ŒBERTç›¸ä¼¼åº¦åˆ†æï¼ˆæ”¯æŒå¤šæ¨¡å‹å¤šè½®æ¬¡åˆ†æï¼‰"""
    print("ğŸ” å¼€å§‹BERTè¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ...")
    
    analyzer = BERTSimilarityAnalyzer()
    merged_df = load_experiment_data()
    if merged_df is None:
        return None
    
    print("ğŸ“Š æå–çŒœæµ‹æ•°æ®...")
    game_analyses = []
    
    for idx, row in merged_df.iterrows():
        if idx % 100 == 0:
            print(f"å¤„ç†è¿›åº¦: {idx}/{len(merged_df)}")
        
        guesses = extract_guesses_from_all_guesses(row['all_guesses'])
        
        if guesses:
            similarities = analyzer.analyze_game_similarities({
                'target_word': row['target_word'],
                'all_guesses': guesses
            })
            for sim in similarities:
                # ä¿®æ­£å­—æ®µåï¼šä½¿ç”¨å®é™…çš„CSVå­—æ®µå
                hinter_model = row.get('hinter_model', 'Unknown')
                guesser_model = row.get('guesser_model', 'Unknown')
                
                # æ¸…ç†æ¨¡å‹åç§°ï¼ˆå»æ‰openai/å‰ç¼€ç­‰ï¼‰
                if isinstance(hinter_model, str) and '/' in hinter_model:
                    hinter_model = hinter_model.split('/')[-1]
                if isinstance(guesser_model, str) and '/' in guesser_model:
                    guesser_model = guesser_model.split('/')[-1]
                
                sim.update({
                    'game_id': row.get('game_id', f'game_{idx}'),
                    'hinter_model': hinter_model,
                    'guesser_model': guesser_model,
                    'model_pair': f"{hinter_model}-{guesser_model}",
                    'success': row.get('success', False),
                    'turns_used': row.get('turns_used', 0),
                    'part_of_speech': row.get('part_of_speech', 'unknown'),
                    'category': row.get('category', 'unknown')
                })
            game_analyses.extend(similarities)
    
    if game_analyses:
        similarity_df = pd.DataFrame(game_analyses)
        print(f"âœ… ç›¸ä¼¼åº¦åˆ†æå®Œæˆ: {len(similarity_df)} æ¡è®°å½•")
        return similarity_df
    else:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„çŒœæµ‹æ•°æ®")
        return None

def analyze_multi_model_similarity_trends(similarity_df):
    """åˆ†æå¤šæ¨¡å‹å¤šè½®æ¬¡ç›¸ä¼¼åº¦è¶‹åŠ¿"""
    if similarity_df is None or len(similarity_df) == 0:
        print("âŒ æ²¡æœ‰æ•°æ®å¯ä¾›åˆ†æ")
        return None
    
    print("\nğŸ” å¤šæ¨¡å‹å¤šè½®æ¬¡ç›¸ä¼¼åº¦è¶‹åŠ¿åˆ†æ")
    
    # æŒ‰æ¨¡å‹å¯¹å’Œè½®æ•°åˆ†æ
    model_turn_similarity = similarity_df.groupby(['model_pair', 'turn_number'])['similarity'].agg(['mean', 'std', 'count']).round(4)
    print("\nå„æ¨¡å‹å¯¹å„è½®å¹³å‡ç›¸ä¼¼åº¦:")
    print(model_turn_similarity)
    
    # æŒ‰Hinteræ¨¡å‹å’Œè½®æ•°åˆ†æ
    hinter_turn_similarity = similarity_df.groupby(['hinter_model', 'turn_number'])['similarity'].agg(['mean', 'std', 'count']).round(4)
    print("\nå„Hinteræ¨¡å‹å„è½®å¹³å‡ç›¸ä¼¼åº¦:")
    print(hinter_turn_similarity)
    
    # æŒ‰Guesseræ¨¡å‹å’Œè½®æ•°åˆ†æ
    guesser_turn_similarity = similarity_df.groupby(['guesser_model', 'turn_number'])['similarity'].agg(['mean', 'std', 'count']).round(4)
    print("\nå„Guesseræ¨¡å‹å„è½®å¹³å‡ç›¸ä¼¼åº¦:")
    print(guesser_turn_similarity)
    
    return {
        'model_turn': model_turn_similarity,
        'hinter_turn': hinter_turn_similarity,
        'guesser_turn': guesser_turn_similarity
    }

def visualize_multi_model_similarity_analysis(similarity_df):
    """å¯è§†åŒ–å¤šæ¨¡å‹å¤šè½®æ¬¡ç›¸ä¼¼åº¦åˆ†æ"""
    if similarity_df is None or len(similarity_df) == 0:
        print("âŒ æ²¡æœ‰æ•°æ®å¯ä¾›å¯è§†åŒ–")
        return
    
    sns.set_style("white")
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('BERTè¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ - å¤šæ¨¡å‹å¤šè½®æ¬¡å¯¹æ¯”', fontsize=16, fontweight='bold')
    
    # 1. å„æ¨¡å‹å¯¹æŒ‰è½®æ•°çš„ç›¸ä¼¼åº¦å˜åŒ–
    model_turn_data = similarity_df.groupby(['model_pair', 'turn_number'])['similarity'].mean().unstack(level=0)
    model_turn_data.plot(kind='line', ax=axes[0, 0], marker='o', linewidth=2)
    axes[0, 0].set_title('å„æ¨¡å‹å¯¹æŒ‰è½®æ•°çš„ç›¸ä¼¼åº¦å˜åŒ–', fontweight='bold')
    axes[0, 0].set_xlabel('è½®æ•°')
    axes[0, 0].set_ylabel('å¹³å‡ç›¸ä¼¼åº¦')
    axes[0, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. Hinteræ¨¡å‹æŒ‰è½®æ•°çš„ç›¸ä¼¼åº¦å˜åŒ–
    hinter_turn_data = similarity_df.groupby(['hinter_model', 'turn_number'])['similarity'].mean().unstack(level=0)
    hinter_turn_data.plot(kind='line', ax=axes[0, 1], marker='s', linewidth=2)
    axes[0, 1].set_title('å„Hinteræ¨¡å‹æŒ‰è½®æ•°çš„ç›¸ä¼¼åº¦å˜åŒ–', fontweight='bold')
    axes[0, 1].set_xlabel('è½®æ•°')
    axes[0, 1].set_ylabel('å¹³å‡ç›¸ä¼¼åº¦')
    axes[0, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. æ¨¡å‹å¯¹ç›¸ä¼¼åº¦çƒ­åŠ›å›¾
    heatmap_data = similarity_df.groupby(['hinter_model', 'guesser_model'])['similarity'].mean().unstack()
    sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='viridis', ax=axes[0, 2])
    axes[0, 2].set_title('æ¨¡å‹å¯¹ç›¸ä¼¼åº¦çƒ­åŠ›å›¾', fontweight='bold')
    
    # 4. Hinteræ¨¡å‹æ•´ä½“è¡¨ç°å¯¹æ¯”
    hinter_means = similarity_df.groupby('hinter_model')['similarity'].mean().sort_values(ascending=False)
    bars = axes[1, 0].bar(range(len(hinter_means)), hinter_means.values)
    axes[1, 0].set_title('Hinteræ¨¡å‹å¹³å‡ç›¸ä¼¼åº¦å¯¹æ¯”', fontweight='bold')
    axes[1, 0].set_xticks(range(len(hinter_means)))
    axes[1, 0].set_xticklabels(hinter_means.index, rotation=45, ha='right')
    
    # 5. Guesseræ¨¡å‹æ•´ä½“è¡¨ç°å¯¹æ¯”
    guesser_means = similarity_df.groupby('guesser_model')['similarity'].mean().sort_values(ascending=False)
    bars = axes[1, 1].bar(range(len(guesser_means)), guesser_means.values)
    axes[1, 1].set_title('Guesseræ¨¡å‹å¹³å‡ç›¸ä¼¼åº¦å¯¹æ¯”', fontweight='bold')
    axes[1, 1].set_xticks(range(len(guesser_means)))
    axes[1, 1].set_xticklabels(guesser_means.index, rotation=45, ha='right')
    
    # 6. å„è½®æ¬¡æ•´ä½“ç›¸ä¼¼åº¦åˆ†å¸ƒ
    turn_data = [similarity_df[similarity_df['turn_number'] == turn]['similarity'].values 
                 for turn in sorted(similarity_df['turn_number'].unique())]
    turn_labels = [f'ç¬¬{turn}è½®' for turn in sorted(similarity_df['turn_number'].unique())]
    
    axes[1, 2].boxplot(turn_data, labels=turn_labels)
    axes[1, 2].set_title('å„è½®æ¬¡ç›¸ä¼¼åº¦åˆ†å¸ƒ', fontweight='bold')
    axes[1, 2].set_xlabel('è½®æ¬¡')
    axes[1, 2].set_ylabel('ç›¸ä¼¼åº¦')
    axes[1, 2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

def generate_multi_model_similarity_report(similarity_df):
    """ç”Ÿæˆå¤šæ¨¡å‹å¤šè½®æ¬¡ç›¸ä¼¼åº¦åˆ†ææŠ¥å‘Š"""
    if similarity_df is None or len(similarity_df) == 0:
        print("âŒ æ²¡æœ‰æ•°æ®å¯ä¾›åˆ†æ")
        return
    
    print("\n" + "="*80)
    print("          BERTè¯­ä¹‰ç›¸ä¼¼åº¦åˆ†ææŠ¥å‘Š - å¤šæ¨¡å‹å¤šè½®æ¬¡ç‰ˆæœ¬")
    print("="*80)
    
    # åŸºç¡€ç»Ÿè®¡
    total_guesses = len(similarity_df)
    avg_similarity = similarity_df['similarity'].mean()
    
    print(f"\nğŸ“Š åŸºç¡€ç»Ÿè®¡:")
    print(f"  â€¢ æ€»çŒœæµ‹æ•°: {total_guesses:,}")
    print(f"  â€¢ å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.4f}")
    
    # æ¨¡å‹å¯¹åˆ†æ
    print(f"\nğŸ¤– æ¨¡å‹å¯¹åˆ†æ:")
    for model_pair in similarity_df['model_pair'].unique():
        data = similarity_df[similarity_df['model_pair'] == model_pair]
        avg_sim = data['similarity'].mean()
        count = len(data)
        success_rate = data['success'].mean()
        print(f"  â€¢ {model_pair}: ç›¸ä¼¼åº¦ {avg_sim:.4f}, æˆåŠŸç‡ {success_rate:.4f} ({count} æ¬¡)")
    
    # è½®æ•°åˆ†æ
    print(f"\nğŸ“ˆ è½®æ•°åˆ†æ:")
    for turn in sorted(similarity_df['turn_number'].unique()):
        turn_data = similarity_df[similarity_df['turn_number'] == turn]
        avg_sim = turn_data['similarity'].mean()
        count = len(turn_data)
        print(f"  â€¢ ç¬¬{turn}è½®: å¹³å‡ç›¸ä¼¼åº¦ {avg_sim:.4f} ({count} æ¬¡çŒœæµ‹)")
    
    print("\n" + "="*80)

def save_multi_model_similarity_results(similarity_df, analysis_results):
    """ä¿å­˜å¤šæ¨¡å‹ç›¸ä¼¼åº¦åˆ†æç»“æœ"""
    if similarity_df is not None:
        similarity_df.to_csv('bert_multi_model_similarity_analysis.csv', index=False, encoding='utf-8')
        print(f"âœ… ç›¸ä¼¼åº¦åˆ†æç»“æœå·²ä¿å­˜")
        
        if analysis_results:
            with pd.ExcelWriter('bert_multi_model_stats.xlsx') as writer:
                analysis_results['model_turn'].to_excel(writer, sheet_name='æ¨¡å‹å¯¹è½®æ•°ç»Ÿè®¡')
                analysis_results['hinter_turn'].to_excel(writer, sheet_name='Hinterè½®æ•°ç»Ÿè®¡')
                analysis_results['guesser_turn'].to_excel(writer, sheet_name='Guesserè½®æ•°ç»Ÿè®¡')
            print(f"âœ… ç»Ÿè®¡ç»“æœå·²ä¿å­˜åˆ°Excelæ–‡ä»¶")

# ä¸»å‡½æ•°
if __name__ == "__main__":
    # è¿è¡ŒBERTç›¸ä¼¼åº¦åˆ†æ
    similarity_df = run_bert_similarity_analysis()
    
    if similarity_df is not None:
        # åˆ†æè¶‹åŠ¿
        analysis_results = analyze_multi_model_similarity_trends(similarity_df)
        
        # å¯è§†åŒ–åˆ†æ
        visualize_multi_model_similarity_analysis(similarity_df)
        
        # ç”ŸæˆæŠ¥å‘Š
        generate_multi_model_similarity_report(similarity_df)
        
        # ä¿å­˜ç»“æœ
        save_multi_model_similarity_results(similarity_df, analysis_results)
        
        print("ğŸ‰ BERTå¤šæ¨¡å‹å¤šè½®æ¬¡è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æå®Œæˆï¼")
    else:
        print("âŒ åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶")
