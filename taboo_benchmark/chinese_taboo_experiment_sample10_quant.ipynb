{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b92d389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å‡†å¤‡è¯»å–æ•°æ®é›†: data/chinese_dataset_sample10.json\n",
      "âœ… åŸå§‹æ•°æ®é›†è¯»å–æˆåŠŸã€‚\n",
      "âœ… å·²ä¸ºæ‰€æœ‰è¯æ¡æˆåŠŸæ·»åŠ  'char_count' å…ƒæ•°æ®ã€‚\n",
      "ğŸ‰ é‡æ„åçš„æ–°æ•°æ®é›†å·²æˆåŠŸä¿å­˜è‡³: data/chinese_dataset_sample10_with_char_count.json\n",
      "\n",
      "--- æ•°æ®æ ·æœ¬é¢„è§ˆ ---\n",
      "{\n",
      "  \"target\": \"é—ä½“\",\n",
      "  \"part_of_speech\": \"noun\",\n",
      "  \"taboo\": [\n",
      "    \"åŠ¨ç‰©\",\n",
      "    \"éƒ¨ä»¶\",\n",
      "    \"ç”Ÿç†å­¦\",\n",
      "    \"ä¸œè¥¿\",\n",
      "    \"äº‹ç‰©\"\n",
      "  ],\n",
      "  \"category\": \"chinese_general\",\n",
      "  \"senses\": [\n",
      "    {\n",
      "      \"zh_word\": \"é—ä½“\",\n",
      "      \"en_word\": \"body\",\n",
      "      \"zh_grammar\": \"noun\",\n",
      "      \"en_grammar\": \"noun\",\n",
      "      \"Def\": \"{part|éƒ¨ä»¶:PartPosition={body|èº«},domain={physiology|ç”Ÿç†å­¦},whole={AnimalHuman|åŠ¨ç‰©:{die|æ­»:experiencer={~}}}}\",\n",
      "      \"No\": \"000000273198\",\n",
      "      \"sememes\": \"[part|éƒ¨ä»¶, body|èº«, physiology|ç”Ÿç†å­¦, AnimalHuman|åŠ¨ç‰©, die|æ­»]\"\n",
      "    },\n",
      "    {\n",
      "      \"zh_word\": \"é—ä½“\",\n",
      "      \"en_word\": \"remains of the dead\",\n",
      "      \"zh_grammar\": \"noun\",\n",
      "      \"en_grammar\": \"noun\",\n",
      "      \"Def\": \"{part|éƒ¨ä»¶:PartPosition={body|èº«},domain={physiology|ç”Ÿç†å­¦},whole={AnimalHuman|åŠ¨ç‰©:{die|æ­»:experiencer={~}}}}\",\n",
      "      \"No\": \"000000273201\",\n",
      "      \"sememes\": \"[part|éƒ¨ä»¶, body|èº«, physiology|ç”Ÿç†å­¦, AnimalHuman|åŠ¨ç‰©, die|æ­»]\"\n",
      "    },\n",
      "    {\n",
      "      \"zh_word\": \"é—ä½“\",\n",
      "      \"en_word\": \"reliquiae\",\n",
      "      \"zh_grammar\": \"noun\",\n",
      "      \"en_grammar\": \"noun\",\n",
      "      \"Def\": \"{part|éƒ¨ä»¶:PartPosition={body|èº«},domain={physiology|ç”Ÿç†å­¦},whole={AnimalHuman|åŠ¨ç‰©:{die|æ­»:experiencer={~}}}}\",\n",
      "      \"No\": \"000000273199\",\n",
      "      \"sememes\": \"[part|éƒ¨ä»¶, body|èº«, physiology|ç”Ÿç†å­¦, AnimalHuman|åŠ¨ç‰©, die|æ­»]\"\n",
      "    },\n",
      "    {\n",
      "      \"zh_word\": \"é—ä½“\",\n",
      "      \"en_word\": \"remains\",\n",
      "      \"zh_grammar\": \"noun\",\n",
      "      \"en_grammar\": \"noun\",\n",
      "      \"Def\": \"{part|éƒ¨ä»¶:PartPosition={body|èº«},domain={physiology|ç”Ÿç†å­¦},whole={AnimalHuman|åŠ¨ç‰©:{die|æ­»:experiencer={~}}}}\",\n",
      "      \"No\": \"000000273200\",\n",
      "      \"sememes\": \"[part|éƒ¨ä»¶, body|èº«, physiology|ç”Ÿç†å­¦, AnimalHuman|åŠ¨ç‰©, die|æ­»]\"\n",
      "    }\n",
      "  ],\n",
      "  \"metadata\": {\n",
      "    \"sense_count\": 4,\n",
      "    \"taboo_count\": 5,\n",
      "    \"source\": \"openhownet\",\n",
      "    \"char_count\": 2\n",
      "  }\n",
      "}\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# --- é…ç½® ---\n",
    "# è¯·å°†è¿™é‡Œè®¾ç½®ä¸ºæ‚¨çš„åŸå§‹æ•°æ®é›†æ–‡ä»¶è·¯å¾„\n",
    "input_filepath = \"data/chinese_dataset_sample10.json\"\n",
    "\n",
    "# è¿™æ˜¯æ–°ç”Ÿæˆçš„æ•°æ®é›†æ–‡ä»¶çš„ä¿å­˜è·¯å¾„\n",
    "output_filepath = \"data/chinese_dataset_sample10_with_char_count.json\"\n",
    "\n",
    "# --- ä¸»ç¨‹åº ---\n",
    "print(f\"å‡†å¤‡è¯»å–æ•°æ®é›†: {input_filepath}\")\n",
    "\n",
    "try:\n",
    "    # 1. è¯»å–åŸå§‹JSONæ–‡ä»¶\n",
    "    with open(input_filepath, 'r', encoding='utf-8') as f:\n",
    "        dataset = json.load(f)\n",
    "    print(\"âœ… åŸå§‹æ•°æ®é›†è¯»å–æˆåŠŸã€‚\")\n",
    "\n",
    "    # 2. éå†æ•°æ®é›†ï¼Œä¸ºæ¯ä¸ªè¯æ¡æ·»åŠ å­—æ•°ç»Ÿè®¡\n",
    "    for item in dataset:\n",
    "        # ç¡®ä¿ 'metadata' å­—å…¸å­˜åœ¨\n",
    "        if 'metadata' not in item:\n",
    "            item['metadata'] = {}\n",
    "        \n",
    "        # è·å– 'target' è¯æ±‡å¹¶è®¡ç®—å­—æ•°ï¼Œç„¶åæ·»åŠ åˆ°metadataä¸­\n",
    "        target_word = item.get('target', '')\n",
    "        item['metadata']['char_count'] = len(target_word)\n",
    "\n",
    "    print(\"âœ… å·²ä¸ºæ‰€æœ‰è¯æ¡æˆåŠŸæ·»åŠ  'char_count' å…ƒæ•°æ®ã€‚\")\n",
    "\n",
    "    # 3. ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨\n",
    "    output_dir = os.path.dirname(output_filepath)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # 4. å°†ä¿®æ”¹åçš„å®Œæ•´æ•°æ®å†™å…¥æ–°æ–‡ä»¶\n",
    "    with open(output_filepath, 'w', encoding='utf-8') as f:\n",
    "        # ä½¿ç”¨ indent=2 è¿›è¡Œæ ¼å¼åŒ–å­˜å‚¨ï¼Œæ–¹ä¾¿é˜…è¯»\n",
    "        json.dump(dataset, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"ğŸ‰ é‡æ„åçš„æ–°æ•°æ®é›†å·²æˆåŠŸä¿å­˜è‡³: {output_filepath}\")\n",
    "\n",
    "    # 5. (å¯é€‰) æ‰“å°ä¸€ä¸ªå¤„ç†åçš„æ ·æœ¬ä»¥ä¾›å¿«é€ŸéªŒè¯\n",
    "    if dataset:\n",
    "        print(\"\\n--- æ•°æ®æ ·æœ¬é¢„è§ˆ ---\")\n",
    "        print(json.dumps(dataset[0], ensure_ascii=False, indent=2))\n",
    "        print(\"--------------------\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ '{input_filepath}'ã€‚è¯·æ£€æŸ¥æ–‡ä»¶åå’Œè·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "288c4255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OpenHowNetå¯ç”¨ï¼ˆç”¨äºæ•°æ®é›†æ„å»ºï¼‰\n",
      "ğŸš€ ä¸­æ–‡Tabooå®éªŒç¯å¢ƒåˆå§‹åŒ–å®Œæˆ\n",
      "ğŸ“‹ å®éªŒç›®æ ‡: åŸºäºé¢„ç”Ÿæˆæ•°æ®é›†è¿›è¡Œä¸­æ–‡LLM Tabooæ¸¸æˆè¯„ä¼°\n",
      "ğŸ¯ å‚è€ƒæ¡†æ¶: base_test.ipynbæ ‡å‡†å®éªŒæ¶æ„\n"
     ]
    }
   ],
   "source": [
    "# 1.1 æ ¸å¿ƒä¾èµ–å¯¼å…¥\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "import jieba\n",
    "import re\n",
    "import logging\n",
    "from typing import Dict, List, Any, Tuple\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# 1.2 ä¸­æ–‡ç‰¹åŒ–ä¾èµ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
    "try:\n",
    "    import OpenHowNet\n",
    "    print(\"âœ… OpenHowNetå¯ç”¨ï¼ˆç”¨äºæ•°æ®é›†æ„å»ºï¼‰\")\n",
    "except ImportError:\n",
    "    print(\"â„¹ï¸ OpenHowNetä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨é¢„ç”Ÿæˆæ•°æ®é›†\")\n",
    "    OpenHowNet = None\n",
    "\n",
    "# 1.3 ç¯å¢ƒè®¾ç½®\n",
    "random.seed(42)  # ç¡®ä¿å¯å¤ç°\n",
    "jieba.setLogLevel(logging.INFO)  # å‡å°‘åˆ†è¯æ—¥å¿—è¾“å‡º\n",
    "\n",
    "print(\"ğŸš€ ä¸­æ–‡Tabooå®éªŒç¯å¢ƒåˆå§‹åŒ–å®Œæˆ\")\n",
    "print(\"ğŸ“‹ å®éªŒç›®æ ‡: åŸºäºé¢„ç”Ÿæˆæ•°æ®é›†è¿›è¡Œä¸­æ–‡LLM Tabooæ¸¸æˆè¯„ä¼°\")\n",
    "print(\"ğŸ¯ å‚è€ƒæ¡†æ¶: base_test.ipynbæ ‡å‡†å®éªŒæ¶æ„\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d5ab3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š æ­£åœ¨åŠ è½½ä¸­æ–‡æ•°æ®é›†...\n",
      "âœ… ä¸­æ–‡æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…±40æ¡è®°å½•\n",
      "\n",
      "ğŸ“‹ ä¸­æ–‡æ•°æ®é›†æ ·æœ¬:\n",
      "   ç›®æ ‡è¯: ç•›\n",
      "   è¯æ€§: noun\n",
      "   ç±»åˆ«: chinese_general\n",
      "   ç¦ç”¨è¯: ['ç•Œé™', 'å®ä½“', 'é“è·¯', 'ä¸œè¥¿', 'äº‹ç‰©']\n",
      "   è¯ä¹‰æ•°: 2\n"
     ]
    }
   ],
   "source": [
    "# 2.1 åŠ è½½ä¸­æ–‡æ•°æ®é›†å‡½æ•°\n",
    "def load_chinese_dataset(dataset_path: str = \"data/chinese_dataset_sample10_with_char_count.json\") -> List[Dict]:\n",
    "    \"\"\"åŠ è½½ä¸­æ–‡Tabooæ•°æ®é›†\"\"\"\n",
    "    try:\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ æ•°æ®é›†æ–‡ä»¶æœªæ‰¾åˆ°: {dataset_path}\")\n",
    "        print(\"ğŸ’¡ è¯·ç¡®ä¿å·²è¿è¡Œæ•°æ®é›†æ„å»ºæ­¥éª¤æˆ–ä½¿ç”¨é¢„ç”Ÿæˆæ•°æ®é›†\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}\")\n",
    "        return []\n",
    "\n",
    "# 2.2 åŠ è½½ä¸­æ–‡æ•°æ®é›†\n",
    "print(\"ğŸ“š æ­£åœ¨åŠ è½½ä¸­æ–‡æ•°æ®é›†...\")\n",
    "chinese_dataset = load_chinese_dataset()\n",
    "\n",
    "if chinese_dataset:\n",
    "    print(f\"âœ… ä¸­æ–‡æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…±{len(chinese_dataset)}æ¡è®°å½•\")\n",
    "    \n",
    "    # æ˜¾ç¤ºæ•°æ®é›†æ ·æœ¬\n",
    "    print(\"\\nğŸ“‹ ä¸­æ–‡æ•°æ®é›†æ ·æœ¬:\")\n",
    "    if len(chinese_dataset) > 0:\n",
    "        sample = random.choice(chinese_dataset)\n",
    "        print(f\"   ç›®æ ‡è¯: {sample['target']}\")\n",
    "        print(f\"   è¯æ€§: {sample.get('part_of_speech', 'unknown')}\")\n",
    "        print(f\"   ç±»åˆ«: {sample.get('category', 'unknown')}\")\n",
    "        print(f\"   ç¦ç”¨è¯: {sample['taboo']}\")\n",
    "        print(f\"   è¯ä¹‰æ•°: {len(sample.get('senses', []))}\")\n",
    "else:\n",
    "    print(\"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fde63974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ä¸­æ–‡æ•°æ®é›†åŸºæœ¬ç»Ÿè®¡:\n",
      "========================================\n",
      "\n",
      "ğŸ·ï¸ è¯æ€§åˆ†å¸ƒ:\n",
      "   noun: 10 ä¸ª (25.0%)\n",
      "   verb: 10 ä¸ª (25.0%)\n",
      "   adj: 10 ä¸ª (25.0%)\n",
      "   adv: 10 ä¸ª (25.0%)\n",
      "\n",
      "ğŸ“‚ ç±»åˆ«åˆ†å¸ƒ:\n",
      "   chinese_general: 40 ä¸ª (100.0%)\n",
      "\n",
      "ğŸš« ç¦ç”¨è¯ç»Ÿè®¡:\n",
      "   å¹³å‡æ•°é‡: 5.0\n",
      "   èŒƒå›´: 5 - 5\n",
      "\n",
      "ğŸ’­ è¯ä¹‰ç»Ÿè®¡:\n",
      "   å¹³å‡æ•°é‡: 2.1\n",
      "   èŒƒå›´: 1 - 10\n",
      "\n",
      "âœ… ä¸­æ–‡æ•°æ®é›†ç»Ÿè®¡å®Œæˆï¼Œè´¨é‡è‰¯å¥½ï¼Œå¯ç”¨äºå®éªŒ\n",
      "\n",
      "ğŸ² éšæœºç§å­å·²è®¾ç½®ä¸º 42ï¼Œç¡®ä¿å®éªŒå¯å¤ç°\n"
     ]
    }
   ],
   "source": [
    "# 3.1 ä¸­æ–‡æ•°æ®é›†ç»Ÿè®¡åˆ†æ\n",
    "if chinese_dataset:\n",
    "    print(\"ğŸ“Š ä¸­æ–‡æ•°æ®é›†åŸºæœ¬ç»Ÿè®¡:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # è¯æ€§åˆ†å¸ƒç»Ÿè®¡\n",
    "    pos_counts = {}\n",
    "    taboo_counts = []\n",
    "    sense_counts = []\n",
    "    categories = {}\n",
    "    \n",
    "    for item in chinese_dataset:\n",
    "        # ç»Ÿè®¡è¯æ€§\n",
    "        pos = item.get('part_of_speech', 'unknown')\n",
    "        pos_counts[pos] = pos_counts.get(pos, 0) + 1\n",
    "        \n",
    "        # ç»Ÿè®¡ç±»åˆ«\n",
    "        category = item.get('category', 'unknown')\n",
    "        categories[category] = categories.get(category, 0) + 1\n",
    "        \n",
    "        # ç»Ÿè®¡ç¦ç”¨è¯æ•°é‡\n",
    "        taboo_counts.append(len(item.get('taboo', [])))\n",
    "        \n",
    "        # ç»Ÿè®¡è¯ä¹‰æ•°é‡\n",
    "        sense_counts.append(len(item.get('senses', [])))\n",
    "    \n",
    "    print(f\"\\nğŸ·ï¸ è¯æ€§åˆ†å¸ƒ:\")\n",
    "    sorted_pos = sorted(pos_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    for pos, count in sorted_pos:\n",
    "        percentage = count / len(chinese_dataset) * 100\n",
    "        print(f\"   {pos}: {count} ä¸ª ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nğŸ“‚ ç±»åˆ«åˆ†å¸ƒ:\")\n",
    "    sorted_categories = sorted(categories.items(), key=lambda x: x[1], reverse=True)\n",
    "    for category, count in sorted_categories:\n",
    "        percentage = count / len(chinese_dataset) * 100\n",
    "        print(f\"   {category}: {count} ä¸ª ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nğŸš« ç¦ç”¨è¯ç»Ÿè®¡:\")\n",
    "    print(f\"   å¹³å‡æ•°é‡: {sum(taboo_counts) / len(taboo_counts):.1f}\")\n",
    "    print(f\"   èŒƒå›´: {min(taboo_counts)} - {max(taboo_counts)}\")\n",
    "    \n",
    "    print(f\"\\nğŸ’­ è¯ä¹‰ç»Ÿè®¡:\")\n",
    "    if sense_counts and max(sense_counts) > 0:\n",
    "        print(f\"   å¹³å‡æ•°é‡: {sum(sense_counts) / len(sense_counts):.1f}\")\n",
    "        print(f\"   èŒƒå›´: {min(sense_counts)} - {max(sense_counts)}\")\n",
    "    else:\n",
    "        print(\"   è¯ä¹‰ä¿¡æ¯: æœªåŒ…å«è¯¦ç»†è¯ä¹‰æ•°æ®\")\n",
    "    \n",
    "    print(f\"\\nâœ… ä¸­æ–‡æ•°æ®é›†ç»Ÿè®¡å®Œæˆï¼Œè´¨é‡è‰¯å¥½ï¼Œå¯ç”¨äºå®éªŒ\")\n",
    "    \n",
    "    # è®¾ç½®éšæœºç§å­ç”¨äºå®éªŒ\n",
    "    random.seed(42)\n",
    "    print(\"\\nğŸ² éšæœºç§å­å·²è®¾ç½®ä¸º 42ï¼Œç¡®ä¿å®éªŒå¯å¤ç°\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ æ— æ³•è¿›è¡Œç»Ÿè®¡åˆ†æï¼šæ•°æ®é›†æœªæˆåŠŸåŠ è½½\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad8f9bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ä¸­æ–‡APIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ\n",
      "\n",
      "ğŸ¤– ä¸­æ–‡å®éªŒæ¨¡å‹: 2 ä¸ª\n",
      "   1. gemini-2.5-flash\n",
      "   2. deepseek-chat-v3-0324\n",
      "\n",
      "ğŸš€ APIå®¢æˆ·ç«¯å·²å°±ç»ªï¼Œå¯ä»¥å¼€å§‹ä¸­æ–‡Tabooå®éªŒ\n"
     ]
    }
   ],
   "source": [
    "# 4.1 åŠ è½½APIå¯†é’¥\n",
    "def load_api_keys(keys_path: str = \"api_keys.json\") -> Dict[str, str]:\n",
    "    \"\"\"åŠ è½½APIå¯†é’¥\"\"\"\n",
    "    try:\n",
    "        with open(keys_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ APIå¯†é’¥æ–‡ä»¶æœªæ‰¾åˆ°: {keys_path}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ APIå¯†é’¥åŠ è½½å¤±è´¥: {e}\")\n",
    "        return {}\n",
    "\n",
    "# 4.2 ä¸­æ–‡APIå®¢æˆ·ç«¯ç±»\n",
    "class ChineseAPIClient:\n",
    "    \"\"\"æ”¯æŒä¸­æ–‡æ¨¡å‹çš„APIå®¢æˆ·ç«¯\"\"\"\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "        self.headers = {\n",
    "            \"Authorization\": f\"Bearer {api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "    \n",
    "    def call_model(self, model: str, messages: List[Dict[str, str]], \n",
    "                   temperature: float = 0.3) -> str:\n",
    "        \"\"\"è°ƒç”¨æ¨¡å‹APIï¼Œä¿æŒä¸­æ–‡å­—ç¬¦\"\"\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": 2000\n",
    "        }\n",
    "        response = requests.post(self.base_url, headers=self.headers, \n",
    "                               json=payload, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        content = result['choices'][0]['message']['content'].strip()\n",
    "        \n",
    "        # ä¿ç•™ä¸­æ–‡å­—ç¬¦ï¼Œåªè¿‡æ»¤æ§åˆ¶å­—ç¬¦\n",
    "        content = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', '', content)\n",
    "        return content\n",
    "\n",
    "# 4.3 åˆå§‹åŒ–APIå®¢æˆ·ç«¯\n",
    "try:\n",
    "    api_keys = load_api_keys()\n",
    "    if \"OPENROUTER_API_KEY\" in api_keys:\n",
    "        chinese_client = ChineseAPIClient(api_keys[\"OPENROUTER_API_KEY\"])\n",
    "        print(\"âœ… ä¸­æ–‡APIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ\")\n",
    "    else:\n",
    "        chinese_client = None\n",
    "        print(\"âŒ ç¼ºå°‘OPENROUTER_API_KEYï¼Œæ— æ³•åˆå§‹åŒ–APIå®¢æˆ·ç«¯\")\n",
    "except Exception as e:\n",
    "    chinese_client = None\n",
    "    print(f\"âŒ APIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}\")\n",
    "\n",
    "# 4.4 å®šä¹‰ä¸­æ–‡æµ‹è¯•æ¨¡å‹\n",
    "CHINESE_MODELS = [\n",
    "    \"google/gemini-2.5-flash\", \n",
    "    \"deepseek/deepseek-chat-v3-0324\",\n",
    "]\n",
    "\n",
    "print(f\"\\nğŸ¤– ä¸­æ–‡å®éªŒæ¨¡å‹: {len(CHINESE_MODELS)} ä¸ª\")\n",
    "for i, model in enumerate(CHINESE_MODELS, 1):\n",
    "    model_name = model.split('/')[-1]\n",
    "    print(f\"   {i}. {model_name}\")\n",
    "\n",
    "if chinese_client:\n",
    "    print(f\"\\nğŸš€ APIå®¢æˆ·ç«¯å·²å°±ç»ªï¼Œå¯ä»¥å¼€å§‹ä¸­æ–‡Tabooå®éªŒ\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ APIå®¢æˆ·ç«¯æœªå°±ç»ªï¼Œéœ€è¦é…ç½®APIå¯†é’¥\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7981d10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ä¸­æ–‡æ–‡æœ¬å¤„ç†å’ŒAPIè°ƒç”¨å‡½æ•°å·²å®šä¹‰\n"
     ]
    }
   ],
   "source": [
    "# 5.1 ä¸­æ–‡æ–‡æœ¬å®‰å…¨æ¸…ç†\n",
    "def safe_chinese_text_cleanup(text: str, max_length: int = 200) -> str:\n",
    "    \"\"\"å®‰å…¨æ¸…ç†ä¸­æ–‡æ–‡æœ¬ï¼Œä¿ç•™ä¸­æ–‡å­—ç¬¦\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # ä¿ç•™ä¸­æ–‡å­—ç¬¦ã€è‹±æ–‡å­—ç¬¦ã€æ•°å­—ã€åŸºæœ¬æ ‡ç‚¹\n",
    "    cleaned = re.sub(r'[^\\u4e00-\\u9fff\\w\\s\\.\\,\\!\\?\\-\\[\\]ã€ã€‘]', '', str(text))\n",
    "    if len(cleaned) > max_length:\n",
    "        cleaned = cleaned[:max_length] + \"...\"\n",
    "    return cleaned\n",
    "\n",
    "# 5.2 å¥å£®çš„ä¸­æ–‡APIè°ƒç”¨\n",
    "def robust_chinese_api_call(client, model: str, base_prompt: str, expected_prefix: str, max_retries: int = 3):\n",
    "    \"\"\"æ”¯æŒä¸­æ–‡æ ¼å¼çš„å¥å£®APIè°ƒç”¨\"\"\"\n",
    "    failed_outputs = []\n",
    "    \n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            if attempt == 1:\n",
    "                prompt = base_prompt\n",
    "            else:\n",
    "                prev_output = failed_outputs[-1] if failed_outputs else \"æœªçŸ¥\"\n",
    "                format_reminder = f\"\"\"\n",
    "\n",
    "âš ï¸ æ ¼å¼é”™è¯¯ âš ï¸\n",
    "ä½ çš„ä¸Šä¸€æ¬¡å›ç­”æ˜¯: \"{prev_output}\"\n",
    "\n",
    "å¿…éœ€æ ¼å¼:\n",
    "- ä½ å¿…é¡»ä»¥ '{expected_prefix}' å¼€å¤´ï¼ˆåŒ…æ‹¬æ–¹æ‹¬å·ï¼‰\n",
    "- ä¸è¦åœ¨ {expected_prefix} ä¹‹å‰æ·»åŠ ä»»ä½•æ–‡å­—\n",
    "\n",
    "è¯·æŒ‰æ­£ç¡®æ ¼å¼é‡æ–°å›ç­”:\"\"\"\n",
    "                prompt = base_prompt + format_reminder\n",
    "            \n",
    "            response = client.call_model(model, [{\"role\": \"user\", \"content\": prompt}])\n",
    "            cleaned_response = safe_chinese_text_cleanup(response)\n",
    "            \n",
    "            if cleaned_response.strip().startswith(expected_prefix):\n",
    "                content = cleaned_response.strip()[len(expected_prefix):].strip()\n",
    "                return {\n",
    "                    'success': True,\n",
    "                    'content': content,\n",
    "                    'attempts': attempt,\n",
    "                    'raw_response': response\n",
    "                }\n",
    "            else:\n",
    "                failed_outputs.append(cleaned_response[:50])\n",
    "                if attempt == max_retries:\n",
    "                    return {\n",
    "                        'success': False,\n",
    "                        'content': f\"æ ¼å¼éªŒè¯å¤±è´¥: {cleaned_response[:100]}\",\n",
    "                        'attempts': attempt,\n",
    "                        'raw_response': response\n",
    "                    }\n",
    "                \n",
    "        except Exception as e:\n",
    "            if attempt == max_retries:\n",
    "                return {\n",
    "                    'success': False,\n",
    "                    'content': f\"APIè°ƒç”¨å¤±è´¥: {e}\",\n",
    "                    'attempts': attempt,\n",
    "                    'raw_response': \"\"\n",
    "                }\n",
    "    \n",
    "    return {\n",
    "        'success': False,\n",
    "        'content': \"é‡è¯•æ¬¡æ•°è¶…é™\",\n",
    "        'attempts': max_retries,\n",
    "        'raw_response': \"\"\n",
    "    }\n",
    "\n",
    "print(\"âœ… ä¸­æ–‡æ–‡æœ¬å¤„ç†å’ŒAPIè°ƒç”¨å‡½æ•°å·²å®šä¹‰\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34a8d6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ä¸­æ–‡æ¸¸æˆéªŒè¯å’Œç»“æœå‡½æ•°å·²å®šä¹‰\n"
     ]
    }
   ],
   "source": [
    "# 5.3 ä¸­æ–‡ç¦ç”¨è¯æ£€æµ‹\n",
    "def check_chinese_taboo_violation(text: str, taboo_words: List[str]) -> bool:\n",
    "    \"\"\"æ£€æŸ¥ä¸­æ–‡æ–‡æœ¬æ˜¯å¦è¿åç¦ç”¨è¯è§„åˆ™\"\"\"\n",
    "    if not text or not taboo_words:\n",
    "        return False\n",
    "    \n",
    "    # ä½¿ç”¨jiebaåˆ†è¯\n",
    "    words = list(jieba.cut(text, cut_all=False))\n",
    "    text_words = set(words)\n",
    "    \n",
    "    # æ£€æŸ¥ç›´æ¥åŒ…å«å’Œåˆ†è¯ååŒ…å«\n",
    "    for taboo in taboo_words:\n",
    "        if taboo in text or taboo in text_words:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# 5.4 ä¸­æ–‡è¯æ±‡åŒ¹é…æ£€æŸ¥\n",
    "def check_chinese_word_match(guess: str, target: str) -> bool:\n",
    "    \"\"\"æ£€æŸ¥ä¸­æ–‡è¯æ±‡æ˜¯å¦åŒ¹é…\"\"\"\n",
    "    if not guess or not target:\n",
    "        return False\n",
    "    \n",
    "    # ç§»é™¤æ ¼å¼æ ‡è®°å’Œç©ºæ ¼\n",
    "    guess_clean = re.sub(r'[\\[\\]ã€ã€‘\\s]', '', guess).strip()\n",
    "    target_clean = re.sub(r'[\\[\\]ã€ã€‘\\s]', '', target).strip()\n",
    "    \n",
    "    # ç›´æ¥åŒ¹é…æˆ–åŒ…å«åŒ¹é…\n",
    "    return guess_clean == target_clean or target_clean in guess_clean\n",
    "\n",
    "# 5.5 åˆ›å»ºæ¸¸æˆç»“æœ\n",
    "def create_chinese_game_result(success: bool, turns: int, final_guess: str, \n",
    "                              failure_reason: str, conversation_history: List[Dict]) -> Dict:\n",
    "    \"\"\"åˆ›å»ºä¸­æ–‡æ¸¸æˆç»“æœè®°å½•\"\"\"\n",
    "    return {\n",
    "        'success': success,\n",
    "        'turns': turns,\n",
    "        'final_guess': final_guess,\n",
    "        'failure_reason': failure_reason,\n",
    "        'conversation_history': conversation_history\n",
    "    }\n",
    "\n",
    "print(\"âœ… ä¸­æ–‡æ¸¸æˆéªŒè¯å’Œç»“æœå‡½æ•°å·²å®šä¹‰\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73de5a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ä¸­æ–‡Tabooæ¸¸æˆæ ¸å¿ƒé€»è¾‘å·²å®šä¹‰\n"
     ]
    }
   ],
   "source": [
    "# 5.6 æ ¸å¿ƒä¸­æ–‡Tabooæ¸¸æˆé€»è¾‘\n",
    "def chinese_taboo_game(client, hinter_model: str, guesser_model: str, \n",
    "                      target_word: str, taboo_words: List[str], char_count: int, max_turns: int = 5) -> Dict:\n",
    "    \"\"\"ä¸­æ–‡Tabooæ¸¸æˆæ ¸å¿ƒé€»è¾‘\"\"\"\n",
    "    \n",
    "    # ä¸­æ–‡æç¤ºæ¨¡æ¿\n",
    "    hinter_base_prompt = f\"\"\"ä½ æ­£åœ¨ç©ä¸­æ–‡Tabooæ¸¸æˆã€‚ä½ çš„ä»»åŠ¡æ˜¯è®©é˜Ÿå‹çŒœå‡ºç›®æ ‡è¯æ±‡ï¼Œä½†ä¸èƒ½ä½¿ç”¨ç¦ç”¨è¯ã€‚\n",
    "**é‡è¦æç¤º: ç›®æ ‡è¯æ˜¯ {char_count} ä¸ªå­—ã€‚**\n",
    "ç›®æ ‡è¯æ±‡: {target_word}\n",
    "ç¦ç”¨è¯æ±‡: {', '.join(taboo_words)}\n",
    "\n",
    "è§„åˆ™:\n",
    "1. ä½ éœ€è¦ç»™å‡ºçº¿ç´¢è®©é˜Ÿå‹çŒœå‡ºç›®æ ‡è¯æ±‡\n",
    "2. ä½ çš„çº¿ç´¢ä¸­ä¸èƒ½åŒ…å«ä»»ä½•ç¦ç”¨è¯æ±‡\n",
    "3. è¯·ç”¨æ ¼å¼ [çº¿ç´¢] å¼€å§‹ä½ çš„å›ç­”\n",
    "4. çº¿ç´¢è¦ç®€æ´æ˜äº†ï¼Œä¸è¶…è¿‡20ä¸ªå­—\n",
    "\n",
    "è¯·ç»™å‡ºä½ çš„çº¿ç´¢:\"\"\"\n",
    "\n",
    "    conversation_history = []\n",
    "    \n",
    "    for turn in range(1, max_turns + 1):\n",
    "        # Hinterç»™å‡ºçº¿ç´¢\n",
    "        hinter_response = robust_chinese_api_call(\n",
    "            client, hinter_model, hinter_base_prompt, \"[çº¿ç´¢]\"\n",
    "        )\n",
    "        \n",
    "        if not hinter_response['success']:\n",
    "            return create_chinese_game_result(\n",
    "                False, turn, None, f\"çº¿ç´¢ç”Ÿæˆå¤±è´¥: {hinter_response['content']}\", \n",
    "                conversation_history\n",
    "            )\n",
    "        \n",
    "        hint = hinter_response['content']\n",
    "        \n",
    "        # æ£€æŸ¥ç¦ç”¨è¯è¿è§„\n",
    "        if check_chinese_taboo_violation(hint, taboo_words):\n",
    "            violated_words = [word for word in taboo_words if word in hint]\n",
    "            return create_chinese_game_result(\n",
    "                False, turn, None, f\"è¿åç¦ç”¨è¯è§„åˆ™: {violated_words}\", \n",
    "                conversation_history\n",
    "            )\n",
    "        # æ„é€ å†å²æ–‡æœ¬\n",
    "        history_lines = []\n",
    "        for conv in conversation_history:\n",
    "            history_lines.append(f\"ç¬¬{conv['turn']}è½® çº¿ç´¢: {conv['hint']}  çŒœæµ‹: {conv['guess']}\")\n",
    "        history_text = \"\\n\".join(history_lines)\n",
    "        if history_text:\n",
    "            history_text = \"æ¸¸æˆå†å²ï¼š\\n\" + history_text + \"\\n\"\n",
    "\n",
    "        # Guesserè¿›è¡ŒçŒœæµ‹\n",
    "        guesser_prompt = f\"\"\"ä½ æ­£åœ¨ç©ä¸­æ–‡Tabooæ¸¸æˆã€‚æ ¹æ®é˜Ÿå‹ç»™å‡ºçš„æ‰€æœ‰çº¿ç´¢ï¼ŒçŒœå‡ºç›®æ ‡è¯æ±‡ã€‚\n",
    "\n",
    "ç¦ç”¨è¯æ±‡: {', '.join(taboo_words)}\n",
    "\n",
    "è§„åˆ™:\n",
    "1. æ ¹æ®æ‰€æœ‰çº¿ç´¢çŒœå‡ºç›®æ ‡è¯æ±‡\n",
    "2. è¯·ç”¨æ ¼å¼ [çŒœæµ‹] å¼€å§‹ä½ çš„å›ç­”\n",
    "3. åªè¯´å‡ºä½ è®¤ä¸ºçš„ç­”æ¡ˆï¼Œä¸è¦è§£é‡Š\n",
    "\n",
    "{history_text}é˜Ÿå‹æœ€æ–°çº¿ç´¢: {hint}\n",
    "\n",
    "ä½ çš„çŒœæµ‹æ˜¯:\"\"\"\n",
    "        \n",
    "        guesser_response = robust_chinese_api_call(\n",
    "            client, guesser_model, guesser_prompt, \"[çŒœæµ‹]\"\n",
    "        )\n",
    "        \n",
    "        if not guesser_response['success']:\n",
    "            return create_chinese_game_result(\n",
    "                False, turn, None, f\"çŒœæµ‹ç”Ÿæˆå¤±è´¥: {guesser_response['content']}\", \n",
    "                conversation_history\n",
    "            )\n",
    "        \n",
    "        guess = guesser_response['content']\n",
    "        \n",
    "        # è®°å½•å¯¹è¯\n",
    "        conversation_history.append({\n",
    "            'turn': turn,\n",
    "            'hint': hint,\n",
    "            'guess': guess,\n",
    "            'hinter_attempts': hinter_response['attempts'],\n",
    "            'guesser_attempts': guesser_response['attempts']\n",
    "        })\n",
    "        \n",
    "        # æ£€æŸ¥æ˜¯å¦çŒœä¸­\n",
    "        if check_chinese_word_match(guess, target_word):\n",
    "            return create_chinese_game_result(\n",
    "                True, turn, guess, \"æˆåŠŸ\", conversation_history\n",
    "            )\n",
    "        \n",
    "        # æ›´æ–°hinterçš„æç¤ºï¼ŒåŒ…å«ä¹‹å‰çš„å†å²\n",
    "        previous_hints = [conv['hint'] for conv in conversation_history]\n",
    "        previous_guesses = [conv['guess'] for conv in conversation_history]\n",
    "        \n",
    "        hinter_base_prompt = f\"\"\"ä½ æ­£åœ¨ç©ä¸­æ–‡Tabooæ¸¸æˆã€‚ä½ çš„ä»»åŠ¡æ˜¯è®©é˜Ÿå‹çŒœå‡ºç›®æ ‡è¯æ±‡ï¼Œä½†ä¸èƒ½ä½¿ç”¨ç¦ç”¨è¯ã€‚\n",
    "\n",
    "ç›®æ ‡è¯æ±‡: {target_word}\n",
    "ç¦ç”¨è¯æ±‡: {', '.join(taboo_words)}\n",
    "\n",
    "ä¹‹å‰çš„çº¿ç´¢: {'; '.join(previous_hints)}\n",
    "é˜Ÿå‹çš„çŒœæµ‹: {'; '.join(previous_guesses)}\n",
    "\n",
    "é˜Ÿå‹è¿˜æ²¡æœ‰çŒœä¸­ã€‚è¯·ç»™å‡ºæ–°çš„çº¿ç´¢:\n",
    "1. ä½ çš„çº¿ç´¢ä¸­ä¸èƒ½åŒ…å«ä»»ä½•ç¦ç”¨è¯æ±‡\n",
    "2. è¯·ç”¨æ ¼å¼ [çº¿ç´¢] å¼€å§‹ä½ çš„å›ç­”\n",
    "3. çº¿ç´¢è¦ç®€æ´æ˜äº†ï¼Œä¸è¶…è¿‡20ä¸ªå­—\n",
    "4. å°è¯•ä»ä¸åŒè§’åº¦ç»™å‡ºçº¿ç´¢\n",
    "\n",
    "è¯·ç»™å‡ºä½ çš„æ–°çº¿ç´¢:\"\"\"\n",
    "    \n",
    "    # è¶…è¿‡æœ€å¤§è½®æ•°\n",
    "    final_guess = conversation_history[-1]['guess'] if conversation_history else None\n",
    "    return create_chinese_game_result(\n",
    "        False, max_turns, final_guess, \"è½®æ•°è€—å°½\", conversation_history\n",
    "    )\n",
    "\n",
    "print(\"âœ… ä¸­æ–‡Tabooæ¸¸æˆæ ¸å¿ƒé€»è¾‘å·²å®šä¹‰\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea0bc7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ä¸­æ–‡å…¨é‡å®éªŒå‡½æ•°å·²å®šä¹‰\n"
     ]
    }
   ],
   "source": [
    "# 7.1 å…¨é‡å®éªŒå‡½æ•°\n",
    "def run_chinese_full_experiment(client, models, dataset, sample_size=None):\n",
    "    \"\"\"è¿è¡Œä¸­æ–‡Tabooå…¨é‡å®éªŒ\"\"\"\n",
    "    \n",
    "    if not client:\n",
    "        print(\"âŒ APIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ‰§è¡Œå…¨é‡å®éªŒ\")\n",
    "        return None\n",
    "    \n",
    "    if not dataset:\n",
    "        print(\"âŒ æ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•æ‰§è¡Œå…¨é‡å®éªŒ\")\n",
    "        return None\n",
    "    \n",
    "    # ç¡®å®šå®éªŒè§„æ¨¡\n",
    "    if sample_size and sample_size < len(dataset):\n",
    "        experiment_dataset = random.sample(dataset, sample_size)\n",
    "        print(f\"ğŸ“Š é‡‡æ ·å®éªŒï¼šä»{len(dataset)}ä¸ªè¯æ±‡ä¸­é€‰æ‹©{sample_size}ä¸ª\")\n",
    "    else:\n",
    "        experiment_dataset = dataset\n",
    "        print(f\"ğŸ“Š å…¨é‡å®éªŒï¼šä½¿ç”¨å…¨éƒ¨{len(dataset)}ä¸ªè¯æ±‡\")\n",
    "    \n",
    "    # åˆ›å»ºè¾“å‡ºç›®å½•\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    experiment_dir = f\"results/chinese_full_experiment_{timestamp}\"\n",
    "    os.makedirs(experiment_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"ğŸš€ å¼€å§‹ä¸­æ–‡Tabooå…¨é‡å®éªŒ...\")\n",
    "    print(f\"ğŸ“ è¾“å‡ºç›®å½•: {experiment_dir}\")\n",
    "    print(f\"ğŸ¯ è¯æ±‡æ•°é‡: {len(experiment_dataset)}\")\n",
    "    print(f\"ğŸ¤– æ¨¡å‹æ•°é‡: {len(models)}\")\n",
    "    \n",
    "    total_games = len(experiment_dataset) * len(models) * len(models)\n",
    "    print(f\"ğŸ® æ€»æ¸¸æˆæ•°: {total_games}\")\n",
    "    print(f\"â±ï¸ é¢„è®¡æ—¶é—´: ~{total_games * 0.5 / 60:.1f} åˆ†é’Ÿ\")\n",
    "    \n",
    "    all_results = []\n",
    "    game_counter = 0\n",
    "    \n",
    "    # æŒ‰è¯æ±‡éå†\n",
    "    for word_idx, word_data in enumerate(experiment_dataset, 1):\n",
    "        target_word = word_data['target']\n",
    "        taboo_words = word_data['taboo']\n",
    "        char_count = word_data['metadata']['char_count']\n",
    "        pos = word_data.get('part_of_speech', 'unknown')\n",
    "        \n",
    "        print(f\"\\\\nğŸ¯ è¯æ±‡ {word_idx}/{len(experiment_dataset)}: {target_word} ({pos})\")\n",
    "        \n",
    "        word_success = 0\n",
    "        word_total = 0\n",
    "        \n",
    "        # è¿è¡Œæ‰€æœ‰æ¨¡å‹ç»„åˆ\n",
    "        for hinter_model in models:\n",
    "            for guesser_model in models:\n",
    "                game_counter += 1\n",
    "                word_total += 1\n",
    "                \n",
    "                hinter_name = hinter_model.split('/')[-1]\n",
    "                guesser_name = guesser_model.split('/')[-1]\n",
    "                \n",
    "                # æ‰§è¡Œæ¸¸æˆ\n",
    "                start_time = time.time()\n",
    "                game_result = chinese_taboo_game(\n",
    "                    client, hinter_model, guesser_model,\n",
    "                    target_word, taboo_words, char_count, max_turns=5\n",
    "                )\n",
    "                duration = time.time() - start_time\n",
    "                \n",
    "                # è®°å½•ç»“æœ\n",
    "                result = {\n",
    "                    'game_id': game_counter,\n",
    "                    'word_index': word_idx,\n",
    "                    'target_word': target_word,\n",
    "                    'part_of_speech': pos,\n",
    "                    'category': word_data.get('category', 'chinese_general'),\n",
    "                    'hinter_model': hinter_model,\n",
    "                    'guesser_model': guesser_model,\n",
    "                    'success': game_result['success'],\n",
    "                    'turns_used': game_result['turns'],\n",
    "                    'final_guess': game_result.get('final_guess', ''),\n",
    "                    'failure_reason': game_result.get('failure_reason', ''),\n",
    "                    'duration_seconds': round(duration, 2),\n",
    "                    'taboo_words': '|'.join(taboo_words)\n",
    "                }\n",
    "                all_results.append(result)\n",
    "\n",
    "                if game_result['success']:\n",
    "                    word_success += 1\n",
    "\n",
    "                # æ˜¾ç¤ºè¿›åº¦ï¼ˆæ¯10ä¸ªæ¸¸æˆæ˜¾ç¤ºä¸€æ¬¡ï¼‰\n",
    "                if game_counter % 10 == 0 or game_counter == total_games:\n",
    "                    progress = game_counter / total_games * 100\n",
    "                    print(f\"   è¿›åº¦: {game_counter}/{total_games} ({progress:.1f}%)\")\n",
    "\n",
    "        \n",
    "        # æ¯20ä¸ªè¯ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ\n",
    "        if word_idx % 20 == 0 or word_idx == len(experiment_dataset):\n",
    "            intermediate_df = pd.DataFrame(all_results)\n",
    "            intermediate_file = f\"{experiment_dir}/intermediate_results_{word_idx:06d}.csv\"\n",
    "            intermediate_df.to_csv(intermediate_file, index=False, encoding='utf-8-sig')\n",
    "            print(f\"   ğŸ’¾ ä¸­é—´ç»“æœå·²ä¿å­˜: {intermediate_file}\")\n",
    "\n",
    "    # ä¿å­˜æœ€ç»ˆç»“æœ\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    final_output_file = f\"{experiment_dir}/chinese_full_char_count_results_{timestamp}.csv\"\n",
    "    results_df.to_csv(final_output_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"\\nâœ… ä¸­æ–‡å…¨é‡å®éªŒå®Œæˆï¼\")\n",
    "    print(f\"ğŸ“Š æ€»æ¸¸æˆæ•°: {len(results_df)}\")\n",
    "    print(f\"ğŸ’¾ æœ€ç»ˆç»“æœ: {final_output_file}\")\n",
    "\n",
    "    return results_df\n",
    "\n",
    "print(\"âœ… ä¸­æ–‡å…¨é‡å®éªŒå‡½æ•°å·²å®šä¹‰\")\n",
    "\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00885e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ä¸­æ–‡ç»“æœåˆ†æå‡½æ•°å·²å®šä¹‰\n"
     ]
    }
   ],
   "source": [
    "# 8.1 ä¸­æ–‡å®éªŒç»“æœåˆ†æå‡½æ•°\n",
    "def analyze_chinese_experiment_results(results_df):\n",
    "    \"\"\"åˆ†æä¸­æ–‡å®éªŒç»“æœ\"\"\"\n",
    "    if results_df is None or len(results_df) == 0:\n",
    "        print(\"âŒ æ²¡æœ‰ç»“æœæ•°æ®å¯ä¾›åˆ†æ\")\n",
    "        return None\n",
    "    \n",
    "    print(\"ğŸ“Š ä¸­æ–‡Tabooå®éªŒç»“æœåˆ†æ\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # æ•´ä½“æˆåŠŸç‡\n",
    "    total_games = len(results_df)\n",
    "    successful_games = sum(results_df['success'])\n",
    "    overall_success_rate = successful_games / total_games * 100\n",
    "    \n",
    "    print(f\"ğŸ® æ€»æ¸¸æˆæ•°: {total_games}\")\n",
    "    print(f\"âœ… æˆåŠŸæ¸¸æˆ: {successful_games}\")\n",
    "    print(f\"ğŸ“ˆ æ•´ä½“æˆåŠŸç‡: {overall_success_rate:.1f}%\")\n",
    "    \n",
    "    # æŒ‰æ¨¡å‹åˆ†æ\n",
    "    print(f\"\\nğŸ¤– æ¨¡å‹è¡¨ç°åˆ†æ:\")\n",
    "    models = results_df['hinter_model'].unique()\n",
    "    \n",
    "    for model in models:\n",
    "        model_name = model.split('/')[-1]\n",
    "        \n",
    "        # ä½œä¸ºHinterçš„è¡¨ç°\n",
    "        hinter_results = results_df[results_df['hinter_model'] == model]\n",
    "        hinter_success = sum(hinter_results['success'])\n",
    "        hinter_total = len(hinter_results)\n",
    "        hinter_rate = hinter_success / hinter_total * 100 if hinter_total > 0 else 0\n",
    "        \n",
    "        # ä½œä¸ºGuesserçš„è¡¨ç°\n",
    "        guesser_results = results_df[results_df['guesser_model'] == model]\n",
    "        guesser_success = sum(guesser_results['success'])\n",
    "        guesser_total = len(guesser_results)\n",
    "        guesser_rate = guesser_success / guesser_total * 100 if guesser_total > 0 else 0\n",
    "        \n",
    "        print(f\"   {model_name}:\")\n",
    "        print(f\"     ä½œä¸ºçº¿ç´¢ç»™å‡ºè€…: {hinter_success}/{hinter_total} ({hinter_rate:.1f}%)\")\n",
    "        print(f\"     ä½œä¸ºçŒœæµ‹è€…: {guesser_success}/{guesser_total} ({guesser_rate:.1f}%)\")\n",
    "    \n",
    "    # æŒ‰è¯æ€§åˆ†æ\n",
    "    if 'part_of_speech' in results_df.columns:\n",
    "        print(f\"\\nğŸ“ è¯æ€§è¡¨ç°åˆ†æ:\")\n",
    "        pos_types = results_df['part_of_speech'].unique()\n",
    "        \n",
    "        for pos in pos_types:\n",
    "            pos_results = results_df[results_df['part_of_speech'] == pos]\n",
    "            pos_success = sum(pos_results['success'])\n",
    "            pos_total = len(pos_results)\n",
    "            pos_rate = pos_success / pos_total * 100 if pos_total > 0 else 0\n",
    "            \n",
    "            print(f\"   {pos}: {pos_success}/{pos_total} ({pos_rate:.1f}%)\")\n",
    "    \n",
    "    # å¤±è´¥åŸå› åˆ†æ\n",
    "    print(f\"\\nâŒ å¤±è´¥åŸå› åˆ†æ:\")\n",
    "    failed_results = results_df[results_df['success'] == False]\n",
    "    \n",
    "    if len(failed_results) > 0:\n",
    "        failure_reasons = failed_results['failure_reason'].value_counts()\n",
    "        \n",
    "        for reason, count in failure_reasons.items():\n",
    "            percentage = count / len(failed_results) * 100\n",
    "            print(f\"   {reason}: {count} æ¬¡ ({percentage:.1f}%)\")\n",
    "    else:\n",
    "        print(\"   ğŸ‰ æ²¡æœ‰å¤±è´¥æ¡ˆä¾‹ï¼\")\n",
    "    \n",
    "    # è½®æ•°åˆ†æ\n",
    "    print(f\"\\nğŸ”„ æ¸¸æˆè½®æ•°åˆ†æ:\")\n",
    "    successful_results = results_df[results_df['success'] == True]\n",
    "    if len(successful_results) > 0:\n",
    "        avg_turns = successful_results['turns_used'].mean()\n",
    "        print(f\"   æˆåŠŸæ¸¸æˆå¹³å‡è½®æ•°: {avg_turns:.1f}\")\n",
    "        \n",
    "        turn_distribution = successful_results['turns_used'].value_counts().sort_index()\n",
    "        for turns, count in turn_distribution.items():\n",
    "            percentage = count / len(successful_results) * 100\n",
    "            print(f\"   {turns}è½®æˆåŠŸ: {count} æ¬¡ ({percentage:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'overall_success_rate': overall_success_rate,\n",
    "        'total_games': total_games,\n",
    "        'successful_games': successful_games,\n",
    "        'model_analysis': {model.split('/')[-1]: {\n",
    "            'hinter_rate': sum(results_df[results_df['hinter_model'] == model]['success']) / len(results_df[results_df['hinter_model'] == model]) * 100,\n",
    "            'guesser_rate': sum(results_df[results_df['guesser_model'] == model]['success']) / len(results_df[results_df['guesser_model'] == model]) * 100\n",
    "        } for model in models}\n",
    "    }\n",
    "\n",
    "print(\"âœ… ä¸­æ–‡ç»“æœåˆ†æå‡½æ•°å·²å®šä¹‰\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de6cad1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ä¸­æ–‡Tabooå®éªŒç³»ç»Ÿå·²å®Œå…¨å‡†å¤‡å°±ç»ªï¼\n",
      "\n",
      "ğŸ“‹ å¯æ‰§è¡Œçš„å®éªŒé€‰é¡¹:\n",
      "âœ… æ‰€æœ‰ç»„ä»¶å·²å‡†å¤‡å°±ç»ª\n",
      "\n",
      "ğŸ’¡ æ‰§è¡Œé€‰é¡¹:\n",
      "1. å¿«é€ŸéªŒè¯æµ‹è¯•ï¼ˆæ¨èå…ˆæ‰§è¡Œï¼‰:\n",
      "   # å·²åœ¨ä¸Šé¢æ‰§è¡Œè¿‡æµ‹è¯•\n",
      "\n",
      "2. å°è§„æ¨¡å®éªŒï¼ˆé‡‡æ ·10ä¸ªè¯æ±‡ï¼‰:\n",
      "   # small_results = run_chinese_full_experiment(chinese_client, CHINESE_MODELS, chinese_dataset, sample_size=10)\n",
      "\n",
      "3. å®Œæ•´å®éªŒï¼ˆæ‰€æœ‰è¯æ±‡ï¼‰:\n",
      "   # full_results = run_chinese_full_experiment(chinese_client, CHINESE_MODELS, chinese_dataset)\n",
      "\n",
      "4. ç»“æœåˆ†æ:\n",
      "   # analysis = analyze_chinese_experiment_results(full_results)\n",
      "\n",
      "ğŸ“Š å®Œæ•´å®éªŒè§„æ¨¡:\n",
      "   è¯æ±‡æ•°é‡: 40\n",
      "   æ¨¡å‹æ•°é‡: 2\n",
      "   æ€»æ¸¸æˆæ•°: 160\n",
      "   é¢„è®¡æ—¶é—´: ~1.3 åˆ†é’Ÿ\n",
      "\n",
      "ğŸ¯ å®éªŒç‰¹ç‚¹:\n",
      "   â€¢ é¦–æ¬¡ä½¿ç”¨OpenHowNetæ„å»ºçš„ä¸­æ–‡æ•°æ®é›†\n",
      "   â€¢ ä¸­æ–‡æ ¼å¼è¦æ±‚: [çº¿ç´¢] å’Œ [çŒœæµ‹]\n",
      "   â€¢ ä¸­æ–‡ç¦ç”¨è¯æ£€æµ‹: ä½¿ç”¨jiebaåˆ†è¯\n",
      "   â€¢ æ‰¹æ¬¡ä¿å­˜: æ¯20ä¸ªè¯æ±‡ä¿å­˜ä¸­é—´ç»“æœ\n",
      "   â€¢ UTF-8ç¼–ç : å®Œæ•´æ”¯æŒä¸­æ–‡å­—ç¬¦\n",
      "\n",
      "==================================================\n",
      "ğŸ ä¸­æ–‡Tabooå®éªŒæ¡†æ¶æ¢³ç†å®Œæˆ!\n",
      "ğŸ“ æ­¤å®éªŒå®Œå…¨å‚ç…§base_test.ipynbçš„8æ¨¡å—æ¶æ„\n",
      "ğŸ”¬ ä¸“é—¨é’ˆå¯¹ä¸­æ–‡è¯­è¨€å’ŒOpenHowNetæ•°æ®é›†ä¼˜åŒ–\n",
      "ğŸ“Š æä¾›å®Œæ•´çš„å®éªŒã€åˆ†æå’ŒæŠ¥å‘Šç”ŸæˆåŠŸèƒ½\n"
     ]
    }
   ],
   "source": [
    "# 8.2 å®éªŒæ‰§è¡Œé€‰é¡¹\n",
    "print(\"ğŸš€ ä¸­æ–‡Tabooå®éªŒç³»ç»Ÿå·²å®Œå…¨å‡†å¤‡å°±ç»ªï¼\")\n",
    "print(\"\\nğŸ“‹ å¯æ‰§è¡Œçš„å®éªŒé€‰é¡¹:\")\n",
    "\n",
    "if chinese_client and chinese_dataset:\n",
    "    print(\"âœ… æ‰€æœ‰ç»„ä»¶å·²å‡†å¤‡å°±ç»ª\")\n",
    "    print(\"\\nğŸ’¡ æ‰§è¡Œé€‰é¡¹:\")\n",
    "    print(\"1. å¿«é€ŸéªŒè¯æµ‹è¯•ï¼ˆæ¨èå…ˆæ‰§è¡Œï¼‰:\")\n",
    "    print(\"   # å·²åœ¨ä¸Šé¢æ‰§è¡Œè¿‡æµ‹è¯•\")\n",
    "    print(\"\\n2. å°è§„æ¨¡å®éªŒï¼ˆé‡‡æ ·10ä¸ªè¯æ±‡ï¼‰:\")\n",
    "    print(\"   # small_results = run_chinese_full_experiment(chinese_client, CHINESE_MODELS, chinese_dataset, sample_size=10)\")\n",
    "    print(\"\\n3. å®Œæ•´å®éªŒï¼ˆæ‰€æœ‰è¯æ±‡ï¼‰:\")\n",
    "    print(\"   # full_results = run_chinese_full_experiment(chinese_client, CHINESE_MODELS, chinese_dataset)\")\n",
    "    print(\"\\n4. ç»“æœåˆ†æ:\")\n",
    "    print(\"   # analysis = analyze_chinese_experiment_results(full_results)\")\n",
    "    \n",
    "    # å®éªŒè§„æ¨¡é¢„ä¼°\n",
    "    total_words = len(chinese_dataset)\n",
    "    total_models = len(CHINESE_MODELS)\n",
    "    total_games_full = total_words * total_models * total_models\n",
    "    \n",
    "    print(f\"\\nğŸ“Š å®Œæ•´å®éªŒè§„æ¨¡:\")\n",
    "    print(f\"   è¯æ±‡æ•°é‡: {total_words}\")\n",
    "    print(f\"   æ¨¡å‹æ•°é‡: {total_models}\")\n",
    "    print(f\"   æ€»æ¸¸æˆæ•°: {total_games_full:,}\")\n",
    "    print(f\"   é¢„è®¡æ—¶é—´: ~{total_games_full * 0.5 / 60:.1f} åˆ†é’Ÿ\")\n",
    "    \n",
    "    print(f\"\\nğŸ¯ å®éªŒç‰¹ç‚¹:\")\n",
    "    print(f\"   â€¢ é¦–æ¬¡ä½¿ç”¨OpenHowNetæ„å»ºçš„ä¸­æ–‡æ•°æ®é›†\")\n",
    "    print(f\"   â€¢ ä¸­æ–‡æ ¼å¼è¦æ±‚: [çº¿ç´¢] å’Œ [çŒœæµ‹]\")\n",
    "    print(f\"   â€¢ ä¸­æ–‡ç¦ç”¨è¯æ£€æµ‹: ä½¿ç”¨jiebaåˆ†è¯\")\n",
    "    print(f\"   â€¢ æ‰¹æ¬¡ä¿å­˜: æ¯20ä¸ªè¯æ±‡ä¿å­˜ä¸­é—´ç»“æœ\")\n",
    "    print(f\"   â€¢ UTF-8ç¼–ç : å®Œæ•´æ”¯æŒä¸­æ–‡å­—ç¬¦\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ ç¼ºå°‘å¿…è¦ç»„ä»¶ï¼š\")\n",
    "    if not chinese_client:\n",
    "        print(\"   - APIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼ˆéœ€è¦é…ç½®api_keys.jsonï¼‰\")\n",
    "    if not chinese_dataset:\n",
    "        print(\"   - ä¸­æ–‡æ•°æ®é›†æœªåŠ è½½ï¼ˆæ£€æŸ¥data/chinese_dataset.jsonï¼‰\")\n",
    "    print(\"\\nğŸ’¡ è¯·å…ˆè¿è¡Œå‰é¢çš„æ­¥éª¤æ¥åˆå§‹åŒ–è¿™äº›ç»„ä»¶\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ ä¸­æ–‡Tabooå®éªŒæ¡†æ¶æ¢³ç†å®Œæˆ!\")\n",
    "print(\"ğŸ“ æ­¤å®éªŒå®Œå…¨å‚ç…§base_test.ipynbçš„8æ¨¡å—æ¶æ„\")\n",
    "print(\"ğŸ”¬ ä¸“é—¨é’ˆå¯¹ä¸­æ–‡è¯­è¨€å’ŒOpenHowNetæ•°æ®é›†ä¼˜åŒ–\")\n",
    "print(\"ğŸ“Š æä¾›å®Œæ•´çš„å®éªŒã€åˆ†æå’ŒæŠ¥å‘Šç”ŸæˆåŠŸèƒ½\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "660da7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š å…¨é‡å®éªŒï¼šä½¿ç”¨å…¨éƒ¨40ä¸ªè¯æ±‡\n",
      "ğŸš€ å¼€å§‹ä¸­æ–‡Tabooå…¨é‡å®éªŒ...\n",
      "ğŸ“ è¾“å‡ºç›®å½•: results/chinese_full_experiment_20250717_231541\n",
      "ğŸ¯ è¯æ±‡æ•°é‡: 40\n",
      "ğŸ¤– æ¨¡å‹æ•°é‡: 2\n",
      "ğŸ® æ€»æ¸¸æˆæ•°: 160\n",
      "â±ï¸ é¢„è®¡æ—¶é—´: ~1.3 åˆ†é’Ÿ\n",
      "\\nğŸ¯ è¯æ±‡ 1/40: é—ä½“ (noun)\n",
      "\\nğŸ¯ è¯æ±‡ 2/40: å®£ä¼ ç‰Œ (noun)\n",
      "\\nğŸ¯ è¯æ±‡ 3/40: å›´å¢™ (noun)\n",
      "   è¿›åº¦: 10/160 (6.2%)\n",
      "\\nğŸ¯ è¯æ±‡ 4/40: åˆºæ¢¨ (noun)\n",
      "\\nğŸ¯ è¯æ±‡ 5/40: ç‰›è›™ (noun)\n",
      "   è¿›åº¦: 20/160 (12.5%)\n",
      "\\nğŸ¯ è¯æ±‡ 6/40: ç¤¼å“åŒ…è£…çº¸ (noun)\n",
      "\\nğŸ¯ è¯æ±‡ 7/40: ç©ºæˆ¿ç‡ (noun)\n",
      "\\nğŸ¯ è¯æ±‡ 8/40: ç•› (noun)\n",
      "   è¿›åº¦: 30/160 (18.8%)\n",
      "\\nğŸ¯ è¯æ±‡ 9/40: çƒ¤é¢åŒ… (noun)\n",
      "\\nğŸ¯ è¯æ±‡ 10/40: ä¸€æœˆ (noun)\n",
      "   è¿›åº¦: 40/160 (25.0%)\n",
      "\\nğŸ¯ è¯æ±‡ 11/40: æ‚ä½ (verb)\n",
      "\\nğŸ¯ è¯æ±‡ 12/40: å‘ŠçŸ¥ (verb)\n",
      "\\nğŸ¯ è¯æ±‡ 13/40: æœ½å (verb)\n",
      "   è¿›åº¦: 50/160 (31.2%)\n",
      "\\nğŸ¯ è¯æ±‡ 14/40: æœé—¨è°¢å®¢ (verb)\n",
      "\\nğŸ¯ è¯æ±‡ 15/40: äº¤æ¸¸ (verb)\n",
      "   è¿›åº¦: 60/160 (37.5%)\n",
      "\\nğŸ¯ è¯æ±‡ 16/40: äº¤æµ (verb)\n",
      "\\nğŸ¯ è¯æ±‡ 17/40: ä»»äººæ‘†å¸ƒ (verb)\n",
      "\\nğŸ¯ è¯æ±‡ 18/40: è·Œå€’ (verb)\n",
      "   è¿›åº¦: 70/160 (43.8%)\n",
      "\\nğŸ¯ è¯æ±‡ 19/40: åœæˆ˜ (verb)\n",
      "\\nğŸ¯ è¯æ±‡ 20/40: ç™¾èˆ¬åˆéš¾ (verb)\n",
      "   è¿›åº¦: 80/160 (50.0%)\n",
      "   ğŸ’¾ ä¸­é—´ç»“æœå·²ä¿å­˜: results/chinese_full_experiment_20250717_231541/intermediate_results_000020.csv\n",
      "\\nğŸ¯ è¯æ±‡ 21/40: ç°è‰² (adj)\n",
      "\\nğŸ¯ è¯æ±‡ 22/40: çœ©ç›® (adj)\n",
      "\\nğŸ¯ è¯æ±‡ 23/40: åšè´ä¸å±ˆ (adj)\n",
      "   è¿›åº¦: 90/160 (56.2%)\n",
      "\\nğŸ¯ è¯æ±‡ 24/40: æ­£å‘ (adj)\n",
      "\\nğŸ¯ è¯æ±‡ 25/40: èº«æ®‹å¿—åš (adj)\n",
      "   è¿›åº¦: 100/160 (62.5%)\n",
      "\\nğŸ¯ è¯æ±‡ 26/40: å§”é¡ä¸æŒ¯ (adj)\n",
      "\\nğŸ¯ è¯æ±‡ 27/40: è±ªæ”¾ (adj)\n",
      "\\nğŸ¯ è¯æ±‡ 28/40: çƒ­é—¨ (adj)\n",
      "   è¿›åº¦: 110/160 (68.8%)\n",
      "\\nğŸ¯ è¯æ±‡ 29/40: å”¯ç‹¬ (adj)\n",
      "\\nğŸ¯ è¯æ±‡ 30/40: ä½å£° (adj)\n",
      "   è¿›åº¦: 120/160 (75.0%)\n",
      "\\nğŸ¯ è¯æ±‡ 31/40: ä¿¡æ¯åŒ®ä¹åœ° (adv)\n",
      "\\nğŸ¯ è¯æ±‡ 32/40: ç¼•ç¼•è¡Œè¡Œ (adv)\n",
      "\\nğŸ¯ è¯æ±‡ 33/40: é’¦ä½©åœ° (adv)\n",
      "   è¿›åº¦: 130/160 (81.2%)\n",
      "\\nğŸ¯ è¯æ±‡ 34/40: æ¯«ä¸ (adv)\n",
      "\\nğŸ¯ è¯æ±‡ 35/40: ç…§ä¾‹ (adv)\n",
      "   è¿›åº¦: 140/160 (87.5%)\n",
      "\\nğŸ¯ è¯æ±‡ 36/40: ç¡®åˆ‡åœ° (adv)\n",
      "\\nğŸ¯ è¯æ±‡ 37/40: å¾ˆé—æ†¾ (adv)\n",
      "\\nğŸ¯ è¯æ±‡ 38/40: ä½•æ•… (adv)\n",
      "   è¿›åº¦: 150/160 (93.8%)\n",
      "\\nğŸ¯ è¯æ±‡ 39/40: ç›¸åº”åœ° (adv)\n",
      "\\nğŸ¯ è¯æ±‡ 40/40: ä»æ­¤ (adv)\n",
      "   è¿›åº¦: 160/160 (100.0%)\n",
      "   ğŸ’¾ ä¸­é—´ç»“æœå·²ä¿å­˜: results/chinese_full_experiment_20250717_231541/intermediate_results_000040.csv\n",
      "\n",
      "âœ… ä¸­æ–‡å…¨é‡å®éªŒå®Œæˆï¼\n",
      "ğŸ“Š æ€»æ¸¸æˆæ•°: 160\n",
      "ğŸ’¾ æœ€ç»ˆç»“æœ: results/chinese_full_experiment_20250717_231541/chinese_full_char_count_results_20250717_231541.csv\n"
     ]
    }
   ],
   "source": [
    "full_results = run_chinese_full_experiment(chinese_client, CHINESE_MODELS, chinese_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae87d66-3b15-49ec-ba0e-b7e0ab8deba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
