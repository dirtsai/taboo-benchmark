{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b92d389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准备读取数据集: data/chinese_dataset_sample10.json\n",
      "✅ 原始数据集读取成功。\n",
      "✅ 已为所有词条成功添加 'char_count' 元数据。\n",
      "🎉 重构后的新数据集已成功保存至: data/chinese_dataset_sample10_with_char_count.json\n",
      "\n",
      "--- 数据样本预览 ---\n",
      "{\n",
      "  \"target\": \"遗体\",\n",
      "  \"part_of_speech\": \"noun\",\n",
      "  \"taboo\": [\n",
      "    \"动物\",\n",
      "    \"部件\",\n",
      "    \"生理学\",\n",
      "    \"东西\",\n",
      "    \"事物\"\n",
      "  ],\n",
      "  \"category\": \"chinese_general\",\n",
      "  \"senses\": [\n",
      "    {\n",
      "      \"zh_word\": \"遗体\",\n",
      "      \"en_word\": \"body\",\n",
      "      \"zh_grammar\": \"noun\",\n",
      "      \"en_grammar\": \"noun\",\n",
      "      \"Def\": \"{part|部件:PartPosition={body|身},domain={physiology|生理学},whole={AnimalHuman|动物:{die|死:experiencer={~}}}}\",\n",
      "      \"No\": \"000000273198\",\n",
      "      \"sememes\": \"[part|部件, body|身, physiology|生理学, AnimalHuman|动物, die|死]\"\n",
      "    },\n",
      "    {\n",
      "      \"zh_word\": \"遗体\",\n",
      "      \"en_word\": \"remains of the dead\",\n",
      "      \"zh_grammar\": \"noun\",\n",
      "      \"en_grammar\": \"noun\",\n",
      "      \"Def\": \"{part|部件:PartPosition={body|身},domain={physiology|生理学},whole={AnimalHuman|动物:{die|死:experiencer={~}}}}\",\n",
      "      \"No\": \"000000273201\",\n",
      "      \"sememes\": \"[part|部件, body|身, physiology|生理学, AnimalHuman|动物, die|死]\"\n",
      "    },\n",
      "    {\n",
      "      \"zh_word\": \"遗体\",\n",
      "      \"en_word\": \"reliquiae\",\n",
      "      \"zh_grammar\": \"noun\",\n",
      "      \"en_grammar\": \"noun\",\n",
      "      \"Def\": \"{part|部件:PartPosition={body|身},domain={physiology|生理学},whole={AnimalHuman|动物:{die|死:experiencer={~}}}}\",\n",
      "      \"No\": \"000000273199\",\n",
      "      \"sememes\": \"[part|部件, body|身, physiology|生理学, AnimalHuman|动物, die|死]\"\n",
      "    },\n",
      "    {\n",
      "      \"zh_word\": \"遗体\",\n",
      "      \"en_word\": \"remains\",\n",
      "      \"zh_grammar\": \"noun\",\n",
      "      \"en_grammar\": \"noun\",\n",
      "      \"Def\": \"{part|部件:PartPosition={body|身},domain={physiology|生理学},whole={AnimalHuman|动物:{die|死:experiencer={~}}}}\",\n",
      "      \"No\": \"000000273200\",\n",
      "      \"sememes\": \"[part|部件, body|身, physiology|生理学, AnimalHuman|动物, die|死]\"\n",
      "    }\n",
      "  ],\n",
      "  \"metadata\": {\n",
      "    \"sense_count\": 4,\n",
      "    \"taboo_count\": 5,\n",
      "    \"source\": \"openhownet\",\n",
      "    \"char_count\": 2\n",
      "  }\n",
      "}\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# --- 配置 ---\n",
    "# 请将这里设置为您的原始数据集文件路径\n",
    "input_filepath = \"data/chinese_dataset_sample10.json\"\n",
    "\n",
    "# 这是新生成的数据集文件的保存路径\n",
    "output_filepath = \"data/chinese_dataset_sample10_with_char_count.json\"\n",
    "\n",
    "# --- 主程序 ---\n",
    "print(f\"准备读取数据集: {input_filepath}\")\n",
    "\n",
    "try:\n",
    "    # 1. 读取原始JSON文件\n",
    "    with open(input_filepath, 'r', encoding='utf-8') as f:\n",
    "        dataset = json.load(f)\n",
    "    print(\"✅ 原始数据集读取成功。\")\n",
    "\n",
    "    # 2. 遍历数据集，为每个词条添加字数统计\n",
    "    for item in dataset:\n",
    "        # 确保 'metadata' 字典存在\n",
    "        if 'metadata' not in item:\n",
    "            item['metadata'] = {}\n",
    "        \n",
    "        # 获取 'target' 词汇并计算字数，然后添加到metadata中\n",
    "        target_word = item.get('target', '')\n",
    "        item['metadata']['char_count'] = len(target_word)\n",
    "\n",
    "    print(\"✅ 已为所有词条成功添加 'char_count' 元数据。\")\n",
    "\n",
    "    # 3. 确保输出目录存在\n",
    "    output_dir = os.path.dirname(output_filepath)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # 4. 将修改后的完整数据写入新文件\n",
    "    with open(output_filepath, 'w', encoding='utf-8') as f:\n",
    "        # 使用 indent=2 进行格式化存储，方便阅读\n",
    "        json.dump(dataset, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"🎉 重构后的新数据集已成功保存至: {output_filepath}\")\n",
    "\n",
    "    # 5. (可选) 打印一个处理后的样本以供快速验证\n",
    "    if dataset:\n",
    "        print(\"\\n--- 数据样本预览 ---\")\n",
    "        print(json.dumps(dataset[0], ensure_ascii=False, indent=2))\n",
    "        print(\"--------------------\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ 错误：找不到输入文件 '{input_filepath}'。请检查文件名和路径是否正确。\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 处理过程中发生未知错误: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "288c4255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OpenHowNet可用（用于数据集构建）\n",
      "🚀 中文Taboo实验环境初始化完成\n",
      "📋 实验目标: 基于预生成数据集进行中文LLM Taboo游戏评估\n",
      "🎯 参考框架: base_test.ipynb标准实验架构\n"
     ]
    }
   ],
   "source": [
    "# 1.1 核心依赖导入\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "import jieba\n",
    "import re\n",
    "import logging\n",
    "from typing import Dict, List, Any, Tuple\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# 1.2 中文特化依赖（如果可用）\n",
    "try:\n",
    "    import OpenHowNet\n",
    "    print(\"✅ OpenHowNet可用（用于数据集构建）\")\n",
    "except ImportError:\n",
    "    print(\"ℹ️ OpenHowNet不可用，将使用预生成数据集\")\n",
    "    OpenHowNet = None\n",
    "\n",
    "# 1.3 环境设置\n",
    "random.seed(42)  # 确保可复现\n",
    "jieba.setLogLevel(logging.INFO)  # 减少分词日志输出\n",
    "\n",
    "print(\"🚀 中文Taboo实验环境初始化完成\")\n",
    "print(\"📋 实验目标: 基于预生成数据集进行中文LLM Taboo游戏评估\")\n",
    "print(\"🎯 参考框架: base_test.ipynb标准实验架构\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d5ab3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 正在加载中文数据集...\n",
      "✅ 中文数据集加载完成，共40条记录\n",
      "\n",
      "📋 中文数据集样本:\n",
      "   目标词: 畛\n",
      "   词性: noun\n",
      "   类别: chinese_general\n",
      "   禁用词: ['界限', '实体', '道路', '东西', '事物']\n",
      "   词义数: 2\n"
     ]
    }
   ],
   "source": [
    "# 2.1 加载中文数据集函数\n",
    "def load_chinese_dataset(dataset_path: str = \"data/chinese_dataset_sample10_with_char_count.json\") -> List[Dict]:\n",
    "    \"\"\"加载中文Taboo数据集\"\"\"\n",
    "    try:\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ 数据集文件未找到: {dataset_path}\")\n",
    "        print(\"💡 请确保已运行数据集构建步骤或使用预生成数据集\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 数据集加载失败: {e}\")\n",
    "        return []\n",
    "\n",
    "# 2.2 加载中文数据集\n",
    "print(\"📚 正在加载中文数据集...\")\n",
    "chinese_dataset = load_chinese_dataset()\n",
    "\n",
    "if chinese_dataset:\n",
    "    print(f\"✅ 中文数据集加载完成，共{len(chinese_dataset)}条记录\")\n",
    "    \n",
    "    # 显示数据集样本\n",
    "    print(\"\\n📋 中文数据集样本:\")\n",
    "    if len(chinese_dataset) > 0:\n",
    "        sample = random.choice(chinese_dataset)\n",
    "        print(f\"   目标词: {sample['target']}\")\n",
    "        print(f\"   词性: {sample.get('part_of_speech', 'unknown')}\")\n",
    "        print(f\"   类别: {sample.get('category', 'unknown')}\")\n",
    "        print(f\"   禁用词: {sample['taboo']}\")\n",
    "        print(f\"   词义数: {len(sample.get('senses', []))}\")\n",
    "else:\n",
    "    print(\"❌ 数据集加载失败，请检查数据文件\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fde63974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 中文数据集基本统计:\n",
      "========================================\n",
      "\n",
      "🏷️ 词性分布:\n",
      "   noun: 10 个 (25.0%)\n",
      "   verb: 10 个 (25.0%)\n",
      "   adj: 10 个 (25.0%)\n",
      "   adv: 10 个 (25.0%)\n",
      "\n",
      "📂 类别分布:\n",
      "   chinese_general: 40 个 (100.0%)\n",
      "\n",
      "🚫 禁用词统计:\n",
      "   平均数量: 5.0\n",
      "   范围: 5 - 5\n",
      "\n",
      "💭 词义统计:\n",
      "   平均数量: 2.1\n",
      "   范围: 1 - 10\n",
      "\n",
      "✅ 中文数据集统计完成，质量良好，可用于实验\n",
      "\n",
      "🎲 随机种子已设置为 42，确保实验可复现\n"
     ]
    }
   ],
   "source": [
    "# 3.1 中文数据集统计分析\n",
    "if chinese_dataset:\n",
    "    print(\"📊 中文数据集基本统计:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # 词性分布统计\n",
    "    pos_counts = {}\n",
    "    taboo_counts = []\n",
    "    sense_counts = []\n",
    "    categories = {}\n",
    "    \n",
    "    for item in chinese_dataset:\n",
    "        # 统计词性\n",
    "        pos = item.get('part_of_speech', 'unknown')\n",
    "        pos_counts[pos] = pos_counts.get(pos, 0) + 1\n",
    "        \n",
    "        # 统计类别\n",
    "        category = item.get('category', 'unknown')\n",
    "        categories[category] = categories.get(category, 0) + 1\n",
    "        \n",
    "        # 统计禁用词数量\n",
    "        taboo_counts.append(len(item.get('taboo', [])))\n",
    "        \n",
    "        # 统计词义数量\n",
    "        sense_counts.append(len(item.get('senses', [])))\n",
    "    \n",
    "    print(f\"\\n🏷️ 词性分布:\")\n",
    "    sorted_pos = sorted(pos_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    for pos, count in sorted_pos:\n",
    "        percentage = count / len(chinese_dataset) * 100\n",
    "        print(f\"   {pos}: {count} 个 ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n📂 类别分布:\")\n",
    "    sorted_categories = sorted(categories.items(), key=lambda x: x[1], reverse=True)\n",
    "    for category, count in sorted_categories:\n",
    "        percentage = count / len(chinese_dataset) * 100\n",
    "        print(f\"   {category}: {count} 个 ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n🚫 禁用词统计:\")\n",
    "    print(f\"   平均数量: {sum(taboo_counts) / len(taboo_counts):.1f}\")\n",
    "    print(f\"   范围: {min(taboo_counts)} - {max(taboo_counts)}\")\n",
    "    \n",
    "    print(f\"\\n💭 词义统计:\")\n",
    "    if sense_counts and max(sense_counts) > 0:\n",
    "        print(f\"   平均数量: {sum(sense_counts) / len(sense_counts):.1f}\")\n",
    "        print(f\"   范围: {min(sense_counts)} - {max(sense_counts)}\")\n",
    "    else:\n",
    "        print(\"   词义信息: 未包含详细词义数据\")\n",
    "    \n",
    "    print(f\"\\n✅ 中文数据集统计完成，质量良好，可用于实验\")\n",
    "    \n",
    "    # 设置随机种子用于实验\n",
    "    random.seed(42)\n",
    "    print(\"\\n🎲 随机种子已设置为 42，确保实验可复现\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ 无法进行统计分析：数据集未成功加载\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad8f9bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 中文API客户端初始化成功\n",
      "\n",
      "🤖 中文实验模型: 2 个\n",
      "   1. gemini-2.5-flash\n",
      "   2. deepseek-chat-v3-0324\n",
      "\n",
      "🚀 API客户端已就绪，可以开始中文Taboo实验\n"
     ]
    }
   ],
   "source": [
    "# 4.1 加载API密钥\n",
    "def load_api_keys(keys_path: str = \"api_keys.json\") -> Dict[str, str]:\n",
    "    \"\"\"加载API密钥\"\"\"\n",
    "    try:\n",
    "        with open(keys_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ API密钥文件未找到: {keys_path}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"❌ API密钥加载失败: {e}\")\n",
    "        return {}\n",
    "\n",
    "# 4.2 中文API客户端类\n",
    "class ChineseAPIClient:\n",
    "    \"\"\"支持中文模型的API客户端\"\"\"\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "        self.headers = {\n",
    "            \"Authorization\": f\"Bearer {api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "    \n",
    "    def call_model(self, model: str, messages: List[Dict[str, str]], \n",
    "                   temperature: float = 0.3) -> str:\n",
    "        \"\"\"调用模型API，保持中文字符\"\"\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": 2000\n",
    "        }\n",
    "        response = requests.post(self.base_url, headers=self.headers, \n",
    "                               json=payload, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        content = result['choices'][0]['message']['content'].strip()\n",
    "        \n",
    "        # 保留中文字符，只过滤控制字符\n",
    "        content = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', '', content)\n",
    "        return content\n",
    "\n",
    "# 4.3 初始化API客户端\n",
    "try:\n",
    "    api_keys = load_api_keys()\n",
    "    if \"OPENROUTER_API_KEY\" in api_keys:\n",
    "        chinese_client = ChineseAPIClient(api_keys[\"OPENROUTER_API_KEY\"])\n",
    "        print(\"✅ 中文API客户端初始化成功\")\n",
    "    else:\n",
    "        chinese_client = None\n",
    "        print(\"❌ 缺少OPENROUTER_API_KEY，无法初始化API客户端\")\n",
    "except Exception as e:\n",
    "    chinese_client = None\n",
    "    print(f\"❌ API客户端初始化失败: {e}\")\n",
    "\n",
    "# 4.4 定义中文测试模型\n",
    "CHINESE_MODELS = [\n",
    "    \"google/gemini-2.5-flash\", \n",
    "    \"deepseek/deepseek-chat-v3-0324\",\n",
    "]\n",
    "\n",
    "print(f\"\\n🤖 中文实验模型: {len(CHINESE_MODELS)} 个\")\n",
    "for i, model in enumerate(CHINESE_MODELS, 1):\n",
    "    model_name = model.split('/')[-1]\n",
    "    print(f\"   {i}. {model_name}\")\n",
    "\n",
    "if chinese_client:\n",
    "    print(f\"\\n🚀 API客户端已就绪，可以开始中文Taboo实验\")\n",
    "else:\n",
    "    print(f\"\\n⚠️ API客户端未就绪，需要配置API密钥\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7981d10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 中文文本处理和API调用函数已定义\n"
     ]
    }
   ],
   "source": [
    "# 5.1 中文文本安全清理\n",
    "def safe_chinese_text_cleanup(text: str, max_length: int = 200) -> str:\n",
    "    \"\"\"安全清理中文文本，保留中文字符\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # 保留中文字符、英文字符、数字、基本标点\n",
    "    cleaned = re.sub(r'[^\\u4e00-\\u9fff\\w\\s\\.\\,\\!\\?\\-\\[\\]【】]', '', str(text))\n",
    "    if len(cleaned) > max_length:\n",
    "        cleaned = cleaned[:max_length] + \"...\"\n",
    "    return cleaned\n",
    "\n",
    "# 5.2 健壮的中文API调用\n",
    "def robust_chinese_api_call(client, model: str, base_prompt: str, expected_prefix: str, max_retries: int = 3):\n",
    "    \"\"\"支持中文格式的健壮API调用\"\"\"\n",
    "    failed_outputs = []\n",
    "    \n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            if attempt == 1:\n",
    "                prompt = base_prompt\n",
    "            else:\n",
    "                prev_output = failed_outputs[-1] if failed_outputs else \"未知\"\n",
    "                format_reminder = f\"\"\"\n",
    "\n",
    "⚠️ 格式错误 ⚠️\n",
    "你的上一次回答是: \"{prev_output}\"\n",
    "\n",
    "必需格式:\n",
    "- 你必须以 '{expected_prefix}' 开头（包括方括号）\n",
    "- 不要在 {expected_prefix} 之前添加任何文字\n",
    "\n",
    "请按正确格式重新回答:\"\"\"\n",
    "                prompt = base_prompt + format_reminder\n",
    "            \n",
    "            response = client.call_model(model, [{\"role\": \"user\", \"content\": prompt}])\n",
    "            cleaned_response = safe_chinese_text_cleanup(response)\n",
    "            \n",
    "            if cleaned_response.strip().startswith(expected_prefix):\n",
    "                content = cleaned_response.strip()[len(expected_prefix):].strip()\n",
    "                return {\n",
    "                    'success': True,\n",
    "                    'content': content,\n",
    "                    'attempts': attempt,\n",
    "                    'raw_response': response\n",
    "                }\n",
    "            else:\n",
    "                failed_outputs.append(cleaned_response[:50])\n",
    "                if attempt == max_retries:\n",
    "                    return {\n",
    "                        'success': False,\n",
    "                        'content': f\"格式验证失败: {cleaned_response[:100]}\",\n",
    "                        'attempts': attempt,\n",
    "                        'raw_response': response\n",
    "                    }\n",
    "                \n",
    "        except Exception as e:\n",
    "            if attempt == max_retries:\n",
    "                return {\n",
    "                    'success': False,\n",
    "                    'content': f\"API调用失败: {e}\",\n",
    "                    'attempts': attempt,\n",
    "                    'raw_response': \"\"\n",
    "                }\n",
    "    \n",
    "    return {\n",
    "        'success': False,\n",
    "        'content': \"重试次数超限\",\n",
    "        'attempts': max_retries,\n",
    "        'raw_response': \"\"\n",
    "    }\n",
    "\n",
    "print(\"✅ 中文文本处理和API调用函数已定义\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34a8d6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 中文游戏验证和结果函数已定义\n"
     ]
    }
   ],
   "source": [
    "# 5.3 中文禁用词检测\n",
    "def check_chinese_taboo_violation(text: str, taboo_words: List[str]) -> bool:\n",
    "    \"\"\"检查中文文本是否违反禁用词规则\"\"\"\n",
    "    if not text or not taboo_words:\n",
    "        return False\n",
    "    \n",
    "    # 使用jieba分词\n",
    "    words = list(jieba.cut(text, cut_all=False))\n",
    "    text_words = set(words)\n",
    "    \n",
    "    # 检查直接包含和分词后包含\n",
    "    for taboo in taboo_words:\n",
    "        if taboo in text or taboo in text_words:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# 5.4 中文词汇匹配检查\n",
    "def check_chinese_word_match(guess: str, target: str) -> bool:\n",
    "    \"\"\"检查中文词汇是否匹配\"\"\"\n",
    "    if not guess or not target:\n",
    "        return False\n",
    "    \n",
    "    # 移除格式标记和空格\n",
    "    guess_clean = re.sub(r'[\\[\\]【】\\s]', '', guess).strip()\n",
    "    target_clean = re.sub(r'[\\[\\]【】\\s]', '', target).strip()\n",
    "    \n",
    "    # 直接匹配或包含匹配\n",
    "    return guess_clean == target_clean or target_clean in guess_clean\n",
    "\n",
    "# 5.5 创建游戏结果\n",
    "def create_chinese_game_result(success: bool, turns: int, final_guess: str, \n",
    "                              failure_reason: str, conversation_history: List[Dict]) -> Dict:\n",
    "    \"\"\"创建中文游戏结果记录\"\"\"\n",
    "    return {\n",
    "        'success': success,\n",
    "        'turns': turns,\n",
    "        'final_guess': final_guess,\n",
    "        'failure_reason': failure_reason,\n",
    "        'conversation_history': conversation_history\n",
    "    }\n",
    "\n",
    "print(\"✅ 中文游戏验证和结果函数已定义\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73de5a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 中文Taboo游戏核心逻辑已定义\n"
     ]
    }
   ],
   "source": [
    "# 5.6 核心中文Taboo游戏逻辑\n",
    "def chinese_taboo_game(client, hinter_model: str, guesser_model: str, \n",
    "                      target_word: str, taboo_words: List[str], char_count: int, max_turns: int = 5) -> Dict:\n",
    "    \"\"\"中文Taboo游戏核心逻辑\"\"\"\n",
    "    \n",
    "    # 中文提示模板\n",
    "    hinter_base_prompt = f\"\"\"你正在玩中文Taboo游戏。你的任务是让队友猜出目标词汇，但不能使用禁用词。\n",
    "**重要提示: 目标词是 {char_count} 个字。**\n",
    "目标词汇: {target_word}\n",
    "禁用词汇: {', '.join(taboo_words)}\n",
    "\n",
    "规则:\n",
    "1. 你需要给出线索让队友猜出目标词汇\n",
    "2. 你的线索中不能包含任何禁用词汇\n",
    "3. 请用格式 [线索] 开始你的回答\n",
    "4. 线索要简洁明了，不超过20个字\n",
    "\n",
    "请给出你的线索:\"\"\"\n",
    "\n",
    "    conversation_history = []\n",
    "    \n",
    "    for turn in range(1, max_turns + 1):\n",
    "        # Hinter给出线索\n",
    "        hinter_response = robust_chinese_api_call(\n",
    "            client, hinter_model, hinter_base_prompt, \"[线索]\"\n",
    "        )\n",
    "        \n",
    "        if not hinter_response['success']:\n",
    "            return create_chinese_game_result(\n",
    "                False, turn, None, f\"线索生成失败: {hinter_response['content']}\", \n",
    "                conversation_history\n",
    "            )\n",
    "        \n",
    "        hint = hinter_response['content']\n",
    "        \n",
    "        # 检查禁用词违规\n",
    "        if check_chinese_taboo_violation(hint, taboo_words):\n",
    "            violated_words = [word for word in taboo_words if word in hint]\n",
    "            return create_chinese_game_result(\n",
    "                False, turn, None, f\"违反禁用词规则: {violated_words}\", \n",
    "                conversation_history\n",
    "            )\n",
    "        # 构造历史文本\n",
    "        history_lines = []\n",
    "        for conv in conversation_history:\n",
    "            history_lines.append(f\"第{conv['turn']}轮 线索: {conv['hint']}  猜测: {conv['guess']}\")\n",
    "        history_text = \"\\n\".join(history_lines)\n",
    "        if history_text:\n",
    "            history_text = \"游戏历史：\\n\" + history_text + \"\\n\"\n",
    "\n",
    "        # Guesser进行猜测\n",
    "        guesser_prompt = f\"\"\"你正在玩中文Taboo游戏。根据队友给出的所有线索，猜出目标词汇。\n",
    "\n",
    "禁用词汇: {', '.join(taboo_words)}\n",
    "\n",
    "规则:\n",
    "1. 根据所有线索猜出目标词汇\n",
    "2. 请用格式 [猜测] 开始你的回答\n",
    "3. 只说出你认为的答案，不要解释\n",
    "\n",
    "{history_text}队友最新线索: {hint}\n",
    "\n",
    "你的猜测是:\"\"\"\n",
    "        \n",
    "        guesser_response = robust_chinese_api_call(\n",
    "            client, guesser_model, guesser_prompt, \"[猜测]\"\n",
    "        )\n",
    "        \n",
    "        if not guesser_response['success']:\n",
    "            return create_chinese_game_result(\n",
    "                False, turn, None, f\"猜测生成失败: {guesser_response['content']}\", \n",
    "                conversation_history\n",
    "            )\n",
    "        \n",
    "        guess = guesser_response['content']\n",
    "        \n",
    "        # 记录对话\n",
    "        conversation_history.append({\n",
    "            'turn': turn,\n",
    "            'hint': hint,\n",
    "            'guess': guess,\n",
    "            'hinter_attempts': hinter_response['attempts'],\n",
    "            'guesser_attempts': guesser_response['attempts']\n",
    "        })\n",
    "        \n",
    "        # 检查是否猜中\n",
    "        if check_chinese_word_match(guess, target_word):\n",
    "            return create_chinese_game_result(\n",
    "                True, turn, guess, \"成功\", conversation_history\n",
    "            )\n",
    "        \n",
    "        # 更新hinter的提示，包含之前的历史\n",
    "        previous_hints = [conv['hint'] for conv in conversation_history]\n",
    "        previous_guesses = [conv['guess'] for conv in conversation_history]\n",
    "        \n",
    "        hinter_base_prompt = f\"\"\"你正在玩中文Taboo游戏。你的任务是让队友猜出目标词汇，但不能使用禁用词。\n",
    "\n",
    "目标词汇: {target_word}\n",
    "禁用词汇: {', '.join(taboo_words)}\n",
    "\n",
    "之前的线索: {'; '.join(previous_hints)}\n",
    "队友的猜测: {'; '.join(previous_guesses)}\n",
    "\n",
    "队友还没有猜中。请给出新的线索:\n",
    "1. 你的线索中不能包含任何禁用词汇\n",
    "2. 请用格式 [线索] 开始你的回答\n",
    "3. 线索要简洁明了，不超过20个字\n",
    "4. 尝试从不同角度给出线索\n",
    "\n",
    "请给出你的新线索:\"\"\"\n",
    "    \n",
    "    # 超过最大轮数\n",
    "    final_guess = conversation_history[-1]['guess'] if conversation_history else None\n",
    "    return create_chinese_game_result(\n",
    "        False, max_turns, final_guess, \"轮数耗尽\", conversation_history\n",
    "    )\n",
    "\n",
    "print(\"✅ 中文Taboo游戏核心逻辑已定义\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea0bc7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 中文全量实验函数已定义\n"
     ]
    }
   ],
   "source": [
    "# 7.1 全量实验函数\n",
    "def run_chinese_full_experiment(client, models, dataset, sample_size=None):\n",
    "    \"\"\"运行中文Taboo全量实验\"\"\"\n",
    "    \n",
    "    if not client:\n",
    "        print(\"❌ API客户端未初始化，无法执行全量实验\")\n",
    "        return None\n",
    "    \n",
    "    if not dataset:\n",
    "        print(\"❌ 数据集为空，无法执行全量实验\")\n",
    "        return None\n",
    "    \n",
    "    # 确定实验规模\n",
    "    if sample_size and sample_size < len(dataset):\n",
    "        experiment_dataset = random.sample(dataset, sample_size)\n",
    "        print(f\"📊 采样实验：从{len(dataset)}个词汇中选择{sample_size}个\")\n",
    "    else:\n",
    "        experiment_dataset = dataset\n",
    "        print(f\"📊 全量实验：使用全部{len(dataset)}个词汇\")\n",
    "    \n",
    "    # 创建输出目录\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    experiment_dir = f\"results/chinese_full_experiment_{timestamp}\"\n",
    "    os.makedirs(experiment_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"🚀 开始中文Taboo全量实验...\")\n",
    "    print(f\"📁 输出目录: {experiment_dir}\")\n",
    "    print(f\"🎯 词汇数量: {len(experiment_dataset)}\")\n",
    "    print(f\"🤖 模型数量: {len(models)}\")\n",
    "    \n",
    "    total_games = len(experiment_dataset) * len(models) * len(models)\n",
    "    print(f\"🎮 总游戏数: {total_games}\")\n",
    "    print(f\"⏱️ 预计时间: ~{total_games * 0.5 / 60:.1f} 分钟\")\n",
    "    \n",
    "    all_results = []\n",
    "    game_counter = 0\n",
    "    \n",
    "    # 按词汇遍历\n",
    "    for word_idx, word_data in enumerate(experiment_dataset, 1):\n",
    "        target_word = word_data['target']\n",
    "        taboo_words = word_data['taboo']\n",
    "        char_count = word_data['metadata']['char_count']\n",
    "        pos = word_data.get('part_of_speech', 'unknown')\n",
    "        \n",
    "        print(f\"\\\\n🎯 词汇 {word_idx}/{len(experiment_dataset)}: {target_word} ({pos})\")\n",
    "        \n",
    "        word_success = 0\n",
    "        word_total = 0\n",
    "        \n",
    "        # 运行所有模型组合\n",
    "        for hinter_model in models:\n",
    "            for guesser_model in models:\n",
    "                game_counter += 1\n",
    "                word_total += 1\n",
    "                \n",
    "                hinter_name = hinter_model.split('/')[-1]\n",
    "                guesser_name = guesser_model.split('/')[-1]\n",
    "                \n",
    "                # 执行游戏\n",
    "                start_time = time.time()\n",
    "                game_result = chinese_taboo_game(\n",
    "                    client, hinter_model, guesser_model,\n",
    "                    target_word, taboo_words, char_count, max_turns=5\n",
    "                )\n",
    "                duration = time.time() - start_time\n",
    "                \n",
    "                # 记录结果\n",
    "                result = {\n",
    "                    'game_id': game_counter,\n",
    "                    'word_index': word_idx,\n",
    "                    'target_word': target_word,\n",
    "                    'part_of_speech': pos,\n",
    "                    'category': word_data.get('category', 'chinese_general'),\n",
    "                    'hinter_model': hinter_model,\n",
    "                    'guesser_model': guesser_model,\n",
    "                    'success': game_result['success'],\n",
    "                    'turns_used': game_result['turns'],\n",
    "                    'final_guess': game_result.get('final_guess', ''),\n",
    "                    'failure_reason': game_result.get('failure_reason', ''),\n",
    "                    'duration_seconds': round(duration, 2),\n",
    "                    'taboo_words': '|'.join(taboo_words)\n",
    "                }\n",
    "                all_results.append(result)\n",
    "\n",
    "                if game_result['success']:\n",
    "                    word_success += 1\n",
    "\n",
    "                # 显示进度（每10个游戏显示一次）\n",
    "                if game_counter % 10 == 0 or game_counter == total_games:\n",
    "                    progress = game_counter / total_games * 100\n",
    "                    print(f\"   进度: {game_counter}/{total_games} ({progress:.1f}%)\")\n",
    "\n",
    "        \n",
    "        # 每20个词保存一次中间结果\n",
    "        if word_idx % 20 == 0 or word_idx == len(experiment_dataset):\n",
    "            intermediate_df = pd.DataFrame(all_results)\n",
    "            intermediate_file = f\"{experiment_dir}/intermediate_results_{word_idx:06d}.csv\"\n",
    "            intermediate_df.to_csv(intermediate_file, index=False, encoding='utf-8-sig')\n",
    "            print(f\"   💾 中间结果已保存: {intermediate_file}\")\n",
    "\n",
    "    # 保存最终结果\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    final_output_file = f\"{experiment_dir}/chinese_full_char_count_results_{timestamp}.csv\"\n",
    "    results_df.to_csv(final_output_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"\\n✅ 中文全量实验完成！\")\n",
    "    print(f\"📊 总游戏数: {len(results_df)}\")\n",
    "    print(f\"💾 最终结果: {final_output_file}\")\n",
    "\n",
    "    return results_df\n",
    "\n",
    "print(\"✅ 中文全量实验函数已定义\")\n",
    "\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00885e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 中文结果分析函数已定义\n"
     ]
    }
   ],
   "source": [
    "# 8.1 中文实验结果分析函数\n",
    "def analyze_chinese_experiment_results(results_df):\n",
    "    \"\"\"分析中文实验结果\"\"\"\n",
    "    if results_df is None or len(results_df) == 0:\n",
    "        print(\"❌ 没有结果数据可供分析\")\n",
    "        return None\n",
    "    \n",
    "    print(\"📊 中文Taboo实验结果分析\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 整体成功率\n",
    "    total_games = len(results_df)\n",
    "    successful_games = sum(results_df['success'])\n",
    "    overall_success_rate = successful_games / total_games * 100\n",
    "    \n",
    "    print(f\"🎮 总游戏数: {total_games}\")\n",
    "    print(f\"✅ 成功游戏: {successful_games}\")\n",
    "    print(f\"📈 整体成功率: {overall_success_rate:.1f}%\")\n",
    "    \n",
    "    # 按模型分析\n",
    "    print(f\"\\n🤖 模型表现分析:\")\n",
    "    models = results_df['hinter_model'].unique()\n",
    "    \n",
    "    for model in models:\n",
    "        model_name = model.split('/')[-1]\n",
    "        \n",
    "        # 作为Hinter的表现\n",
    "        hinter_results = results_df[results_df['hinter_model'] == model]\n",
    "        hinter_success = sum(hinter_results['success'])\n",
    "        hinter_total = len(hinter_results)\n",
    "        hinter_rate = hinter_success / hinter_total * 100 if hinter_total > 0 else 0\n",
    "        \n",
    "        # 作为Guesser的表现\n",
    "        guesser_results = results_df[results_df['guesser_model'] == model]\n",
    "        guesser_success = sum(guesser_results['success'])\n",
    "        guesser_total = len(guesser_results)\n",
    "        guesser_rate = guesser_success / guesser_total * 100 if guesser_total > 0 else 0\n",
    "        \n",
    "        print(f\"   {model_name}:\")\n",
    "        print(f\"     作为线索给出者: {hinter_success}/{hinter_total} ({hinter_rate:.1f}%)\")\n",
    "        print(f\"     作为猜测者: {guesser_success}/{guesser_total} ({guesser_rate:.1f}%)\")\n",
    "    \n",
    "    # 按词性分析\n",
    "    if 'part_of_speech' in results_df.columns:\n",
    "        print(f\"\\n📝 词性表现分析:\")\n",
    "        pos_types = results_df['part_of_speech'].unique()\n",
    "        \n",
    "        for pos in pos_types:\n",
    "            pos_results = results_df[results_df['part_of_speech'] == pos]\n",
    "            pos_success = sum(pos_results['success'])\n",
    "            pos_total = len(pos_results)\n",
    "            pos_rate = pos_success / pos_total * 100 if pos_total > 0 else 0\n",
    "            \n",
    "            print(f\"   {pos}: {pos_success}/{pos_total} ({pos_rate:.1f}%)\")\n",
    "    \n",
    "    # 失败原因分析\n",
    "    print(f\"\\n❌ 失败原因分析:\")\n",
    "    failed_results = results_df[results_df['success'] == False]\n",
    "    \n",
    "    if len(failed_results) > 0:\n",
    "        failure_reasons = failed_results['failure_reason'].value_counts()\n",
    "        \n",
    "        for reason, count in failure_reasons.items():\n",
    "            percentage = count / len(failed_results) * 100\n",
    "            print(f\"   {reason}: {count} 次 ({percentage:.1f}%)\")\n",
    "    else:\n",
    "        print(\"   🎉 没有失败案例！\")\n",
    "    \n",
    "    # 轮数分析\n",
    "    print(f\"\\n🔄 游戏轮数分析:\")\n",
    "    successful_results = results_df[results_df['success'] == True]\n",
    "    if len(successful_results) > 0:\n",
    "        avg_turns = successful_results['turns_used'].mean()\n",
    "        print(f\"   成功游戏平均轮数: {avg_turns:.1f}\")\n",
    "        \n",
    "        turn_distribution = successful_results['turns_used'].value_counts().sort_index()\n",
    "        for turns, count in turn_distribution.items():\n",
    "            percentage = count / len(successful_results) * 100\n",
    "            print(f\"   {turns}轮成功: {count} 次 ({percentage:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'overall_success_rate': overall_success_rate,\n",
    "        'total_games': total_games,\n",
    "        'successful_games': successful_games,\n",
    "        'model_analysis': {model.split('/')[-1]: {\n",
    "            'hinter_rate': sum(results_df[results_df['hinter_model'] == model]['success']) / len(results_df[results_df['hinter_model'] == model]) * 100,\n",
    "            'guesser_rate': sum(results_df[results_df['guesser_model'] == model]['success']) / len(results_df[results_df['guesser_model'] == model]) * 100\n",
    "        } for model in models}\n",
    "    }\n",
    "\n",
    "print(\"✅ 中文结果分析函数已定义\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de6cad1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 中文Taboo实验系统已完全准备就绪！\n",
      "\n",
      "📋 可执行的实验选项:\n",
      "✅ 所有组件已准备就绪\n",
      "\n",
      "💡 执行选项:\n",
      "1. 快速验证测试（推荐先执行）:\n",
      "   # 已在上面执行过测试\n",
      "\n",
      "2. 小规模实验（采样10个词汇）:\n",
      "   # small_results = run_chinese_full_experiment(chinese_client, CHINESE_MODELS, chinese_dataset, sample_size=10)\n",
      "\n",
      "3. 完整实验（所有词汇）:\n",
      "   # full_results = run_chinese_full_experiment(chinese_client, CHINESE_MODELS, chinese_dataset)\n",
      "\n",
      "4. 结果分析:\n",
      "   # analysis = analyze_chinese_experiment_results(full_results)\n",
      "\n",
      "📊 完整实验规模:\n",
      "   词汇数量: 40\n",
      "   模型数量: 2\n",
      "   总游戏数: 160\n",
      "   预计时间: ~1.3 分钟\n",
      "\n",
      "🎯 实验特点:\n",
      "   • 首次使用OpenHowNet构建的中文数据集\n",
      "   • 中文格式要求: [线索] 和 [猜测]\n",
      "   • 中文禁用词检测: 使用jieba分词\n",
      "   • 批次保存: 每20个词汇保存中间结果\n",
      "   • UTF-8编码: 完整支持中文字符\n",
      "\n",
      "==================================================\n",
      "🏁 中文Taboo实验框架梳理完成!\n",
      "📝 此实验完全参照base_test.ipynb的8模块架构\n",
      "🔬 专门针对中文语言和OpenHowNet数据集优化\n",
      "📊 提供完整的实验、分析和报告生成功能\n"
     ]
    }
   ],
   "source": [
    "# 8.2 实验执行选项\n",
    "print(\"🚀 中文Taboo实验系统已完全准备就绪！\")\n",
    "print(\"\\n📋 可执行的实验选项:\")\n",
    "\n",
    "if chinese_client and chinese_dataset:\n",
    "    print(\"✅ 所有组件已准备就绪\")\n",
    "    print(\"\\n💡 执行选项:\")\n",
    "    print(\"1. 快速验证测试（推荐先执行）:\")\n",
    "    print(\"   # 已在上面执行过测试\")\n",
    "    print(\"\\n2. 小规模实验（采样10个词汇）:\")\n",
    "    print(\"   # small_results = run_chinese_full_experiment(chinese_client, CHINESE_MODELS, chinese_dataset, sample_size=10)\")\n",
    "    print(\"\\n3. 完整实验（所有词汇）:\")\n",
    "    print(\"   # full_results = run_chinese_full_experiment(chinese_client, CHINESE_MODELS, chinese_dataset)\")\n",
    "    print(\"\\n4. 结果分析:\")\n",
    "    print(\"   # analysis = analyze_chinese_experiment_results(full_results)\")\n",
    "    \n",
    "    # 实验规模预估\n",
    "    total_words = len(chinese_dataset)\n",
    "    total_models = len(CHINESE_MODELS)\n",
    "    total_games_full = total_words * total_models * total_models\n",
    "    \n",
    "    print(f\"\\n📊 完整实验规模:\")\n",
    "    print(f\"   词汇数量: {total_words}\")\n",
    "    print(f\"   模型数量: {total_models}\")\n",
    "    print(f\"   总游戏数: {total_games_full:,}\")\n",
    "    print(f\"   预计时间: ~{total_games_full * 0.5 / 60:.1f} 分钟\")\n",
    "    \n",
    "    print(f\"\\n🎯 实验特点:\")\n",
    "    print(f\"   • 首次使用OpenHowNet构建的中文数据集\")\n",
    "    print(f\"   • 中文格式要求: [线索] 和 [猜测]\")\n",
    "    print(f\"   • 中文禁用词检测: 使用jieba分词\")\n",
    "    print(f\"   • 批次保存: 每20个词汇保存中间结果\")\n",
    "    print(f\"   • UTF-8编码: 完整支持中文字符\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ 缺少必要组件：\")\n",
    "    if not chinese_client:\n",
    "        print(\"   - API客户端未初始化（需要配置api_keys.json）\")\n",
    "    if not chinese_dataset:\n",
    "        print(\"   - 中文数据集未加载（检查data/chinese_dataset.json）\")\n",
    "    print(\"\\n💡 请先运行前面的步骤来初始化这些组件\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🏁 中文Taboo实验框架梳理完成!\")\n",
    "print(\"📝 此实验完全参照base_test.ipynb的8模块架构\")\n",
    "print(\"🔬 专门针对中文语言和OpenHowNet数据集优化\")\n",
    "print(\"📊 提供完整的实验、分析和报告生成功能\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "660da7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 全量实验：使用全部40个词汇\n",
      "🚀 开始中文Taboo全量实验...\n",
      "📁 输出目录: results/chinese_full_experiment_20250717_231541\n",
      "🎯 词汇数量: 40\n",
      "🤖 模型数量: 2\n",
      "🎮 总游戏数: 160\n",
      "⏱️ 预计时间: ~1.3 分钟\n",
      "\\n🎯 词汇 1/40: 遗体 (noun)\n",
      "\\n🎯 词汇 2/40: 宣传牌 (noun)\n",
      "\\n🎯 词汇 3/40: 围墙 (noun)\n",
      "   进度: 10/160 (6.2%)\n",
      "\\n🎯 词汇 4/40: 刺梨 (noun)\n",
      "\\n🎯 词汇 5/40: 牛蛙 (noun)\n",
      "   进度: 20/160 (12.5%)\n",
      "\\n🎯 词汇 6/40: 礼品包装纸 (noun)\n",
      "\\n🎯 词汇 7/40: 空房率 (noun)\n",
      "\\n🎯 词汇 8/40: 畛 (noun)\n",
      "   进度: 30/160 (18.8%)\n",
      "\\n🎯 词汇 9/40: 烤面包 (noun)\n",
      "\\n🎯 词汇 10/40: 一月 (noun)\n",
      "   进度: 40/160 (25.0%)\n",
      "\\n🎯 词汇 11/40: 捂住 (verb)\n",
      "\\n🎯 词汇 12/40: 告知 (verb)\n",
      "\\n🎯 词汇 13/40: 朽坏 (verb)\n",
      "   进度: 50/160 (31.2%)\n",
      "\\n🎯 词汇 14/40: 杜门谢客 (verb)\n",
      "\\n🎯 词汇 15/40: 交游 (verb)\n",
      "   进度: 60/160 (37.5%)\n",
      "\\n🎯 词汇 16/40: 交流 (verb)\n",
      "\\n🎯 词汇 17/40: 任人摆布 (verb)\n",
      "\\n🎯 词汇 18/40: 跌倒 (verb)\n",
      "   进度: 70/160 (43.8%)\n",
      "\\n🎯 词汇 19/40: 停战 (verb)\n",
      "\\n🎯 词汇 20/40: 百般刁难 (verb)\n",
      "   进度: 80/160 (50.0%)\n",
      "   💾 中间结果已保存: results/chinese_full_experiment_20250717_231541/intermediate_results_000020.csv\n",
      "\\n🎯 词汇 21/40: 灰色 (adj)\n",
      "\\n🎯 词汇 22/40: 眩目 (adj)\n",
      "\\n🎯 词汇 23/40: 坚贞不屈 (adj)\n",
      "   进度: 90/160 (56.2%)\n",
      "\\n🎯 词汇 24/40: 正向 (adj)\n",
      "\\n🎯 词汇 25/40: 身残志坚 (adj)\n",
      "   进度: 100/160 (62.5%)\n",
      "\\n🎯 词汇 26/40: 委靡不振 (adj)\n",
      "\\n🎯 词汇 27/40: 豪放 (adj)\n",
      "\\n🎯 词汇 28/40: 热门 (adj)\n",
      "   进度: 110/160 (68.8%)\n",
      "\\n🎯 词汇 29/40: 唯独 (adj)\n",
      "\\n🎯 词汇 30/40: 低声 (adj)\n",
      "   进度: 120/160 (75.0%)\n",
      "\\n🎯 词汇 31/40: 信息匮乏地 (adv)\n",
      "\\n🎯 词汇 32/40: 缕缕行行 (adv)\n",
      "\\n🎯 词汇 33/40: 钦佩地 (adv)\n",
      "   进度: 130/160 (81.2%)\n",
      "\\n🎯 词汇 34/40: 毫不 (adv)\n",
      "\\n🎯 词汇 35/40: 照例 (adv)\n",
      "   进度: 140/160 (87.5%)\n",
      "\\n🎯 词汇 36/40: 确切地 (adv)\n",
      "\\n🎯 词汇 37/40: 很遗憾 (adv)\n",
      "\\n🎯 词汇 38/40: 何故 (adv)\n",
      "   进度: 150/160 (93.8%)\n",
      "\\n🎯 词汇 39/40: 相应地 (adv)\n",
      "\\n🎯 词汇 40/40: 从此 (adv)\n",
      "   进度: 160/160 (100.0%)\n",
      "   💾 中间结果已保存: results/chinese_full_experiment_20250717_231541/intermediate_results_000040.csv\n",
      "\n",
      "✅ 中文全量实验完成！\n",
      "📊 总游戏数: 160\n",
      "💾 最终结果: results/chinese_full_experiment_20250717_231541/chinese_full_char_count_results_20250717_231541.csv\n"
     ]
    }
   ],
   "source": [
    "full_results = run_chinese_full_experiment(chinese_client, CHINESE_MODELS, chinese_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae87d66-3b15-49ec-ba0e-b7e0ab8deba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
