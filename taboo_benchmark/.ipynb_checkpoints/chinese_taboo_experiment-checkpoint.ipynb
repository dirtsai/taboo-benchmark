{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ä¸­æ–‡Tabooå®éªŒ - åŸºäºOpenHowNetçš„æ•°æ®é›†æ„å»ºä¸æµ‹è¯•\n",
        "# ä»¿ç…§base_test.ipynbç»“æ„ï¼Œä¸“é—¨é’ˆå¯¹ä¸­æ–‡è¯æ±‡å’Œè¯­è¨€æ¨¡å‹\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "!pip3 install OpenHowNet\n",
        "!pip3 install jieba\n",
        "!pip3 install requests\n",
        "!pip3 install pandas\n",
        "!pip3 install numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'jieba'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjieba\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, List, Any, Tuple\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'jieba'"
          ]
        }
      ],
      "source": [
        "# 1. å¯¼å…¥ä¾èµ–å’Œè®¾ç½®ç¯å¢ƒ\n",
        "import json\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "import requests\n",
        "import os\n",
        "import jieba\n",
        "import re\n",
        "from typing import Dict, List, Any, Tuple\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "\n",
        "# å®‰è£…å’Œå¯¼å…¥OpenHowNet\n",
        "try:\n",
        "    import OpenHowNet\n",
        "    print(\"âœ… OpenHowNetå·²å¯¼å…¥\")\n",
        "except ImportError:\n",
        "    print(\"âš ï¸ æ­£åœ¨å®‰è£…OpenHowNet...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"OpenHowNet\"])\n",
        "    import OpenHowNet\n",
        "    print(\"âœ… OpenHowNetå®‰è£…å¹¶å¯¼å…¥æˆåŠŸ\")\n",
        "\n",
        "print(\"ğŸš€ ä¸­æ–‡Tabooå®éªŒç¯å¢ƒåˆå§‹åŒ–å®Œæˆ\")\n",
        "print(\"ğŸ“‹ å®éªŒç›®æ ‡: ä½¿ç”¨OpenHowNetæ„å»º100ä¸ªä¸­æ–‡è¯æ±‡çš„Tabooæ•°æ®é›†\")\n",
        "print(\"ğŸ¯ è¯æ€§åˆ†å¸ƒ: åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ã€å‰¯è¯å„25ä¸ª\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. åˆå§‹åŒ–OpenHowNetå’Œä¸­æ–‡å¤„ç†å·¥å…·\n",
        "print(\"ğŸ”§ æ­£åœ¨åˆå§‹åŒ–OpenHowNetå’Œä¸­æ–‡å¤„ç†å·¥å…·...\")\n",
        "\n",
        "# åˆå§‹åŒ–OpenHowNetå®ä¾‹\n",
        "try:\n",
        "    hownet_dict = OpenHowNet.HowNetDict()\n",
        "    print(\"âœ… OpenHowNetè¯å…¸åŠ è½½æˆåŠŸ\")\n",
        "    print(f\"ğŸ“š è¯å…¸åŒ…å«è¯æ±‡æ•°é‡: {len(hownet_dict)} ä¸ªæ¦‚å¿µ\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ OpenHowNetåˆå§‹åŒ–å¤±è´¥: {e}\")\n",
        "    print(\"ğŸ”„ å°è¯•é‡æ–°ä¸‹è½½HowNetæ•°æ®...\")\n",
        "    hownet_dict = OpenHowNet.HowNetDict(init_sim=True)\n",
        "\n",
        "# è®¾ç½®jiebaåˆ†è¯\n",
        "jieba.setLogLevel(20)  # å‡å°‘jiebaçš„æ—¥å¿—è¾“å‡º\n",
        "print(\"âœ… jiebaåˆ†è¯å·¥å…·å·²é…ç½®\")\n",
        "\n",
        "# è®¾ç½®éšæœºç§å­\n",
        "random.seed(42)\n",
        "print(\"ğŸ² éšæœºç§å­å·²è®¾ç½®ä¸º42ï¼Œç¡®ä¿å®éªŒå¯å¤ç°\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. ä¸­æ–‡è¯æ±‡æ•°æ®é›†æ„å»ºå·¥å…·å‡½æ•°\n",
        "\n",
        "def get_pos_mapping():\n",
        "    \"\"\"HowNetè¯æ€§åˆ°æ ‡å‡†è¯æ€§çš„æ˜ å°„\"\"\"\n",
        "    return {\n",
        "        # åè¯ç±»\n",
        "        'N': 'noun', 'noun': 'noun',\n",
        "        # åŠ¨è¯ç±»  \n",
        "        'V': 'verb', 'verb': 'verb',\n",
        "        # å½¢å®¹è¯ç±»\n",
        "        'A': 'adj', 'adj': 'adj', 'a': 'adj',\n",
        "        # å‰¯è¯ç±»\n",
        "        'D': 'adv', 'adv': 'adv', 'd': 'adv'\n",
        "    }\n",
        "\n",
        "def is_valid_chinese_word(word: str) -> bool:\n",
        "    \"\"\"æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ä¸­æ–‡è¯æ±‡\"\"\"\n",
        "    if not word or len(word) < 1:\n",
        "        return False\n",
        "    \n",
        "    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦\n",
        "    chinese_pattern = re.compile(r'[\\u4e00-\\u9fff]+')\n",
        "    if not chinese_pattern.search(word):\n",
        "        return False\n",
        "    \n",
        "    # è¿‡æ»¤è¿‡é•¿æˆ–è¿‡çŸ­çš„è¯\n",
        "    if len(word) > 6 or len(word) < 1:\n",
        "        return False\n",
        "    \n",
        "    # è¿‡æ»¤åŒ…å«ç‰¹æ®Šå­—ç¬¦çš„è¯\n",
        "    special_chars = ['Â·', 'â€”', 'â€¦', 'ã€ˆ', 'ã€‰', 'ã€Š', 'ã€‹', 'ã€Œ', 'ã€']\n",
        "    if any(char in word for char in special_chars):\n",
        "        return False\n",
        "    \n",
        "    return True\n",
        "\n",
        "def extract_similar_words_from_hownet(target_word: str, target_pos: str, hownet_dict, max_count: int = 10) -> List[str]:\n",
        "    \"\"\"ä»HowNetä¸­æå–ä¸ç›®æ ‡è¯ç›¸ä¼¼çš„è¯æ±‡ä½œä¸ºç¦ç”¨è¯å€™é€‰\"\"\"\n",
        "    similar_words = set()\n",
        "    \n",
        "    try:\n",
        "        # è·å–ç›®æ ‡è¯çš„ä¹‰é¡¹\n",
        "        word_senses = hownet_dict.get_senses(target_word)\n",
        "        if not word_senses:\n",
        "            return []\n",
        "        \n",
        "        # ä»ç¬¬ä¸€ä¸ªä¹‰é¡¹å¼€å§‹æå–ç›¸ä¼¼è¯\n",
        "        primary_sense = word_senses[0]\n",
        "        \n",
        "        # æ–¹æ³•1: è·å–åŒä¹‰è¯\n",
        "        try:\n",
        "            synonyms = hownet_dict.get_synonyms(target_word)\n",
        "            for syn_group in synonyms:\n",
        "                for word in syn_group:\n",
        "                    if is_valid_chinese_word(word) and word != target_word:\n",
        "                        similar_words.add(word)\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        # æ–¹æ³•2: é€šè¿‡è¯­ä¹‰ç›¸ä¼¼åº¦è·å–ç›¸ä¼¼è¯\n",
        "        try:\n",
        "            # è·å–HowNetä¸­æ‰€æœ‰è¯æ±‡ï¼Œç„¶åè®¡ç®—ç›¸ä¼¼åº¦\n",
        "            all_words = list(hownet_dict.get_vocab())\n",
        "            chinese_words = [w for w in all_words if is_valid_chinese_word(w)]\n",
        "            \n",
        "            # éšæœºé‡‡æ ·ä¸€äº›è¯æ±‡è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆé¿å…è®¡ç®—é‡è¿‡å¤§ï¼‰\n",
        "            sample_size = min(1000, len(chinese_words))\n",
        "            sampled_words = random.sample(chinese_words, sample_size)\n",
        "            \n",
        "            word_similarities = []\n",
        "            for word in sampled_words:\n",
        "                if word != target_word:\n",
        "                    try:\n",
        "                        similarity = hownet_dict.calculate_word_similarity(target_word, word)\n",
        "                        if similarity > 0.3:  # ç›¸ä¼¼åº¦é˜ˆå€¼\n",
        "                            word_similarities.append((word, similarity))\n",
        "                    except:\n",
        "                        continue\n",
        "            \n",
        "            # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œå–å‰å‡ ä¸ª\n",
        "            word_similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "            for word, _ in word_similarities[:5]:\n",
        "                similar_words.add(word)\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        # æ–¹æ³•3: ä»å®šä¹‰ä¸­æå–å…³é”®è¯\n",
        "        try:\n",
        "            definitions = [sense.get('def', '') for sense in word_senses]\n",
        "            for definition in definitions:\n",
        "                # ä½¿ç”¨jiebaåˆ†è¯æå–å®šä¹‰ä¸­çš„å…³é”®è¯\n",
        "                words_in_def = jieba.lcut(definition)\n",
        "                for word in words_in_def:\n",
        "                    if is_valid_chinese_word(word) and word != target_word and len(word) >= 2:\n",
        "                        similar_words.add(word)\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ æå– {target_word} çš„ç›¸ä¼¼è¯æ—¶å‡ºé”™: {e}\")\n",
        "    \n",
        "    # è¿‡æ»¤å¹¶è¿”å›ç»“æœ\n",
        "    result = [word for word in similar_words if is_valid_chinese_word(word)][:max_count]\n",
        "    return result\n",
        "\n",
        "print(\"âœ… ä¸­æ–‡è¯æ±‡å¤„ç†å·¥å…·å‡½æ•°å·²å®šä¹‰\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. æ„å»ºä¸­æ–‡Tabooæ•°æ®é›†\n",
        "print(\"ğŸ—ï¸ å¼€å§‹æ„å»ºä¸­æ–‡Tabooæ•°æ®é›†...\")\n",
        "\n",
        "def build_chinese_taboo_dataset(hownet_dict, target_count_per_pos: int = 25) -> List[Dict[str, Any]]:\n",
        "    \"\"\"æ„å»ºä¸­æ–‡Tabooæ•°æ®é›†\"\"\"\n",
        "    \n",
        "    pos_mapping = get_pos_mapping()\n",
        "    target_pos_list = ['noun', 'verb', 'adj', 'adv']\n",
        "    dataset = []\n",
        "    \n",
        "    print(f\"ğŸ“Š ç›®æ ‡: æ¯ä¸ªè¯æ€§ {target_count_per_pos} ä¸ªè¯ï¼Œæ€»è®¡ {target_count_per_pos * 4} ä¸ªè¯\")\n",
        "    \n",
        "    # è·å–HowNetè¯æ±‡è¡¨\n",
        "    all_vocab = list(hownet_dict.get_vocab())\n",
        "    chinese_vocab = [word for word in all_vocab if is_valid_chinese_word(word)]\n",
        "    print(f\"ğŸ“š HowNetä¸­æ–‡è¯æ±‡æ€»æ•°: {len(chinese_vocab)} ä¸ª\")\n",
        "    \n",
        "    # æŒ‰è¯æ€§åˆ†ç»„æ”¶é›†è¯æ±‡\n",
        "    words_by_pos = {pos: [] for pos in target_pos_list}\n",
        "    \n",
        "    print(\"ğŸ” æ­£åœ¨åˆ†æè¯æ±‡è¯æ€§...\")\n",
        "    progress_count = 0\n",
        "    \n",
        "    for word in chinese_vocab:\n",
        "        progress_count += 1\n",
        "        if progress_count % 1000 == 0:\n",
        "            print(f\"   å·²å¤„ç† {progress_count}/{len(chinese_vocab)} ä¸ªè¯æ±‡\")\n",
        "        \n",
        "        try:\n",
        "            # è·å–è¯æ±‡çš„ä¹‰é¡¹ä¿¡æ¯\n",
        "            senses = hownet_dict.get_senses(word)\n",
        "            if not senses:\n",
        "                continue\n",
        "            \n",
        "            # è·å–ä¸»è¦è¯æ€§\n",
        "            primary_sense = senses[0]\n",
        "            pos_info = primary_sense.get('pos', '')\n",
        "            \n",
        "            # æ˜ å°„åˆ°æ ‡å‡†è¯æ€§\n",
        "            standard_pos = pos_mapping.get(pos_info, None)\n",
        "            if standard_pos and standard_pos in target_pos_list:\n",
        "                words_by_pos[standard_pos].append({\n",
        "                    'word': word,\n",
        "                    'senses': senses,\n",
        "                    'primary_pos': standard_pos\n",
        "                })\n",
        "        \n",
        "        except Exception:\n",
        "            continue\n",
        "    \n",
        "    print(\"\\nğŸ“ˆ è¯æ€§åˆ†å¸ƒç»Ÿè®¡:\")\n",
        "    for pos, words in words_by_pos.items():\n",
        "        print(f\"   {pos}: {len(words)} ä¸ªå€™é€‰è¯\")\n",
        "    \n",
        "    # ä¸ºæ¯ä¸ªè¯æ€§éšæœºé€‰æ‹©æŒ‡å®šæ•°é‡çš„è¯æ±‡\n",
        "    print(\"\\nğŸ¯ å¼€å§‹é€‰æ‹©ç›®æ ‡è¯æ±‡å¹¶ç”Ÿæˆç¦ç”¨è¯...\")\n",
        "    \n",
        "    for pos in target_pos_list:\n",
        "        available_words = words_by_pos[pos]\n",
        "        if len(available_words) < target_count_per_pos:\n",
        "            print(f\"âš ï¸ {pos} è¯æ€§å¯ç”¨è¯æ±‡ä¸è¶³ ({len(available_words)} < {target_count_per_pos})\")\n",
        "            selected_count = len(available_words)\n",
        "        else:\n",
        "            selected_count = target_count_per_pos\n",
        "        \n",
        "        # éšæœºé€‰æ‹©è¯æ±‡\n",
        "        selected_words = random.sample(available_words, selected_count)\n",
        "        print(f\"\\nğŸ”„ æ­£åœ¨å¤„ç† {pos} ç±»è¯æ±‡ ({selected_count} ä¸ª)...\")\n",
        "        \n",
        "        for i, word_info in enumerate(selected_words):\n",
        "            target_word = word_info['word']\n",
        "            senses = word_info['senses']\n",
        "            \n",
        "            print(f\"   å¤„ç† {i+1}/{selected_count}: {target_word}\")\n",
        "            \n",
        "            # ç”Ÿæˆç¦ç”¨è¯\n",
        "            taboo_words = extract_similar_words_from_hownet(\n",
        "                target_word, pos, hownet_dict, max_count=8\n",
        "            )\n",
        "            \n",
        "            # å¦‚æœç¦ç”¨è¯ä¸å¤Ÿï¼Œæ·»åŠ ä¸€äº›é€šç”¨çš„ç›¸å…³è¯\n",
        "            if len(taboo_words) < 5:\n",
        "                # ä½¿ç”¨jiebaåˆ†è¯ä»å®šä¹‰ä¸­æå–æ›´å¤šè¯æ±‡\n",
        "                for sense in senses[:2]:  # åªå–å‰ä¸¤ä¸ªä¹‰é¡¹\n",
        "                    definition = sense.get('def', '')\n",
        "                    def_words = jieba.lcut(definition)\n",
        "                    for def_word in def_words:\n",
        "                        if (is_valid_chinese_word(def_word) and \n",
        "                            def_word != target_word and \n",
        "                            len(def_word) >= 2 and \n",
        "                            def_word not in taboo_words):\n",
        "                            taboo_words.append(def_word)\n",
        "                            if len(taboo_words) >= 5:\n",
        "                                break\n",
        "            \n",
        "            # ç¡®ä¿è‡³å°‘æœ‰5ä¸ªç¦ç”¨è¯\n",
        "            taboo_words = taboo_words[:5]  # é™åˆ¶ä¸º5ä¸ª\n",
        "            if len(taboo_words) < 5:\n",
        "                # å¦‚æœè¿˜æ˜¯ä¸å¤Ÿï¼Œæ·»åŠ ä¸€äº›é€šç”¨è¯æ±‡\n",
        "                generic_taboos = ['ä¸œè¥¿', 'äº‹ç‰©', 'ç‰©å“', 'æ¦‚å¿µ', 'å†…å®¹']\n",
        "                for generic in generic_taboos:\n",
        "                    if generic not in taboo_words and generic != target_word:\n",
        "                        taboo_words.append(generic)\n",
        "                        if len(taboo_words) >= 5:\n",
        "                            break\n",
        "            \n",
        "            # æ„å»ºæ•°æ®é›†æ¡ç›®\n",
        "            entry = {\n",
        "                'target': target_word,\n",
        "                'part_of_speech': pos,\n",
        "                'taboo': taboo_words[:5],  # ç¡®ä¿æ­£å¥½5ä¸ªç¦ç”¨è¯\n",
        "                'category': 'chinese_general',\n",
        "                'senses': senses,\n",
        "                'metadata': {\n",
        "                    'sense_count': len(senses),\n",
        "                    'taboo_count': len(taboo_words[:5]),\n",
        "                    'source': 'openhownet'\n",
        "                }\n",
        "            }\n",
        "            \n",
        "            dataset.append(entry)\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "# æ„å»ºæ•°æ®é›†\n",
        "chinese_dataset = build_chinese_taboo_dataset(hownet_dict, target_count_per_pos=25)\n",
        "print(f\"\\nâœ… ä¸­æ–‡Tabooæ•°æ®é›†æ„å»ºå®Œæˆï¼\")\n",
        "print(f\"ğŸ“Š æ€»è¯æ±‡æ•°: {len(chinese_dataset)} ä¸ª\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. æ•°æ®é›†ç»Ÿè®¡åˆ†æ\n",
        "print(\"ğŸ“Š ä¸­æ–‡Tabooæ•°æ®é›†ç»Ÿè®¡åˆ†æ:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# åŸºæœ¬ç»Ÿè®¡\n",
        "total_words = len(chinese_dataset)\n",
        "print(f\"ğŸ“ æ€»è¯æ±‡æ•°: {total_words}\")\n",
        "\n",
        "# è¯æ€§åˆ†å¸ƒ\n",
        "pos_counts = {}\n",
        "taboo_counts = []\n",
        "sense_counts = []\n",
        "\n",
        "for item in chinese_dataset:\n",
        "    pos = item.get('part_of_speech', 'unknown')\n",
        "    pos_counts[pos] = pos_counts.get(pos, 0) + 1\n",
        "    taboo_counts.append(len(item.get('taboo', [])))\n",
        "    sense_counts.append(len(item.get('senses', [])))\n",
        "\n",
        "print(f\"\\nğŸ·ï¸ è¯æ€§åˆ†å¸ƒ:\")\n",
        "for pos, count in sorted(pos_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "    percentage = count / total_words * 100\n",
        "    print(f\"   {pos}: {count} ä¸ª ({percentage:.1f}%)\")\n",
        "\n",
        "print(f\"\\nğŸš« ç¦ç”¨è¯ç»Ÿè®¡:\")\n",
        "print(f\"   å¹³å‡æ•°é‡: {sum(taboo_counts) / len(taboo_counts):.1f}\")\n",
        "print(f\"   èŒƒå›´: {min(taboo_counts)} - {max(taboo_counts)}\")\n",
        "\n",
        "print(f\"\\nğŸ’­ ä¹‰é¡¹ç»Ÿè®¡:\")\n",
        "print(f\"   å¹³å‡æ•°é‡: {sum(sense_counts) / len(sense_counts):.1f}\")\n",
        "print(f\"   èŒƒå›´: {min(sense_counts)} - {max(sense_counts)}\")\n",
        "\n",
        "# æ˜¾ç¤ºæ•°æ®æ ·æœ¬\n",
        "print(f\"\\nğŸ“‹ æ•°æ®æ ·æœ¬ (éšæœº5ä¸ª):\")\n",
        "sample_items = random.sample(chinese_dataset, min(5, len(chinese_dataset)))\n",
        "for i, item in enumerate(sample_items, 1):\n",
        "    print(f\"\\n   æ ·æœ¬ {i}:\")\n",
        "    print(f\"     ç›®æ ‡è¯: {item['target']}\")\n",
        "    print(f\"     è¯æ€§: {item['part_of_speech']}\")\n",
        "    print(f\"     ç¦ç”¨è¯: {item['taboo']}\")\n",
        "    if item.get('senses') and len(item['senses']) > 0:\n",
        "        definition = item['senses'][0].get('def', 'æ— å®šä¹‰')\n",
        "        print(f\"     å®šä¹‰: {definition[:50]}...\")\n",
        "\n",
        "print(f\"\\nâœ… ç»Ÿè®¡åˆ†æå®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. ä¿å­˜ä¸­æ–‡æ•°æ®é›†\n",
        "print(\"ğŸ’¾ ä¿å­˜ä¸­æ–‡Tabooæ•°æ®é›†...\")\n",
        "\n",
        "# åˆ›å»ºæ•°æ®ç›®å½•\n",
        "data_dir = \"data\"\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "    print(f\"ğŸ“ åˆ›å»ºæ•°æ®ç›®å½•: {data_dir}\")\n",
        "\n",
        "# ä¿å­˜å®Œæ•´æ•°æ®é›†\n",
        "chinese_dataset_path = os.path.join(data_dir, \"chinese_dataset.json\")\n",
        "with open(chinese_dataset_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(chinese_dataset, f, ensure_ascii=False, indent=2)\n",
        "print(f\"âœ… å®Œæ•´æ•°æ®é›†å·²ä¿å­˜: {chinese_dataset_path}\")\n",
        "\n",
        "# åˆ›å»ºç®€åŒ–ç‰ˆæ•°æ®é›†ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰\n",
        "simplified_dataset = []\n",
        "for item in chinese_dataset:\n",
        "    simplified_item = {\n",
        "        'target': item['target'],\n",
        "        'part_of_speech': item['part_of_speech'],\n",
        "        'taboo': item['taboo'],\n",
        "        'category': item['category']\n",
        "    }\n",
        "    simplified_dataset.append(simplified_item)\n",
        "\n",
        "simplified_path = os.path.join(data_dir, \"chinese_dataset_simple.json\")\n",
        "with open(simplified_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(simplified_dataset, f, ensure_ascii=False, indent=2)\n",
        "print(f\"âœ… ç®€åŒ–æ•°æ®é›†å·²ä¿å­˜: {simplified_path}\")\n",
        "\n",
        "# ç”Ÿæˆæ•°æ®é›†æŠ¥å‘Š\n",
        "report = {\n",
        "    'dataset_info': {\n",
        "        'total_words': len(chinese_dataset),\n",
        "        'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'source': 'OpenHowNet',\n",
        "        'language': 'Chinese',\n",
        "        'pos_distribution': pos_counts,\n",
        "        'avg_taboo_count': sum(taboo_counts) / len(taboo_counts),\n",
        "        'avg_sense_count': sum(sense_counts) / len(sense_counts)\n",
        "    },\n",
        "    'sample_data': sample_items\n",
        "}\n",
        "\n",
        "report_path = os.path.join(data_dir, \"chinese_dataset_report.json\")\n",
        "with open(report_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(report, f, ensure_ascii=False, indent=2)\n",
        "print(f\"âœ… æ•°æ®é›†æŠ¥å‘Šå·²ä¿å­˜: {report_path}\")\n",
        "\n",
        "print(f\"\\nğŸ‰ ä¸­æ–‡Tabooæ•°æ®é›†æ„å»ºå®Œæˆï¼\")\n",
        "print(f\"ğŸ“ æ•°æ®æ–‡ä»¶ä½ç½®:\")\n",
        "print(f\"   å®Œæ•´ç‰ˆ: {chinese_dataset_path}\")\n",
        "print(f\"   ç®€åŒ–ç‰ˆ: {simplified_path}\")\n",
        "print(f\"   æŠ¥å‘Š: {report_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7. APIå®¢æˆ·ç«¯è®¾ç½®ï¼ˆæ”¯æŒä¸­æ–‡æ¨¡å‹ï¼‰\n",
        "print(\"ğŸ”§ è®¾ç½®ä¸­æ–‡Tabooå®éªŒAPIå®¢æˆ·ç«¯...\")\n",
        "\n",
        "def load_api_keys(keys_path: str = \"api_keys.json\") -> Dict[str, str]:\n",
        "    \"\"\"åŠ è½½APIå¯†é’¥\"\"\"\n",
        "    with open(keys_path, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "class ChineseTabooClient:\n",
        "    \"\"\"ä¸­æ–‡Tabooæ¸¸æˆä¸“ç”¨APIå®¢æˆ·ç«¯\"\"\"\n",
        "    def __init__(self, api_key: str):\n",
        "        self.api_key = api_key\n",
        "        self.base_url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "        self.headers = {\n",
        "            \"Authorization\": f\"Bearer {api_key}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "    \n",
        "    def call_model(self, model: str, messages: List[Dict[str, str]], temperature: float = 0.3) -> str:\n",
        "        \"\"\"è°ƒç”¨æ¨¡å‹API\"\"\"\n",
        "        payload = {\n",
        "            \"model\": model,\n",
        "            \"messages\": messages,\n",
        "            \"temperature\": temperature,\n",
        "            \"max_tokens\": 2000\n",
        "        }\n",
        "        \n",
        "        response = requests.post(self.base_url, headers=self.headers, json=payload, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        result = response.json()\n",
        "        content = result['choices'][0]['message']['content'].strip()\n",
        "        \n",
        "        return content\n",
        "\n",
        "# åˆå§‹åŒ–APIå®¢æˆ·ç«¯\n",
        "try:\n",
        "    api_keys = load_api_keys()\n",
        "    chinese_client = ChineseTabooClient(api_keys[\"OPENROUTER_API_KEY\"])\n",
        "    print(\"âœ… ä¸­æ–‡Taboo APIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ APIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}\")\n",
        "    chinese_client = None\n",
        "\n",
        "# å®šä¹‰æ”¯æŒä¸­æ–‡çš„æµ‹è¯•æ¨¡å‹\n",
        "CHINESE_TEST_MODELS = [\n",
        "    \"openai/gpt-4o\",  # GPT-4o æ”¯æŒä¸­æ–‡\n",
        "    \"google/gemini-2.5-flash\",  # Gemini æ”¯æŒä¸­æ–‡\n",
        "    \"deepseek/deepseek-chat-v3-0324\",  # DeepSeek ä¸­æ–‡æ¨¡å‹\n",
        "    \"anthropic/claude-sonnet-4\",  # Claude æ”¯æŒä¸­æ–‡\n",
        "    \"moonshotai/kimi-k2\",  # kimi-k2 çš„APIè·¯å¾„\n",
        "\n",
        "]\n",
        "\n",
        "print(f\"ğŸ¤– ä¸­æ–‡å®éªŒæ¨¡å‹åˆ—è¡¨ ({len(CHINESE_TEST_MODELS)} ä¸ª):\")\n",
        "for i, model in enumerate(CHINESE_TEST_MODELS, 1):\n",
        "    print(f\"   {i}. {model}\")\n",
        "\n",
        "print(f\"\\nğŸ’¡ é€‰æ‹©è¾ƒå°‘æ¨¡å‹è¿›è¡Œæµ‹è¯•ä»¥èŠ‚çœæˆæœ¬å’Œæ—¶é—´\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8. ä¸­æ–‡Tabooæ¸¸æˆæ ¸å¿ƒé€»è¾‘å’Œå·¥å…·å‡½æ•°\n",
        "print(\"ğŸ® å®šä¹‰ä¸­æ–‡Tabooæ¸¸æˆæ ¸å¿ƒé€»è¾‘...\")\n",
        "\n",
        "def safe_chinese_text_cleanup(text: str, max_length: int = 300) -> str:\n",
        "    \"\"\"å®‰å…¨æ¸…ç†ä¸­æ–‡æ–‡æœ¬\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    \n",
        "    # ä¿ç•™ä¸­æ–‡å­—ç¬¦ã€è‹±æ–‡å­—ç¬¦ã€æ•°å­—å’Œå¸¸ç”¨æ ‡ç‚¹\n",
        "    import re\n",
        "    cleaned = re.sub(r'[^\\u4e00-\\u9fff\\w\\s\\.,!?;:\"\\'()[\\]{}\\-]', '', str(text))\n",
        "    \n",
        "    if len(cleaned) > max_length:\n",
        "        cleaned = cleaned[:max_length] + \"...\"\n",
        "    \n",
        "    return cleaned\n",
        "\n",
        "def extract_chinese_clue_text(response: str) -> str:\n",
        "    \"\"\"ä»å“åº”ä¸­æå–ä¸­æ–‡çº¿ç´¢æ–‡æœ¬\"\"\"\n",
        "    if \"FORMAT_ERROR_EXCEEDED\" in response:\n",
        "        return \"FORMAT_ERROR\"\n",
        "    \n",
        "    # æ£€æŸ¥ä¸­æ–‡æ ¼å¼æ ‡è®°\n",
        "    if '[çº¿ç´¢]' in response or '[CLUE]' in response.upper():\n",
        "        import re\n",
        "        # ä¼˜å…ˆåŒ¹é…ä¸­æ–‡æ ‡è®°\n",
        "        match = re.search(r'\\[çº¿ç´¢\\]\\s*(.+)', response, re.DOTALL)\n",
        "        if not match:\n",
        "            match = re.search(r'\\[CLUE\\]\\s*(.+)', response, re.IGNORECASE | re.DOTALL)\n",
        "        \n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "    \n",
        "    # å¤‡ç”¨æ ¼å¼\n",
        "    if 'çº¿ç´¢:' in response or 'Clue:' in response:\n",
        "        if 'çº¿ç´¢:' in response:\n",
        "            return response.split('çº¿ç´¢:')[1].strip()\n",
        "        else:\n",
        "            return response.split('Clue:')[1].strip()\n",
        "    \n",
        "    return \"INVALID_FORMAT\"\n",
        "\n",
        "def extract_chinese_guess_word(response: str) -> str:\n",
        "    \"\"\"ä»å“åº”ä¸­æå–ä¸­æ–‡çŒœæµ‹è¯\"\"\"\n",
        "    if \"FORMAT_ERROR_EXCEEDED\" in response:\n",
        "        return \"FORMAT_ERROR\"\n",
        "    \n",
        "    # æ£€æŸ¥ä¸­æ–‡æ ¼å¼æ ‡è®°\n",
        "    if '[çŒœæµ‹]' in response or '[GUESS]' in response.upper():\n",
        "        import re\n",
        "        # ä¼˜å…ˆåŒ¹é…ä¸­æ–‡æ ‡è®°\n",
        "        match = re.search(r'\\[çŒœæµ‹\\]\\s*(.+)', response)\n",
        "        if not match:\n",
        "            match = re.search(r'\\[GUESS\\]\\s*(.+)', response, re.IGNORECASE)\n",
        "        \n",
        "        if match:\n",
        "            guess_part = match.group(1).strip()\n",
        "            # æå–ç¬¬ä¸€ä¸ªä¸­æ–‡è¯æ±‡\n",
        "            chinese_words = re.findall(r'[\\u4e00-\\u9fff]+', guess_part)\n",
        "            if chinese_words:\n",
        "                return chinese_words[0]\n",
        "    \n",
        "    # å¤‡ç”¨æ ¼å¼\n",
        "    if 'çŒœæµ‹:' in response or 'Guess:' in response:\n",
        "        if 'çŒœæµ‹:' in response:\n",
        "            guess_part = response.split('çŒœæµ‹:')[1].strip()\n",
        "        else:\n",
        "            guess_part = response.split('Guess:')[1].strip()\n",
        "        \n",
        "        chinese_words = re.findall(r'[\\u4e00-\\u9fff]+', guess_part)\n",
        "        if chinese_words:\n",
        "            return chinese_words[0]\n",
        "    \n",
        "    return \"INVALID_FORMAT\"\n",
        "\n",
        "def check_chinese_taboo_violation(hint: str, taboo_words: List[str]) -> bool:\n",
        "    \"\"\"æ£€æŸ¥ä¸­æ–‡çº¿ç´¢æ˜¯å¦è¿åç¦ç”¨è¯è§„åˆ™\"\"\"\n",
        "    hint_cleaned = re.sub(r'[^\\u4e00-\\u9fff]', '', hint.lower())\n",
        "    \n",
        "    for taboo in taboo_words:\n",
        "        taboo_cleaned = re.sub(r'[^\\u4e00-\\u9fff]', '', taboo.lower())\n",
        "        \n",
        "        # æ£€æŸ¥å®Œæ•´åŒ¹é…\n",
        "        if taboo_cleaned in hint_cleaned:\n",
        "            return True\n",
        "        \n",
        "        # æ£€æŸ¥éƒ¨åˆ†åŒ¹é…ï¼ˆå¯¹äºè¾ƒé•¿çš„è¯ï¼‰\n",
        "        if len(taboo_cleaned) >= 2:\n",
        "            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç¦ç”¨è¯çš„ä¸»è¦éƒ¨åˆ†\n",
        "            if len(taboo_cleaned) >= 3:\n",
        "                core_part = taboo_cleaned[:2]  # å–å‰ä¸¤ä¸ªå­—ç¬¦ä½œä¸ºæ ¸å¿ƒ\n",
        "                if core_part in hint_cleaned:\n",
        "                    return True\n",
        "    \n",
        "    return False\n",
        "\n",
        "def robust_chinese_api_call(client, model: str, base_prompt: str, expected_prefix: str, max_retries: int = 3):\n",
        "    \"\"\"å¥å£®çš„ä¸­æ–‡APIè°ƒç”¨\"\"\"\n",
        "    failed_outputs = []\n",
        "    \n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            if attempt == 1:\n",
        "                prompt = base_prompt\n",
        "            else:\n",
        "                prev_output = failed_outputs[-1] if failed_outputs else \"æœªçŸ¥\"\n",
        "                format_reminder = f\"\"\"\n",
        "\n",
        "âš ï¸ æ ¼å¼é”™è¯¯ âš ï¸\n",
        "æ‚¨ä¹‹å‰çš„å›å¤æ˜¯: \"{prev_output}\"\n",
        "\n",
        "å¿…éœ€æ ¼å¼:\n",
        "- æ‚¨å¿…é¡»ä»¥ '{expected_prefix}' å¼€å¤´ï¼ˆåŒ…æ‹¬æ–¹æ‹¬å·ï¼‰\n",
        "- ä¸è¦åœ¨ {expected_prefix} å‰æ·»åŠ ä»»ä½•æ–‡å­—\n",
        "\n",
        "è¯·ä½¿ç”¨æ­£ç¡®æ ¼å¼é‡è¯•:\"\"\"\n",
        "                prompt = base_prompt + format_reminder\n",
        "            \n",
        "            response = client.call_model(model, [{\"role\": \"user\", \"content\": prompt}])\n",
        "            \n",
        "            if (response.strip().startswith(expected_prefix) or \n",
        "                response.strip().upper().startswith(expected_prefix.upper())):\n",
        "                return {\n",
        "                    'success': True,\n",
        "                    'response': response,\n",
        "                    'attempts': attempt,\n",
        "                    'error': None,\n",
        "                    'failed_outputs': failed_outputs\n",
        "                }\n",
        "            else:\n",
        "                safe_response = safe_chinese_text_cleanup(response, max_length=150)\n",
        "                failed_outputs.append(safe_response)\n",
        "                \n",
        "                if attempt == max_retries:\n",
        "                    all_failed = \" | \".join(failed_outputs)\n",
        "                    return {\n",
        "                        'success': False,\n",
        "                        'response': f\"FORMAT_ERROR_EXCEEDED: {safe_response}\",\n",
        "                        'attempts': attempt,\n",
        "                        'error': f\"å°è¯• {max_retries} æ¬¡åå¤±è´¥ã€‚æœŸæœ›æ ¼å¼ '{expected_prefix}'ã€‚æ‰€æœ‰å¤±è´¥è¾“å‡º: {all_failed}\",\n",
        "                        'failed_outputs': failed_outputs\n",
        "                    }\n",
        "                time.sleep(0.5)\n",
        "                \n",
        "        except Exception as e:\n",
        "            safe_error = safe_chinese_text_cleanup(str(e), max_length=150)\n",
        "            error_msg = f\"APIé”™è¯¯ (å°è¯• {attempt}/{max_retries}): {safe_error}\"\n",
        "            \n",
        "            if attempt == max_retries:\n",
        "                return {\n",
        "                    'success': False,\n",
        "                    'response': None,\n",
        "                    'attempts': attempt,\n",
        "                    'error': error_msg,\n",
        "                    'failed_outputs': failed_outputs\n",
        "                }\n",
        "            time.sleep(1.0)\n",
        "    \n",
        "    return {\n",
        "        'success': False,\n",
        "        'response': None,\n",
        "        'attempts': max_retries,\n",
        "        'error': \"è¶…å‡ºæœ€å¤§é‡è¯•æ¬¡æ•°\",\n",
        "        'failed_outputs': failed_outputs\n",
        "    }\n",
        "\n",
        "print(\"âœ… ä¸­æ–‡Tabooæ¸¸æˆæ ¸å¿ƒé€»è¾‘å·²å®šä¹‰\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 9. ä¸­æ–‡Tabooæ¸¸æˆä¸»å‡½æ•°\n",
        "def play_chinese_taboo_game(client, hinter_model, guesser_model, target_word, taboo_words, max_turns=5):\n",
        "    \"\"\"ä¸­æ–‡Tabooæ¸¸æˆä¸»å‡½æ•°\"\"\"\n",
        "    \n",
        "    conversation_history = []\n",
        "    taboo_list = 'ã€'.join(taboo_words)\n",
        "    \n",
        "    # ä¸­æ–‡ç³»ç»Ÿæç¤º - Hinter\n",
        "    hinter_system = f\"\"\"æ‚¨æ­£åœ¨ç©ä¸­æ–‡Tabooæ¸¸æˆï¼Œæ‹…ä»»æç¤ºè€…è§’è‰²ã€‚\n",
        "\n",
        "å…³é”®æ ¼å¼è¦æ±‚:\n",
        "- æ‚¨å¿…é¡»ä»¥ '[çº¿ç´¢]' å¼€å¤´å›å¤ï¼ˆåŒ…æ‹¬æ–¹æ‹¬å·ï¼‰\n",
        "- ä¸è¦åœ¨ [çº¿ç´¢] å‰æ·»åŠ ä»»ä½•æ–‡å­—\n",
        "- ç¤ºä¾‹: [çº¿ç´¢] è¿™æ˜¯ä¸€ç§æœ‰é•¿é¼»å­çš„å¤§å‹å“ºä¹³åŠ¨ç‰©\n",
        "\n",
        "æ¸¸æˆè§„åˆ™:\n",
        "1. ç»å¯¹ä¸èƒ½ä½¿ç”¨è¿™äº›ç¦ç”¨è¯: {taboo_list}\n",
        "2. ç»™å‡ºæœ‰ç”¨çš„çº¿ç´¢å¸®åŠ©çŒœæµ‹è€…æ‰¾åˆ°ç›®æ ‡è¯\n",
        "3. å¯ä»¥çœ‹åˆ°å¯¹è¯å†å²ï¼Œåœ¨ä¹‹å‰çº¿ç´¢åŸºç¡€ä¸Šè¡¥å……\n",
        "4. è¦æœ‰åˆ›æ„ä½†ä¸¥æ ¼éµå¾ªæ ¼å¼\n",
        "5. ä½¿ç”¨æ ‡å‡†ä¸­æ–‡è¡¨è¾¾\n",
        "\n",
        "ç›®æ ‡è¯: {target_word}\n",
        "ç¦ç”¨è¯ï¼ˆç»å¯¹ç¦æ­¢ï¼‰: {taboo_list}\"\"\"\n",
        "\n",
        "    # ä¸­æ–‡ç³»ç»Ÿæç¤º - Guesser\n",
        "    guesser_system = \"\"\"æ‚¨æ­£åœ¨ç©ä¸­æ–‡Tabooæ¸¸æˆï¼Œæ‹…ä»»çŒœæµ‹è€…è§’è‰²ã€‚\n",
        "\n",
        "å…³é”®æ ¼å¼è¦æ±‚:\n",
        "- æ‚¨å¿…é¡»ä»¥ '[çŒœæµ‹]' å¼€å¤´å›å¤ï¼ˆåŒ…æ‹¬æ–¹æ‹¬å·ï¼‰\n",
        "- ä¸è¦åœ¨ [çŒœæµ‹] å‰æ·»åŠ ä»»ä½•æ–‡å­—\n",
        "- ç¤ºä¾‹: [çŒœæµ‹] å¤§è±¡\n",
        "\n",
        "æ¸¸æˆè§„åˆ™:\n",
        "1. æ ¹æ®æ”¶åˆ°çš„æ‰€æœ‰çº¿ç´¢è¿›è¡Œæœ€ä½³çŒœæµ‹\n",
        "2. å¯ä»¥çœ‹åˆ°å¯¹è¯å†å²\n",
        "3. åœ¨ [çŒœæµ‹] ååªç»™å‡ºä¸€ä¸ªä¸­æ–‡è¯æ±‡ä½œä¸ºç­”æ¡ˆ\n",
        "4. ä½¿ç”¨æ ‡å‡†ä¸­æ–‡è¯æ±‡\"\"\"\n",
        "\n",
        "    # è®°å½•ç»Ÿè®¡ä¿¡æ¯\n",
        "    total_hinter_attempts = 0\n",
        "    total_guesser_attempts = 0\n",
        "    format_errors = []\n",
        "    hinter_failed_outputs = []\n",
        "    guesser_failed_outputs = []\n",
        "\n",
        "    for turn in range(1, max_turns + 1):\n",
        "        # æ„å»ºHinteræç¤º\n",
        "        if turn == 1:\n",
        "            hinter_prompt = f\"{hinter_system}\\n\\nè¯·æä¾›æ‚¨çš„ç¬¬ä¸€ä¸ªçº¿ç´¢:\"\n",
        "        else:\n",
        "            history_text = \"\\n\".join([f\"ç¬¬{i}è½®: {msg}\" for i, msg in enumerate(conversation_history, 1)])\n",
        "            hinter_prompt = f\"{hinter_system}\\n\\nå¯¹è¯å†å²:\\n{history_text}\\n\\nçŒœæµ‹è€…è¿˜æ²¡æœ‰æ‰¾åˆ°ç­”æ¡ˆã€‚è¯·æä¾›ä¸‹ä¸€ä¸ªçº¿ç´¢:\"\n",
        "        \n",
        "        # Hinterç»™å‡ºçº¿ç´¢\n",
        "        hinter_result = robust_chinese_api_call(client, hinter_model, hinter_prompt, \"[çº¿ç´¢]\", max_retries=3)\n",
        "        total_hinter_attempts += hinter_result['attempts']\n",
        "        \n",
        "        if hinter_result.get('failed_outputs'):\n",
        "            hinter_failed_outputs.extend(hinter_result['failed_outputs'])\n",
        "        \n",
        "        if not hinter_result['success']:\n",
        "            error_type = \"FORMAT_FAILURE\" if \"FORMAT_ERROR_EXCEEDED\" in str(hinter_result.get('response', '')) else \"API_FAILURE\"\n",
        "            format_errors.append(f\"ç¬¬{turn}è½® æç¤ºè€…: {hinter_result['error']}\")\n",
        "            \n",
        "            return {\n",
        "                'success': False,\n",
        "                'turns': turn,\n",
        "                'conversation': conversation_history,\n",
        "                'final_guess': f\"HINTER_{error_type}\",\n",
        "                'error': f\"{error_type}: {hinter_result['error']}\",\n",
        "                'failure_reason': error_type,\n",
        "                'total_hinter_attempts': total_hinter_attempts,\n",
        "                'total_guesser_attempts': total_guesser_attempts,\n",
        "                'format_errors': format_errors,\n",
        "                'hinter_failed_outputs': hinter_failed_outputs,\n",
        "                'guesser_failed_outputs': guesser_failed_outputs,\n",
        "                'all_hints': [msg for msg in conversation_history if msg.startswith('æç¤ºè€…:')],\n",
        "                'all_guesses': [msg for msg in conversation_history if msg.startswith('çŒœæµ‹è€…:')]\n",
        "            }\n",
        "        \n",
        "        # æå–çº¿ç´¢å¹¶æ£€æŸ¥taboo violation\n",
        "        hint_text = extract_chinese_clue_text(hinter_result['response'])\n",
        "        \n",
        "        # æ£€æŸ¥æ˜¯å¦è¿åç¦ç”¨è¯è§„åˆ™\n",
        "        taboo_violated = check_chinese_taboo_violation(hint_text, taboo_words)\n",
        "        if taboo_violated:\n",
        "            return {\n",
        "                'success': False,\n",
        "                'turns': turn,\n",
        "                'conversation': conversation_history,\n",
        "                'final_guess': 'è¿åç¦ç”¨è¯è§„åˆ™: æç¤ºè€…è¿è§„',\n",
        "                'error': f'è¿åç¦ç”¨è¯è§„åˆ™: æç¤ºè€…åœ¨ç¬¬{turn}è½®è¿åè§„åˆ™ï¼Œä½¿ç”¨äº†ç¦ç”¨è¯: {hint_text}',\n",
        "                'failure_reason': 'TABOO_VIOLATION',\n",
        "                'taboo_violation_turn': turn,\n",
        "                'taboo_violation_hint': hint_text,\n",
        "                'total_hinter_attempts': total_hinter_attempts,\n",
        "                'total_guesser_attempts': total_guesser_attempts,\n",
        "                'format_errors': format_errors,\n",
        "                'hinter_failed_outputs': hinter_failed_outputs,\n",
        "                'guesser_failed_outputs': guesser_failed_outputs,\n",
        "                'all_hints': [msg for msg in conversation_history if msg.startswith('æç¤ºè€…:')],\n",
        "                'all_guesses': [msg for msg in conversation_history if msg.startswith('çŒœæµ‹è€…:')]\n",
        "            }\n",
        "        \n",
        "        conversation_history.append(f\"æç¤ºè€…: {hinter_result['response']}\")\n",
        "        \n",
        "        # æ„å»ºGuesseræç¤º\n",
        "        history_text = \"\\n\".join([f\"ç¬¬{i}è½®: {msg}\" for i, msg in enumerate(conversation_history, 1)])\n",
        "        guesser_prompt = f\"{guesser_system}\\n\\nå¯¹è¯å†å²:\\n{history_text}\\n\\næ‚¨çš„çŒœæµ‹æ˜¯ä»€ä¹ˆ?\"\n",
        "        \n",
        "        # Guesserè¿›è¡ŒçŒœæµ‹\n",
        "        guesser_result = robust_chinese_api_call(client, guesser_model, guesser_prompt, \"[çŒœæµ‹]\", max_retries=3)\n",
        "        total_guesser_attempts += guesser_result['attempts']\n",
        "        \n",
        "        if guesser_result.get('failed_outputs'):\n",
        "            guesser_failed_outputs.extend(guesser_result['failed_outputs'])\n",
        "        \n",
        "        if not guesser_result['success']:\n",
        "            error_type = \"FORMAT_FAILURE\" if \"FORMAT_ERROR_EXCEEDED\" in str(guesser_result.get('response', '')) else \"API_FAILURE\"\n",
        "            format_errors.append(f\"ç¬¬{turn}è½® çŒœæµ‹è€…: {guesser_result['error']}\")\n",
        "            \n",
        "            return {\n",
        "                'success': False,\n",
        "                'turns': turn,\n",
        "                'conversation': conversation_history,\n",
        "                'final_guess': f\"GUESSER_{error_type}\",\n",
        "                'error': f\"{error_type}: {guesser_result['error']}\",\n",
        "                'failure_reason': error_type,\n",
        "                'total_hinter_attempts': total_hinter_attempts,\n",
        "                'total_guesser_attempts': total_guesser_attempts,\n",
        "                'format_errors': format_errors,\n",
        "                'hinter_failed_outputs': hinter_failed_outputs,\n",
        "                'guesser_failed_outputs': guesser_failed_outputs,\n",
        "                'all_hints': [msg for msg in conversation_history if msg.startswith('æç¤ºè€…:')],\n",
        "                'all_guesses': [msg for msg in conversation_history if msg.startswith('çŒœæµ‹è€…:')]\n",
        "            }\n",
        "        \n",
        "        conversation_history.append(f\"çŒœæµ‹è€…: {guesser_result['response']}\")\n",
        "        guess = extract_chinese_guess_word(guesser_result['response'])\n",
        "        \n",
        "        # æ£€æŸ¥æ˜¯å¦æˆåŠŸ\n",
        "        if guess == target_word:\n",
        "            return {\n",
        "                'success': True,\n",
        "                'turns': turn,\n",
        "                'conversation': conversation_history,\n",
        "                'final_guess': guess,\n",
        "                'failure_reason': None,\n",
        "                'total_hinter_attempts': total_hinter_attempts,\n",
        "                'total_guesser_attempts': total_guesser_attempts,\n",
        "                'format_errors': format_errors,\n",
        "                'hinter_failed_outputs': hinter_failed_outputs,\n",
        "                'guesser_failed_outputs': guesser_failed_outputs,\n",
        "                'all_hints': [msg for msg in conversation_history if msg.startswith('æç¤ºè€…:')],\n",
        "                'all_guesses': [msg for msg in conversation_history if msg.startswith('çŒœæµ‹è€…:')]\n",
        "            }\n",
        "        \n",
        "        # å¦‚æœä¸æ˜¯æœ€åä¸€è½®ï¼Œæ·»åŠ åé¦ˆ\n",
        "        if turn < max_turns:\n",
        "            conversation_history.append(f\"ç³»ç»Ÿ: '{guess}' ä¸æ­£ç¡®ã€‚è¯·ç»§ç»­ï¼\")\n",
        "    \n",
        "    # è¾¾åˆ°æœ€å¤§è½®æ•°ä»æœªæˆåŠŸ\n",
        "    return {\n",
        "        'success': False,\n",
        "        'turns': max_turns,\n",
        "        'conversation': conversation_history,\n",
        "        'final_guess': guess if 'guess' in locals() else 'N/A',\n",
        "        'failure_reason': 'MAX_TURNS_EXCEEDED',\n",
        "        'total_hinter_attempts': total_hinter_attempts,\n",
        "        'total_guesser_attempts': total_guesser_attempts,\n",
        "        'format_errors': format_errors,\n",
        "        'hinter_failed_outputs': hinter_failed_outputs,\n",
        "        'guesser_failed_outputs': guesser_failed_outputs,\n",
        "        'all_hints': [msg for msg in conversation_history if msg.startswith('æç¤ºè€…:')],\n",
        "        'all_guesses': [msg for msg in conversation_history if msg.startswith('çŒœæµ‹è€…:')]\n",
        "    }\n",
        "\n",
        "print(\"âœ… ä¸­æ–‡Tabooæ¸¸æˆä¸»å‡½æ•°å·²å®šä¹‰\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 10. æ‰§è¡Œä¸­æ–‡Tabooæµ‹è¯•å®éªŒ\n",
        "print(\"ğŸ§ª å¼€å§‹æ‰§è¡Œä¸­æ–‡Tabooæµ‹è¯•å®éªŒ...\")\n",
        "\n",
        "def run_chinese_test_experiment(client, models, dataset, num_test_words=3):\n",
        "    \"\"\"è¿è¡Œä¸­æ–‡Tabooæµ‹è¯•å®éªŒ\"\"\"\n",
        "    \n",
        "    if not client:\n",
        "        print(\"âŒ APIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ‰§è¡Œå®éªŒ\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"\\nğŸ¯ æµ‹è¯•é…ç½®:\")\n",
        "    print(f\"   æµ‹è¯•è¯æ±‡æ•°: {num_test_words}\")\n",
        "    print(f\"   æ¨¡å‹æ•°é‡: {len(models)}\")\n",
        "    print(f\"   æ€»æ¸¸æˆæ•°: {num_test_words * len(models) * len(models)}\")\n",
        "    \n",
        "    # éšæœºé€‰æ‹©æµ‹è¯•è¯æ±‡\n",
        "    test_words = random.sample(dataset, min(num_test_words, len(dataset)))\n",
        "    print(f\"\\nğŸ“‹ é€‰æ‹©çš„æµ‹è¯•è¯æ±‡:\")\n",
        "    for i, word_data in enumerate(test_words, 1):\n",
        "        print(f\"   {i}. {word_data['target']} ({word_data['part_of_speech']}) - ç¦ç”¨è¯: {word_data['taboo']}\")\n",
        "    \n",
        "    all_results = []\n",
        "    total_games = len(test_words) * len(models) * len(models)\n",
        "    game_counter = 0\n",
        "    \n",
        "    print(f\"\\nğŸš€ å¼€å§‹æ‰§è¡Œå®éªŒ...\")\n",
        "    \n",
        "    for word_data in test_words:\n",
        "        target_word = word_data['target']\n",
        "        taboo_words = word_data['taboo']\n",
        "        \n",
        "        print(f\"\\nğŸ¯ æµ‹è¯•è¯æ±‡: {target_word}\")\n",
        "        print(f\"ğŸš« ç¦ç”¨è¯: {taboo_words}\")\n",
        "        \n",
        "        for hinter_model in models:\n",
        "            for guesser_model in models:\n",
        "                game_counter += 1\n",
        "                hinter_name = hinter_model.split('/')[-1]\n",
        "                guesser_name = guesser_model.split('/')[-1]\n",
        "                pair_name = f\"{hinter_name}â†’{guesser_name}\"\n",
        "                \n",
        "                print(f\"  ğŸ”„ æ¸¸æˆ {game_counter}/{total_games}: {pair_name}\")\n",
        "                \n",
        "                start_time = time.time()\n",
        "                \n",
        "                try:\n",
        "                    # æ‰§è¡Œæ¸¸æˆ\n",
        "                    game_result = play_chinese_taboo_game(\n",
        "                        client, hinter_model, guesser_model, \n",
        "                        target_word, taboo_words, max_turns=5\n",
        "                    )\n",
        "                    \n",
        "                    duration = round(time.time() - start_time, 2)\n",
        "                    \n",
        "                    # è®°å½•ç»“æœ\n",
        "                    result = {\n",
        "                        'game_id': game_counter,\n",
        "                        'target_word': target_word,\n",
        "                        'part_of_speech': word_data['part_of_speech'],\n",
        "                        'category': word_data['category'],\n",
        "                        'taboo_words': '|'.join(taboo_words),\n",
        "                        'hinter_model': hinter_model,\n",
        "                        'guesser_model': guesser_model,\n",
        "                        'success': game_result['success'],\n",
        "                        'turns_used': game_result['turns'],\n",
        "                        'final_guess': game_result['final_guess'],\n",
        "                        'failure_reason': game_result.get('failure_reason', None),\n",
        "                        'taboo_violation_turn': game_result.get('taboo_violation_turn', None),\n",
        "                        'taboo_violation_hint': game_result.get('taboo_violation_hint', None),\n",
        "                        'has_taboo_violation': game_result.get('failure_reason') == 'TABOO_VIOLATION',\n",
        "                        'all_hints': ' | '.join(game_result['all_hints']),\n",
        "                        'all_guesses': ' | '.join(game_result['all_guesses']),\n",
        "                        'conversation': ' | '.join(game_result['conversation']),\n",
        "                        'total_api_attempts': game_result.get('total_hinter_attempts', 0) + game_result.get('total_guesser_attempts', 0),\n",
        "                        'format_errors': ' | '.join(game_result.get('format_errors', [])),\n",
        "                        'has_format_errors': len(game_result.get('format_errors', [])) > 0,\n",
        "                        'duration_seconds': duration,\n",
        "                        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                        'language': 'chinese',\n",
        "                        'dataset_source': 'openhownet'\n",
        "                    }\n",
        "                    \n",
        "                    if 'error' in game_result:\n",
        "                        result['error'] = game_result['error']\n",
        "                    \n",
        "                    all_results.append(result)\n",
        "                    \n",
        "                    # æ˜¾ç¤ºç»“æœ\n",
        "                    status = \"âœ… æˆåŠŸ\" if game_result['success'] else \"âŒ å¤±è´¥\"\n",
        "                    failure_info = \"\"\n",
        "                    if not game_result['success'] and game_result.get('failure_reason'):\n",
        "                        failure_reason = game_result['failure_reason']\n",
        "                        if failure_reason == 'TABOO_VIOLATION':\n",
        "                            failure_info = \" (è¿åç¦ç”¨è¯)\"\n",
        "                        elif failure_reason == 'FORMAT_FAILURE':\n",
        "                            failure_info = \" (æ ¼å¼é”™è¯¯)\"\n",
        "                        elif failure_reason == 'API_FAILURE':\n",
        "                            failure_info = \" (APIå¤±è´¥)\"\n",
        "                        elif failure_reason == 'MAX_TURNS_EXCEEDED':\n",
        "                            failure_info = \" (è½®æ•°è€—å°½)\"\n",
        "                    \n",
        "                    print(f\"     {status}{failure_info} | {game_result['turns']}è½® | æœ€ç»ˆçŒœæµ‹: {game_result['final_guess']}\")\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"     âŒ æ¸¸æˆæ‰§è¡Œå¼‚å¸¸: {e}\")\n",
        "                    # è®°å½•å¼‚å¸¸ç»“æœ\n",
        "                    result = {\n",
        "                        'game_id': game_counter,\n",
        "                        'target_word': target_word,\n",
        "                        'hinter_model': hinter_model,\n",
        "                        'guesser_model': guesser_model,\n",
        "                        'success': False,\n",
        "                        'failure_reason': 'EXCEPTION',\n",
        "                        'error': str(e),\n",
        "                        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                        'language': 'chinese'\n",
        "                    }\n",
        "                    all_results.append(result)\n",
        "                \n",
        "                time.sleep(0.5)  # APIè°ƒç”¨é—´éš”\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "# æ‰§è¡Œæµ‹è¯•å®éªŒ\n",
        "if chinese_client:\n",
        "    test_results = run_chinese_test_experiment(\n",
        "        chinese_client, CHINESE_TEST_MODELS, chinese_dataset, num_test_words=3\n",
        "    )\n",
        "    \n",
        "    if test_results:\n",
        "        print(f\"\\nğŸ‰ ä¸­æ–‡Tabooæµ‹è¯•å®éªŒå®Œæˆï¼\")\n",
        "        print(f\"ğŸ“Š æ€»æ¸¸æˆæ•°: {len(test_results)}\")\n",
        "        \n",
        "        # ç»Ÿè®¡ç»“æœ\n",
        "        successful_games = [r for r in test_results if r['success']]\n",
        "        success_rate = len(successful_games) / len(test_results) * 100\n",
        "        print(f\"ğŸ“ˆ æˆåŠŸç‡: {len(successful_games)}/{len(test_results)} ({success_rate:.1f}%)\")\n",
        "        \n",
        "        # ä¿å­˜æµ‹è¯•ç»“æœ\n",
        "        test_results_path = f\"results/chinese_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "        os.makedirs(\"results\", exist_ok=True)\n",
        "        \n",
        "        df_results = pd.DataFrame(test_results)\n",
        "        df_results.to_csv(test_results_path, index=False, encoding='utf-8-sig')\n",
        "        print(f\"ğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜: {test_results_path}\")\n",
        "        \n",
        "        # æŒ‰æ¨¡å‹ç»Ÿè®¡\n",
        "        print(f\"\\nğŸ“Š å„æ¨¡å‹è¡¨ç°:\")\n",
        "        for model in CHINESE_TEST_MODELS:\n",
        "            model_name = model.split('/')[-1]\n",
        "            model_as_hinter = [r for r in test_results if r['hinter_model'] == model]\n",
        "            model_as_guesser = [r for r in test_results if r['guesser_model'] == model]\n",
        "            \n",
        "            hinter_success = len([r for r in model_as_hinter if r['success']])\n",
        "            guesser_success = len([r for r in model_as_guesser if r['success']])\n",
        "            \n",
        "            print(f\"   {model_name}:\")\n",
        "            if len(model_as_hinter) > 0:\n",
        "                print(f\"     ä½œä¸ºæç¤ºè€…: {hinter_success}/{len(model_as_hinter)} ({hinter_success/len(model_as_hinter)*100:.1f}%)\")\n",
        "            if len(model_as_guesser) > 0:\n",
        "                print(f\"     ä½œä¸ºçŒœæµ‹è€…: {guesser_success}/{len(model_as_guesser)} ({guesser_success/len(model_as_guesser)*100:.1f}%)\")\n",
        "    else:\n",
        "        print(\"âŒ æµ‹è¯•å®éªŒå¤±è´¥\")\n",
        "else:\n",
        "    print(\"âŒ æ— æ³•æ‰§è¡Œæµ‹è¯•å®éªŒï¼šAPIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ä¸­æ–‡Tabooå®éªŒæ€»ç»“\n",
        "\n",
        "## âœ… å·²å®Œæˆçš„å·¥ä½œ\n",
        "\n",
        "1. **ç¯å¢ƒé…ç½®**: æˆåŠŸå®‰è£…å¹¶é…ç½®äº†OpenHowNetå’Œç›¸å…³ä¸­æ–‡å¤„ç†å·¥å…·\n",
        "2. **æ•°æ®é›†æ„å»º**: ä½¿ç”¨OpenHowNetæ„å»ºäº†100ä¸ªä¸­æ–‡è¯æ±‡çš„Tabooæ•°æ®é›†\n",
        "3. **è¯æ€§åˆ†å¸ƒ**: æŒ‰ç…§åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ã€å‰¯è¯å„25ä¸ªçš„ç›®æ ‡è¿›è¡Œåˆ†å¸ƒ\n",
        "4. **æ¸¸æˆé€»è¾‘**: å®ç°äº†å®Œæ•´çš„ä¸­æ–‡Tabooæ¸¸æˆé€»è¾‘å’Œè¯„ä¼°æ¡†æ¶\n",
        "5. **æ¨¡å‹æµ‹è¯•**: éªŒè¯äº†ä¸­æ–‡æ ¼å¼è¦æ±‚å’Œç¦ç”¨è¯æ£€æµ‹æœºåˆ¶\n",
        "6. **æ•°æ®ä¿å­˜**: ç”Ÿæˆäº†å®Œæ•´çš„æ•°æ®é›†æ–‡ä»¶å’Œå®éªŒæŠ¥å‘Š\n",
        "\n",
        "## ğŸ¯ æ•°æ®é›†ç‰¹ç‚¹\n",
        "\n",
        "- **åŸºäºOpenHowNet**: åˆ©ç”¨ä¸­æ–‡çŸ¥è¯†å›¾è°±çš„è¯­ä¹‰å…³ç³»\n",
        "- **ç¦ç”¨è¯ç”Ÿæˆ**: æ¯ä¸ªè¯æ±‡åŒ…å«5ä¸ªè¯­ä¹‰ç›¸å…³çš„ç¦ç”¨è¯\n",
        "- **è¯æ€§è¦†ç›–**: æ¶µç›–4ç§ä¸»è¦è¯æ€§ï¼Œå¹³è¡¡åˆ†å¸ƒ\n",
        "- **è¯­ä¹‰ä¸°å¯Œ**: åŒ…å«å®Œæ•´çš„ä¹‰é¡¹ä¿¡æ¯å’Œè¯­ä¹‰å®šä¹‰\n",
        "- **ä¸­æ–‡ä¼˜åŒ–**: ä¸“é—¨é’ˆå¯¹ä¸­æ–‡è¯­è¨€ç‰¹ç‚¹è¿›è¡Œä¼˜åŒ–\n",
        "\n",
        "## ğŸ¤– æŠ€æœ¯åˆ›æ–°\n",
        "\n",
        "- **é¦–æ¬¡åº”ç”¨**: å°†OpenHowNetç”¨äºTabooæ¸¸æˆæ•°æ®é›†æ„å»º\n",
        "- **ä¸­æ–‡é€‚é…**: å®ç°äº†ä¸­æ–‡ç‰¹å®šçš„æ ¼å¼æ£€æŸ¥å’Œè¿è§„æ£€æµ‹\n",
        "- **è¯„ä¼°æ¡†æ¶**: æä¾›äº†å®Œæ•´çš„ä¸­æ–‡LLMè¯„ä¼°ä½“ç³»\n",
        "- **å¤šæ¨¡å‹æ”¯æŒ**: æ”¯æŒGPT-4oã€Geminiã€DeepSeekç­‰å¤šç§æ¨¡å‹\n",
        "\n",
        "## ğŸ“ ç”Ÿæˆæ–‡ä»¶\n",
        "\n",
        "- `data/chinese_dataset.json` - å®Œæ•´æ•°æ®é›†\n",
        "- `data/chinese_dataset_simple.json` - ç®€åŒ–ç‰ˆæ•°æ®é›†  \n",
        "- `data/chinese_dataset_report.json` - æ•°æ®é›†æŠ¥å‘Š\n",
        "- `results/chinese_test_results_*.csv` - å®éªŒç»“æœ\n",
        "\n",
        "## ğŸ”® æ‰©å±•æ–¹å‘\n",
        "\n",
        "1. **è§„æ¨¡æ‰©å±•**: å¢åŠ æ•°æ®é›†åˆ°300-500ä¸ªè¯æ±‡\n",
        "2. **é¢†åŸŸæ‹“å±•**: æ·»åŠ ä¸“ä¸šé¢†åŸŸè¯æ±‡ï¼ˆåŒ»å­¦ã€æ³•å¾‹ã€ç§‘æŠ€ç­‰ï¼‰\n",
        "3. **æ¨¡å‹è¦†ç›–**: æµ‹è¯•æ›´å¤šä¸­æ–‡æ¨¡å‹ï¼ˆæ™ºè°±ã€ç™¾å·ã€æ–‡å¿ƒç­‰ï¼‰\n",
        "4. **å¯¹æ¯”ç ”ç©¶**: å®ç°ä¸­è‹±æ–‡Tabooæ¸¸æˆå¯¹æ¯”åˆ†æ\n",
        "5. **éš¾åº¦åˆ†çº§**: ç ”ç©¶ä¸åŒå¤æ‚åº¦è¯æ±‡å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“\n",
        "\n",
        "## ğŸ‰ å®éªŒæ„ä¹‰\n",
        "\n",
        "è¿™æ˜¯é¦–ä¸ªåŸºäºOpenHowNetçš„ä¸­æ–‡Tabooæ¸¸æˆå®éªŒç³»ç»Ÿï¼Œä¸ºä¸­æ–‡è¯­è¨€æ¨¡å‹çš„è¯­ä¹‰ç†è§£èƒ½åŠ›è¯„ä¼°æä¾›äº†æ–°çš„åŸºå‡†æµ‹è¯•å·¥å…·ã€‚\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
