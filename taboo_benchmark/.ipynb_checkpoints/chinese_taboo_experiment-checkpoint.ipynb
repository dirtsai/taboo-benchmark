{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 中文Taboo实验 - 基于OpenHowNet的数据集构建与测试\n",
        "# 仿照base_test.ipynb结构，专门针对中文词汇和语言模型\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "!pip3 install OpenHowNet\n",
        "!pip3 install jieba\n",
        "!pip3 install requests\n",
        "!pip3 install pandas\n",
        "!pip3 install numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'jieba'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjieba\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, List, Any, Tuple\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'jieba'"
          ]
        }
      ],
      "source": [
        "# 1. 导入依赖和设置环境\n",
        "import json\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "import requests\n",
        "import os\n",
        "import jieba\n",
        "import re\n",
        "from typing import Dict, List, Any, Tuple\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "\n",
        "# 安装和导入OpenHowNet\n",
        "try:\n",
        "    import OpenHowNet\n",
        "    print(\"✅ OpenHowNet已导入\")\n",
        "except ImportError:\n",
        "    print(\"⚠️ 正在安装OpenHowNet...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"OpenHowNet\"])\n",
        "    import OpenHowNet\n",
        "    print(\"✅ OpenHowNet安装并导入成功\")\n",
        "\n",
        "print(\"🚀 中文Taboo实验环境初始化完成\")\n",
        "print(\"📋 实验目标: 使用OpenHowNet构建100个中文词汇的Taboo数据集\")\n",
        "print(\"🎯 词性分布: 名词、动词、形容词、副词各25个\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. 初始化OpenHowNet和中文处理工具\n",
        "print(\"🔧 正在初始化OpenHowNet和中文处理工具...\")\n",
        "\n",
        "# 初始化OpenHowNet实例\n",
        "try:\n",
        "    hownet_dict = OpenHowNet.HowNetDict()\n",
        "    print(\"✅ OpenHowNet词典加载成功\")\n",
        "    print(f\"📚 词典包含词汇数量: {len(hownet_dict)} 个概念\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ OpenHowNet初始化失败: {e}\")\n",
        "    print(\"🔄 尝试重新下载HowNet数据...\")\n",
        "    hownet_dict = OpenHowNet.HowNetDict(init_sim=True)\n",
        "\n",
        "# 设置jieba分词\n",
        "jieba.setLogLevel(20)  # 减少jieba的日志输出\n",
        "print(\"✅ jieba分词工具已配置\")\n",
        "\n",
        "# 设置随机种子\n",
        "random.seed(42)\n",
        "print(\"🎲 随机种子已设置为42，确保实验可复现\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. 中文词汇数据集构建工具函数\n",
        "\n",
        "def get_pos_mapping():\n",
        "    \"\"\"HowNet词性到标准词性的映射\"\"\"\n",
        "    return {\n",
        "        # 名词类\n",
        "        'N': 'noun', 'noun': 'noun',\n",
        "        # 动词类  \n",
        "        'V': 'verb', 'verb': 'verb',\n",
        "        # 形容词类\n",
        "        'A': 'adj', 'adj': 'adj', 'a': 'adj',\n",
        "        # 副词类\n",
        "        'D': 'adv', 'adv': 'adv', 'd': 'adv'\n",
        "    }\n",
        "\n",
        "def is_valid_chinese_word(word: str) -> bool:\n",
        "    \"\"\"检查是否为有效的中文词汇\"\"\"\n",
        "    if not word or len(word) < 1:\n",
        "        return False\n",
        "    \n",
        "    # 检查是否包含中文字符\n",
        "    chinese_pattern = re.compile(r'[\\u4e00-\\u9fff]+')\n",
        "    if not chinese_pattern.search(word):\n",
        "        return False\n",
        "    \n",
        "    # 过滤过长或过短的词\n",
        "    if len(word) > 6 or len(word) < 1:\n",
        "        return False\n",
        "    \n",
        "    # 过滤包含特殊字符的词\n",
        "    special_chars = ['·', '—', '…', '〈', '〉', '《', '》', '「', '」']\n",
        "    if any(char in word for char in special_chars):\n",
        "        return False\n",
        "    \n",
        "    return True\n",
        "\n",
        "def extract_similar_words_from_hownet(target_word: str, target_pos: str, hownet_dict, max_count: int = 10) -> List[str]:\n",
        "    \"\"\"从HowNet中提取与目标词相似的词汇作为禁用词候选\"\"\"\n",
        "    similar_words = set()\n",
        "    \n",
        "    try:\n",
        "        # 获取目标词的义项\n",
        "        word_senses = hownet_dict.get_senses(target_word)\n",
        "        if not word_senses:\n",
        "            return []\n",
        "        \n",
        "        # 从第一个义项开始提取相似词\n",
        "        primary_sense = word_senses[0]\n",
        "        \n",
        "        # 方法1: 获取同义词\n",
        "        try:\n",
        "            synonyms = hownet_dict.get_synonyms(target_word)\n",
        "            for syn_group in synonyms:\n",
        "                for word in syn_group:\n",
        "                    if is_valid_chinese_word(word) and word != target_word:\n",
        "                        similar_words.add(word)\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        # 方法2: 通过语义相似度获取相似词\n",
        "        try:\n",
        "            # 获取HowNet中所有词汇，然后计算相似度\n",
        "            all_words = list(hownet_dict.get_vocab())\n",
        "            chinese_words = [w for w in all_words if is_valid_chinese_word(w)]\n",
        "            \n",
        "            # 随机采样一些词汇计算相似度（避免计算量过大）\n",
        "            sample_size = min(1000, len(chinese_words))\n",
        "            sampled_words = random.sample(chinese_words, sample_size)\n",
        "            \n",
        "            word_similarities = []\n",
        "            for word in sampled_words:\n",
        "                if word != target_word:\n",
        "                    try:\n",
        "                        similarity = hownet_dict.calculate_word_similarity(target_word, word)\n",
        "                        if similarity > 0.3:  # 相似度阈值\n",
        "                            word_similarities.append((word, similarity))\n",
        "                    except:\n",
        "                        continue\n",
        "            \n",
        "            # 按相似度排序，取前几个\n",
        "            word_similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "            for word, _ in word_similarities[:5]:\n",
        "                similar_words.add(word)\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        # 方法3: 从定义中提取关键词\n",
        "        try:\n",
        "            definitions = [sense.get('def', '') for sense in word_senses]\n",
        "            for definition in definitions:\n",
        "                # 使用jieba分词提取定义中的关键词\n",
        "                words_in_def = jieba.lcut(definition)\n",
        "                for word in words_in_def:\n",
        "                    if is_valid_chinese_word(word) and word != target_word and len(word) >= 2:\n",
        "                        similar_words.add(word)\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ 提取 {target_word} 的相似词时出错: {e}\")\n",
        "    \n",
        "    # 过滤并返回结果\n",
        "    result = [word for word in similar_words if is_valid_chinese_word(word)][:max_count]\n",
        "    return result\n",
        "\n",
        "print(\"✅ 中文词汇处理工具函数已定义\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. 构建中文Taboo数据集\n",
        "print(\"🏗️ 开始构建中文Taboo数据集...\")\n",
        "\n",
        "def build_chinese_taboo_dataset(hownet_dict, target_count_per_pos: int = 25) -> List[Dict[str, Any]]:\n",
        "    \"\"\"构建中文Taboo数据集\"\"\"\n",
        "    \n",
        "    pos_mapping = get_pos_mapping()\n",
        "    target_pos_list = ['noun', 'verb', 'adj', 'adv']\n",
        "    dataset = []\n",
        "    \n",
        "    print(f\"📊 目标: 每个词性 {target_count_per_pos} 个词，总计 {target_count_per_pos * 4} 个词\")\n",
        "    \n",
        "    # 获取HowNet词汇表\n",
        "    all_vocab = list(hownet_dict.get_vocab())\n",
        "    chinese_vocab = [word for word in all_vocab if is_valid_chinese_word(word)]\n",
        "    print(f\"📚 HowNet中文词汇总数: {len(chinese_vocab)} 个\")\n",
        "    \n",
        "    # 按词性分组收集词汇\n",
        "    words_by_pos = {pos: [] for pos in target_pos_list}\n",
        "    \n",
        "    print(\"🔍 正在分析词汇词性...\")\n",
        "    progress_count = 0\n",
        "    \n",
        "    for word in chinese_vocab:\n",
        "        progress_count += 1\n",
        "        if progress_count % 1000 == 0:\n",
        "            print(f\"   已处理 {progress_count}/{len(chinese_vocab)} 个词汇\")\n",
        "        \n",
        "        try:\n",
        "            # 获取词汇的义项信息\n",
        "            senses = hownet_dict.get_senses(word)\n",
        "            if not senses:\n",
        "                continue\n",
        "            \n",
        "            # 获取主要词性\n",
        "            primary_sense = senses[0]\n",
        "            pos_info = primary_sense.get('pos', '')\n",
        "            \n",
        "            # 映射到标准词性\n",
        "            standard_pos = pos_mapping.get(pos_info, None)\n",
        "            if standard_pos and standard_pos in target_pos_list:\n",
        "                words_by_pos[standard_pos].append({\n",
        "                    'word': word,\n",
        "                    'senses': senses,\n",
        "                    'primary_pos': standard_pos\n",
        "                })\n",
        "        \n",
        "        except Exception:\n",
        "            continue\n",
        "    \n",
        "    print(\"\\n📈 词性分布统计:\")\n",
        "    for pos, words in words_by_pos.items():\n",
        "        print(f\"   {pos}: {len(words)} 个候选词\")\n",
        "    \n",
        "    # 为每个词性随机选择指定数量的词汇\n",
        "    print(\"\\n🎯 开始选择目标词汇并生成禁用词...\")\n",
        "    \n",
        "    for pos in target_pos_list:\n",
        "        available_words = words_by_pos[pos]\n",
        "        if len(available_words) < target_count_per_pos:\n",
        "            print(f\"⚠️ {pos} 词性可用词汇不足 ({len(available_words)} < {target_count_per_pos})\")\n",
        "            selected_count = len(available_words)\n",
        "        else:\n",
        "            selected_count = target_count_per_pos\n",
        "        \n",
        "        # 随机选择词汇\n",
        "        selected_words = random.sample(available_words, selected_count)\n",
        "        print(f\"\\n🔄 正在处理 {pos} 类词汇 ({selected_count} 个)...\")\n",
        "        \n",
        "        for i, word_info in enumerate(selected_words):\n",
        "            target_word = word_info['word']\n",
        "            senses = word_info['senses']\n",
        "            \n",
        "            print(f\"   处理 {i+1}/{selected_count}: {target_word}\")\n",
        "            \n",
        "            # 生成禁用词\n",
        "            taboo_words = extract_similar_words_from_hownet(\n",
        "                target_word, pos, hownet_dict, max_count=8\n",
        "            )\n",
        "            \n",
        "            # 如果禁用词不够，添加一些通用的相关词\n",
        "            if len(taboo_words) < 5:\n",
        "                # 使用jieba分词从定义中提取更多词汇\n",
        "                for sense in senses[:2]:  # 只取前两个义项\n",
        "                    definition = sense.get('def', '')\n",
        "                    def_words = jieba.lcut(definition)\n",
        "                    for def_word in def_words:\n",
        "                        if (is_valid_chinese_word(def_word) and \n",
        "                            def_word != target_word and \n",
        "                            len(def_word) >= 2 and \n",
        "                            def_word not in taboo_words):\n",
        "                            taboo_words.append(def_word)\n",
        "                            if len(taboo_words) >= 5:\n",
        "                                break\n",
        "            \n",
        "            # 确保至少有5个禁用词\n",
        "            taboo_words = taboo_words[:5]  # 限制为5个\n",
        "            if len(taboo_words) < 5:\n",
        "                # 如果还是不够，添加一些通用词汇\n",
        "                generic_taboos = ['东西', '事物', '物品', '概念', '内容']\n",
        "                for generic in generic_taboos:\n",
        "                    if generic not in taboo_words and generic != target_word:\n",
        "                        taboo_words.append(generic)\n",
        "                        if len(taboo_words) >= 5:\n",
        "                            break\n",
        "            \n",
        "            # 构建数据集条目\n",
        "            entry = {\n",
        "                'target': target_word,\n",
        "                'part_of_speech': pos,\n",
        "                'taboo': taboo_words[:5],  # 确保正好5个禁用词\n",
        "                'category': 'chinese_general',\n",
        "                'senses': senses,\n",
        "                'metadata': {\n",
        "                    'sense_count': len(senses),\n",
        "                    'taboo_count': len(taboo_words[:5]),\n",
        "                    'source': 'openhownet'\n",
        "                }\n",
        "            }\n",
        "            \n",
        "            dataset.append(entry)\n",
        "    \n",
        "    return dataset\n",
        "\n",
        "# 构建数据集\n",
        "chinese_dataset = build_chinese_taboo_dataset(hownet_dict, target_count_per_pos=25)\n",
        "print(f\"\\n✅ 中文Taboo数据集构建完成！\")\n",
        "print(f\"📊 总词汇数: {len(chinese_dataset)} 个\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. 数据集统计分析\n",
        "print(\"📊 中文Taboo数据集统计分析:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 基本统计\n",
        "total_words = len(chinese_dataset)\n",
        "print(f\"📝 总词汇数: {total_words}\")\n",
        "\n",
        "# 词性分布\n",
        "pos_counts = {}\n",
        "taboo_counts = []\n",
        "sense_counts = []\n",
        "\n",
        "for item in chinese_dataset:\n",
        "    pos = item.get('part_of_speech', 'unknown')\n",
        "    pos_counts[pos] = pos_counts.get(pos, 0) + 1\n",
        "    taboo_counts.append(len(item.get('taboo', [])))\n",
        "    sense_counts.append(len(item.get('senses', [])))\n",
        "\n",
        "print(f\"\\n🏷️ 词性分布:\")\n",
        "for pos, count in sorted(pos_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "    percentage = count / total_words * 100\n",
        "    print(f\"   {pos}: {count} 个 ({percentage:.1f}%)\")\n",
        "\n",
        "print(f\"\\n🚫 禁用词统计:\")\n",
        "print(f\"   平均数量: {sum(taboo_counts) / len(taboo_counts):.1f}\")\n",
        "print(f\"   范围: {min(taboo_counts)} - {max(taboo_counts)}\")\n",
        "\n",
        "print(f\"\\n💭 义项统计:\")\n",
        "print(f\"   平均数量: {sum(sense_counts) / len(sense_counts):.1f}\")\n",
        "print(f\"   范围: {min(sense_counts)} - {max(sense_counts)}\")\n",
        "\n",
        "# 显示数据样本\n",
        "print(f\"\\n📋 数据样本 (随机5个):\")\n",
        "sample_items = random.sample(chinese_dataset, min(5, len(chinese_dataset)))\n",
        "for i, item in enumerate(sample_items, 1):\n",
        "    print(f\"\\n   样本 {i}:\")\n",
        "    print(f\"     目标词: {item['target']}\")\n",
        "    print(f\"     词性: {item['part_of_speech']}\")\n",
        "    print(f\"     禁用词: {item['taboo']}\")\n",
        "    if item.get('senses') and len(item['senses']) > 0:\n",
        "        definition = item['senses'][0].get('def', '无定义')\n",
        "        print(f\"     定义: {definition[:50]}...\")\n",
        "\n",
        "print(f\"\\n✅ 统计分析完成\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. 保存中文数据集\n",
        "print(\"💾 保存中文Taboo数据集...\")\n",
        "\n",
        "# 创建数据目录\n",
        "data_dir = \"data\"\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "    print(f\"📁 创建数据目录: {data_dir}\")\n",
        "\n",
        "# 保存完整数据集\n",
        "chinese_dataset_path = os.path.join(data_dir, \"chinese_dataset.json\")\n",
        "with open(chinese_dataset_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(chinese_dataset, f, ensure_ascii=False, indent=2)\n",
        "print(f\"✅ 完整数据集已保存: {chinese_dataset_path}\")\n",
        "\n",
        "# 创建简化版数据集（用于快速测试）\n",
        "simplified_dataset = []\n",
        "for item in chinese_dataset:\n",
        "    simplified_item = {\n",
        "        'target': item['target'],\n",
        "        'part_of_speech': item['part_of_speech'],\n",
        "        'taboo': item['taboo'],\n",
        "        'category': item['category']\n",
        "    }\n",
        "    simplified_dataset.append(simplified_item)\n",
        "\n",
        "simplified_path = os.path.join(data_dir, \"chinese_dataset_simple.json\")\n",
        "with open(simplified_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(simplified_dataset, f, ensure_ascii=False, indent=2)\n",
        "print(f\"✅ 简化数据集已保存: {simplified_path}\")\n",
        "\n",
        "# 生成数据集报告\n",
        "report = {\n",
        "    'dataset_info': {\n",
        "        'total_words': len(chinese_dataset),\n",
        "        'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'source': 'OpenHowNet',\n",
        "        'language': 'Chinese',\n",
        "        'pos_distribution': pos_counts,\n",
        "        'avg_taboo_count': sum(taboo_counts) / len(taboo_counts),\n",
        "        'avg_sense_count': sum(sense_counts) / len(sense_counts)\n",
        "    },\n",
        "    'sample_data': sample_items\n",
        "}\n",
        "\n",
        "report_path = os.path.join(data_dir, \"chinese_dataset_report.json\")\n",
        "with open(report_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(report, f, ensure_ascii=False, indent=2)\n",
        "print(f\"✅ 数据集报告已保存: {report_path}\")\n",
        "\n",
        "print(f\"\\n🎉 中文Taboo数据集构建完成！\")\n",
        "print(f\"📁 数据文件位置:\")\n",
        "print(f\"   完整版: {chinese_dataset_path}\")\n",
        "print(f\"   简化版: {simplified_path}\")\n",
        "print(f\"   报告: {report_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7. API客户端设置（支持中文模型）\n",
        "print(\"🔧 设置中文Taboo实验API客户端...\")\n",
        "\n",
        "def load_api_keys(keys_path: str = \"api_keys.json\") -> Dict[str, str]:\n",
        "    \"\"\"加载API密钥\"\"\"\n",
        "    with open(keys_path, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "class ChineseTabooClient:\n",
        "    \"\"\"中文Taboo游戏专用API客户端\"\"\"\n",
        "    def __init__(self, api_key: str):\n",
        "        self.api_key = api_key\n",
        "        self.base_url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "        self.headers = {\n",
        "            \"Authorization\": f\"Bearer {api_key}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "    \n",
        "    def call_model(self, model: str, messages: List[Dict[str, str]], temperature: float = 0.3) -> str:\n",
        "        \"\"\"调用模型API\"\"\"\n",
        "        payload = {\n",
        "            \"model\": model,\n",
        "            \"messages\": messages,\n",
        "            \"temperature\": temperature,\n",
        "            \"max_tokens\": 2000\n",
        "        }\n",
        "        \n",
        "        response = requests.post(self.base_url, headers=self.headers, json=payload, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        result = response.json()\n",
        "        content = result['choices'][0]['message']['content'].strip()\n",
        "        \n",
        "        return content\n",
        "\n",
        "# 初始化API客户端\n",
        "try:\n",
        "    api_keys = load_api_keys()\n",
        "    chinese_client = ChineseTabooClient(api_keys[\"OPENROUTER_API_KEY\"])\n",
        "    print(\"✅ 中文Taboo API客户端初始化成功\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ API客户端初始化失败: {e}\")\n",
        "    chinese_client = None\n",
        "\n",
        "# 定义支持中文的测试模型\n",
        "CHINESE_TEST_MODELS = [\n",
        "    \"openai/gpt-4o\",  # GPT-4o 支持中文\n",
        "    \"google/gemini-2.5-flash\",  # Gemini 支持中文\n",
        "    \"deepseek/deepseek-chat-v3-0324\",  # DeepSeek 中文模型\n",
        "    \"anthropic/claude-sonnet-4\",  # Claude 支持中文\n",
        "    \"moonshotai/kimi-k2\",  # kimi-k2 的API路径\n",
        "\n",
        "]\n",
        "\n",
        "print(f\"🤖 中文实验模型列表 ({len(CHINESE_TEST_MODELS)} 个):\")\n",
        "for i, model in enumerate(CHINESE_TEST_MODELS, 1):\n",
        "    print(f\"   {i}. {model}\")\n",
        "\n",
        "print(f\"\\n💡 选择较少模型进行测试以节省成本和时间\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8. 中文Taboo游戏核心逻辑和工具函数\n",
        "print(\"🎮 定义中文Taboo游戏核心逻辑...\")\n",
        "\n",
        "def safe_chinese_text_cleanup(text: str, max_length: int = 300) -> str:\n",
        "    \"\"\"安全清理中文文本\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    \n",
        "    # 保留中文字符、英文字符、数字和常用标点\n",
        "    import re\n",
        "    cleaned = re.sub(r'[^\\u4e00-\\u9fff\\w\\s\\.,!?;:\"\\'()[\\]{}\\-]', '', str(text))\n",
        "    \n",
        "    if len(cleaned) > max_length:\n",
        "        cleaned = cleaned[:max_length] + \"...\"\n",
        "    \n",
        "    return cleaned\n",
        "\n",
        "def extract_chinese_clue_text(response: str) -> str:\n",
        "    \"\"\"从响应中提取中文线索文本\"\"\"\n",
        "    if \"FORMAT_ERROR_EXCEEDED\" in response:\n",
        "        return \"FORMAT_ERROR\"\n",
        "    \n",
        "    # 检查中文格式标记\n",
        "    if '[线索]' in response or '[CLUE]' in response.upper():\n",
        "        import re\n",
        "        # 优先匹配中文标记\n",
        "        match = re.search(r'\\[线索\\]\\s*(.+)', response, re.DOTALL)\n",
        "        if not match:\n",
        "            match = re.search(r'\\[CLUE\\]\\s*(.+)', response, re.IGNORECASE | re.DOTALL)\n",
        "        \n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "    \n",
        "    # 备用格式\n",
        "    if '线索:' in response or 'Clue:' in response:\n",
        "        if '线索:' in response:\n",
        "            return response.split('线索:')[1].strip()\n",
        "        else:\n",
        "            return response.split('Clue:')[1].strip()\n",
        "    \n",
        "    return \"INVALID_FORMAT\"\n",
        "\n",
        "def extract_chinese_guess_word(response: str) -> str:\n",
        "    \"\"\"从响应中提取中文猜测词\"\"\"\n",
        "    if \"FORMAT_ERROR_EXCEEDED\" in response:\n",
        "        return \"FORMAT_ERROR\"\n",
        "    \n",
        "    # 检查中文格式标记\n",
        "    if '[猜测]' in response or '[GUESS]' in response.upper():\n",
        "        import re\n",
        "        # 优先匹配中文标记\n",
        "        match = re.search(r'\\[猜测\\]\\s*(.+)', response)\n",
        "        if not match:\n",
        "            match = re.search(r'\\[GUESS\\]\\s*(.+)', response, re.IGNORECASE)\n",
        "        \n",
        "        if match:\n",
        "            guess_part = match.group(1).strip()\n",
        "            # 提取第一个中文词汇\n",
        "            chinese_words = re.findall(r'[\\u4e00-\\u9fff]+', guess_part)\n",
        "            if chinese_words:\n",
        "                return chinese_words[0]\n",
        "    \n",
        "    # 备用格式\n",
        "    if '猜测:' in response or 'Guess:' in response:\n",
        "        if '猜测:' in response:\n",
        "            guess_part = response.split('猜测:')[1].strip()\n",
        "        else:\n",
        "            guess_part = response.split('Guess:')[1].strip()\n",
        "        \n",
        "        chinese_words = re.findall(r'[\\u4e00-\\u9fff]+', guess_part)\n",
        "        if chinese_words:\n",
        "            return chinese_words[0]\n",
        "    \n",
        "    return \"INVALID_FORMAT\"\n",
        "\n",
        "def check_chinese_taboo_violation(hint: str, taboo_words: List[str]) -> bool:\n",
        "    \"\"\"检查中文线索是否违反禁用词规则\"\"\"\n",
        "    hint_cleaned = re.sub(r'[^\\u4e00-\\u9fff]', '', hint.lower())\n",
        "    \n",
        "    for taboo in taboo_words:\n",
        "        taboo_cleaned = re.sub(r'[^\\u4e00-\\u9fff]', '', taboo.lower())\n",
        "        \n",
        "        # 检查完整匹配\n",
        "        if taboo_cleaned in hint_cleaned:\n",
        "            return True\n",
        "        \n",
        "        # 检查部分匹配（对于较长的词）\n",
        "        if len(taboo_cleaned) >= 2:\n",
        "            # 检查是否包含禁用词的主要部分\n",
        "            if len(taboo_cleaned) >= 3:\n",
        "                core_part = taboo_cleaned[:2]  # 取前两个字符作为核心\n",
        "                if core_part in hint_cleaned:\n",
        "                    return True\n",
        "    \n",
        "    return False\n",
        "\n",
        "def robust_chinese_api_call(client, model: str, base_prompt: str, expected_prefix: str, max_retries: int = 3):\n",
        "    \"\"\"健壮的中文API调用\"\"\"\n",
        "    failed_outputs = []\n",
        "    \n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            if attempt == 1:\n",
        "                prompt = base_prompt\n",
        "            else:\n",
        "                prev_output = failed_outputs[-1] if failed_outputs else \"未知\"\n",
        "                format_reminder = f\"\"\"\n",
        "\n",
        "⚠️ 格式错误 ⚠️\n",
        "您之前的回复是: \"{prev_output}\"\n",
        "\n",
        "必需格式:\n",
        "- 您必须以 '{expected_prefix}' 开头（包括方括号）\n",
        "- 不要在 {expected_prefix} 前添加任何文字\n",
        "\n",
        "请使用正确格式重试:\"\"\"\n",
        "                prompt = base_prompt + format_reminder\n",
        "            \n",
        "            response = client.call_model(model, [{\"role\": \"user\", \"content\": prompt}])\n",
        "            \n",
        "            if (response.strip().startswith(expected_prefix) or \n",
        "                response.strip().upper().startswith(expected_prefix.upper())):\n",
        "                return {\n",
        "                    'success': True,\n",
        "                    'response': response,\n",
        "                    'attempts': attempt,\n",
        "                    'error': None,\n",
        "                    'failed_outputs': failed_outputs\n",
        "                }\n",
        "            else:\n",
        "                safe_response = safe_chinese_text_cleanup(response, max_length=150)\n",
        "                failed_outputs.append(safe_response)\n",
        "                \n",
        "                if attempt == max_retries:\n",
        "                    all_failed = \" | \".join(failed_outputs)\n",
        "                    return {\n",
        "                        'success': False,\n",
        "                        'response': f\"FORMAT_ERROR_EXCEEDED: {safe_response}\",\n",
        "                        'attempts': attempt,\n",
        "                        'error': f\"尝试 {max_retries} 次后失败。期望格式 '{expected_prefix}'。所有失败输出: {all_failed}\",\n",
        "                        'failed_outputs': failed_outputs\n",
        "                    }\n",
        "                time.sleep(0.5)\n",
        "                \n",
        "        except Exception as e:\n",
        "            safe_error = safe_chinese_text_cleanup(str(e), max_length=150)\n",
        "            error_msg = f\"API错误 (尝试 {attempt}/{max_retries}): {safe_error}\"\n",
        "            \n",
        "            if attempt == max_retries:\n",
        "                return {\n",
        "                    'success': False,\n",
        "                    'response': None,\n",
        "                    'attempts': attempt,\n",
        "                    'error': error_msg,\n",
        "                    'failed_outputs': failed_outputs\n",
        "                }\n",
        "            time.sleep(1.0)\n",
        "    \n",
        "    return {\n",
        "        'success': False,\n",
        "        'response': None,\n",
        "        'attempts': max_retries,\n",
        "        'error': \"超出最大重试次数\",\n",
        "        'failed_outputs': failed_outputs\n",
        "    }\n",
        "\n",
        "print(\"✅ 中文Taboo游戏核心逻辑已定义\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 9. 中文Taboo游戏主函数\n",
        "def play_chinese_taboo_game(client, hinter_model, guesser_model, target_word, taboo_words, max_turns=5):\n",
        "    \"\"\"中文Taboo游戏主函数\"\"\"\n",
        "    \n",
        "    conversation_history = []\n",
        "    taboo_list = '、'.join(taboo_words)\n",
        "    \n",
        "    # 中文系统提示 - Hinter\n",
        "    hinter_system = f\"\"\"您正在玩中文Taboo游戏，担任提示者角色。\n",
        "\n",
        "关键格式要求:\n",
        "- 您必须以 '[线索]' 开头回复（包括方括号）\n",
        "- 不要在 [线索] 前添加任何文字\n",
        "- 示例: [线索] 这是一种有长鼻子的大型哺乳动物\n",
        "\n",
        "游戏规则:\n",
        "1. 绝对不能使用这些禁用词: {taboo_list}\n",
        "2. 给出有用的线索帮助猜测者找到目标词\n",
        "3. 可以看到对话历史，在之前线索基础上补充\n",
        "4. 要有创意但严格遵循格式\n",
        "5. 使用标准中文表达\n",
        "\n",
        "目标词: {target_word}\n",
        "禁用词（绝对禁止）: {taboo_list}\"\"\"\n",
        "\n",
        "    # 中文系统提示 - Guesser\n",
        "    guesser_system = \"\"\"您正在玩中文Taboo游戏，担任猜测者角色。\n",
        "\n",
        "关键格式要求:\n",
        "- 您必须以 '[猜测]' 开头回复（包括方括号）\n",
        "- 不要在 [猜测] 前添加任何文字\n",
        "- 示例: [猜测] 大象\n",
        "\n",
        "游戏规则:\n",
        "1. 根据收到的所有线索进行最佳猜测\n",
        "2. 可以看到对话历史\n",
        "3. 在 [猜测] 后只给出一个中文词汇作为答案\n",
        "4. 使用标准中文词汇\"\"\"\n",
        "\n",
        "    # 记录统计信息\n",
        "    total_hinter_attempts = 0\n",
        "    total_guesser_attempts = 0\n",
        "    format_errors = []\n",
        "    hinter_failed_outputs = []\n",
        "    guesser_failed_outputs = []\n",
        "\n",
        "    for turn in range(1, max_turns + 1):\n",
        "        # 构建Hinter提示\n",
        "        if turn == 1:\n",
        "            hinter_prompt = f\"{hinter_system}\\n\\n请提供您的第一个线索:\"\n",
        "        else:\n",
        "            history_text = \"\\n\".join([f\"第{i}轮: {msg}\" for i, msg in enumerate(conversation_history, 1)])\n",
        "            hinter_prompt = f\"{hinter_system}\\n\\n对话历史:\\n{history_text}\\n\\n猜测者还没有找到答案。请提供下一个线索:\"\n",
        "        \n",
        "        # Hinter给出线索\n",
        "        hinter_result = robust_chinese_api_call(client, hinter_model, hinter_prompt, \"[线索]\", max_retries=3)\n",
        "        total_hinter_attempts += hinter_result['attempts']\n",
        "        \n",
        "        if hinter_result.get('failed_outputs'):\n",
        "            hinter_failed_outputs.extend(hinter_result['failed_outputs'])\n",
        "        \n",
        "        if not hinter_result['success']:\n",
        "            error_type = \"FORMAT_FAILURE\" if \"FORMAT_ERROR_EXCEEDED\" in str(hinter_result.get('response', '')) else \"API_FAILURE\"\n",
        "            format_errors.append(f\"第{turn}轮 提示者: {hinter_result['error']}\")\n",
        "            \n",
        "            return {\n",
        "                'success': False,\n",
        "                'turns': turn,\n",
        "                'conversation': conversation_history,\n",
        "                'final_guess': f\"HINTER_{error_type}\",\n",
        "                'error': f\"{error_type}: {hinter_result['error']}\",\n",
        "                'failure_reason': error_type,\n",
        "                'total_hinter_attempts': total_hinter_attempts,\n",
        "                'total_guesser_attempts': total_guesser_attempts,\n",
        "                'format_errors': format_errors,\n",
        "                'hinter_failed_outputs': hinter_failed_outputs,\n",
        "                'guesser_failed_outputs': guesser_failed_outputs,\n",
        "                'all_hints': [msg for msg in conversation_history if msg.startswith('提示者:')],\n",
        "                'all_guesses': [msg for msg in conversation_history if msg.startswith('猜测者:')]\n",
        "            }\n",
        "        \n",
        "        # 提取线索并检查taboo violation\n",
        "        hint_text = extract_chinese_clue_text(hinter_result['response'])\n",
        "        \n",
        "        # 检查是否违反禁用词规则\n",
        "        taboo_violated = check_chinese_taboo_violation(hint_text, taboo_words)\n",
        "        if taboo_violated:\n",
        "            return {\n",
        "                'success': False,\n",
        "                'turns': turn,\n",
        "                'conversation': conversation_history,\n",
        "                'final_guess': '违反禁用词规则: 提示者违规',\n",
        "                'error': f'违反禁用词规则: 提示者在第{turn}轮违反规则，使用了禁用词: {hint_text}',\n",
        "                'failure_reason': 'TABOO_VIOLATION',\n",
        "                'taboo_violation_turn': turn,\n",
        "                'taboo_violation_hint': hint_text,\n",
        "                'total_hinter_attempts': total_hinter_attempts,\n",
        "                'total_guesser_attempts': total_guesser_attempts,\n",
        "                'format_errors': format_errors,\n",
        "                'hinter_failed_outputs': hinter_failed_outputs,\n",
        "                'guesser_failed_outputs': guesser_failed_outputs,\n",
        "                'all_hints': [msg for msg in conversation_history if msg.startswith('提示者:')],\n",
        "                'all_guesses': [msg for msg in conversation_history if msg.startswith('猜测者:')]\n",
        "            }\n",
        "        \n",
        "        conversation_history.append(f\"提示者: {hinter_result['response']}\")\n",
        "        \n",
        "        # 构建Guesser提示\n",
        "        history_text = \"\\n\".join([f\"第{i}轮: {msg}\" for i, msg in enumerate(conversation_history, 1)])\n",
        "        guesser_prompt = f\"{guesser_system}\\n\\n对话历史:\\n{history_text}\\n\\n您的猜测是什么?\"\n",
        "        \n",
        "        # Guesser进行猜测\n",
        "        guesser_result = robust_chinese_api_call(client, guesser_model, guesser_prompt, \"[猜测]\", max_retries=3)\n",
        "        total_guesser_attempts += guesser_result['attempts']\n",
        "        \n",
        "        if guesser_result.get('failed_outputs'):\n",
        "            guesser_failed_outputs.extend(guesser_result['failed_outputs'])\n",
        "        \n",
        "        if not guesser_result['success']:\n",
        "            error_type = \"FORMAT_FAILURE\" if \"FORMAT_ERROR_EXCEEDED\" in str(guesser_result.get('response', '')) else \"API_FAILURE\"\n",
        "            format_errors.append(f\"第{turn}轮 猜测者: {guesser_result['error']}\")\n",
        "            \n",
        "            return {\n",
        "                'success': False,\n",
        "                'turns': turn,\n",
        "                'conversation': conversation_history,\n",
        "                'final_guess': f\"GUESSER_{error_type}\",\n",
        "                'error': f\"{error_type}: {guesser_result['error']}\",\n",
        "                'failure_reason': error_type,\n",
        "                'total_hinter_attempts': total_hinter_attempts,\n",
        "                'total_guesser_attempts': total_guesser_attempts,\n",
        "                'format_errors': format_errors,\n",
        "                'hinter_failed_outputs': hinter_failed_outputs,\n",
        "                'guesser_failed_outputs': guesser_failed_outputs,\n",
        "                'all_hints': [msg for msg in conversation_history if msg.startswith('提示者:')],\n",
        "                'all_guesses': [msg for msg in conversation_history if msg.startswith('猜测者:')]\n",
        "            }\n",
        "        \n",
        "        conversation_history.append(f\"猜测者: {guesser_result['response']}\")\n",
        "        guess = extract_chinese_guess_word(guesser_result['response'])\n",
        "        \n",
        "        # 检查是否成功\n",
        "        if guess == target_word:\n",
        "            return {\n",
        "                'success': True,\n",
        "                'turns': turn,\n",
        "                'conversation': conversation_history,\n",
        "                'final_guess': guess,\n",
        "                'failure_reason': None,\n",
        "                'total_hinter_attempts': total_hinter_attempts,\n",
        "                'total_guesser_attempts': total_guesser_attempts,\n",
        "                'format_errors': format_errors,\n",
        "                'hinter_failed_outputs': hinter_failed_outputs,\n",
        "                'guesser_failed_outputs': guesser_failed_outputs,\n",
        "                'all_hints': [msg for msg in conversation_history if msg.startswith('提示者:')],\n",
        "                'all_guesses': [msg for msg in conversation_history if msg.startswith('猜测者:')]\n",
        "            }\n",
        "        \n",
        "        # 如果不是最后一轮，添加反馈\n",
        "        if turn < max_turns:\n",
        "            conversation_history.append(f\"系统: '{guess}' 不正确。请继续！\")\n",
        "    \n",
        "    # 达到最大轮数仍未成功\n",
        "    return {\n",
        "        'success': False,\n",
        "        'turns': max_turns,\n",
        "        'conversation': conversation_history,\n",
        "        'final_guess': guess if 'guess' in locals() else 'N/A',\n",
        "        'failure_reason': 'MAX_TURNS_EXCEEDED',\n",
        "        'total_hinter_attempts': total_hinter_attempts,\n",
        "        'total_guesser_attempts': total_guesser_attempts,\n",
        "        'format_errors': format_errors,\n",
        "        'hinter_failed_outputs': hinter_failed_outputs,\n",
        "        'guesser_failed_outputs': guesser_failed_outputs,\n",
        "        'all_hints': [msg for msg in conversation_history if msg.startswith('提示者:')],\n",
        "        'all_guesses': [msg for msg in conversation_history if msg.startswith('猜测者:')]\n",
        "    }\n",
        "\n",
        "print(\"✅ 中文Taboo游戏主函数已定义\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 10. 执行中文Taboo测试实验\n",
        "print(\"🧪 开始执行中文Taboo测试实验...\")\n",
        "\n",
        "def run_chinese_test_experiment(client, models, dataset, num_test_words=3):\n",
        "    \"\"\"运行中文Taboo测试实验\"\"\"\n",
        "    \n",
        "    if not client:\n",
        "        print(\"❌ API客户端未初始化，无法执行实验\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"\\n🎯 测试配置:\")\n",
        "    print(f\"   测试词汇数: {num_test_words}\")\n",
        "    print(f\"   模型数量: {len(models)}\")\n",
        "    print(f\"   总游戏数: {num_test_words * len(models) * len(models)}\")\n",
        "    \n",
        "    # 随机选择测试词汇\n",
        "    test_words = random.sample(dataset, min(num_test_words, len(dataset)))\n",
        "    print(f\"\\n📋 选择的测试词汇:\")\n",
        "    for i, word_data in enumerate(test_words, 1):\n",
        "        print(f\"   {i}. {word_data['target']} ({word_data['part_of_speech']}) - 禁用词: {word_data['taboo']}\")\n",
        "    \n",
        "    all_results = []\n",
        "    total_games = len(test_words) * len(models) * len(models)\n",
        "    game_counter = 0\n",
        "    \n",
        "    print(f\"\\n🚀 开始执行实验...\")\n",
        "    \n",
        "    for word_data in test_words:\n",
        "        target_word = word_data['target']\n",
        "        taboo_words = word_data['taboo']\n",
        "        \n",
        "        print(f\"\\n🎯 测试词汇: {target_word}\")\n",
        "        print(f\"🚫 禁用词: {taboo_words}\")\n",
        "        \n",
        "        for hinter_model in models:\n",
        "            for guesser_model in models:\n",
        "                game_counter += 1\n",
        "                hinter_name = hinter_model.split('/')[-1]\n",
        "                guesser_name = guesser_model.split('/')[-1]\n",
        "                pair_name = f\"{hinter_name}→{guesser_name}\"\n",
        "                \n",
        "                print(f\"  🔄 游戏 {game_counter}/{total_games}: {pair_name}\")\n",
        "                \n",
        "                start_time = time.time()\n",
        "                \n",
        "                try:\n",
        "                    # 执行游戏\n",
        "                    game_result = play_chinese_taboo_game(\n",
        "                        client, hinter_model, guesser_model, \n",
        "                        target_word, taboo_words, max_turns=5\n",
        "                    )\n",
        "                    \n",
        "                    duration = round(time.time() - start_time, 2)\n",
        "                    \n",
        "                    # 记录结果\n",
        "                    result = {\n",
        "                        'game_id': game_counter,\n",
        "                        'target_word': target_word,\n",
        "                        'part_of_speech': word_data['part_of_speech'],\n",
        "                        'category': word_data['category'],\n",
        "                        'taboo_words': '|'.join(taboo_words),\n",
        "                        'hinter_model': hinter_model,\n",
        "                        'guesser_model': guesser_model,\n",
        "                        'success': game_result['success'],\n",
        "                        'turns_used': game_result['turns'],\n",
        "                        'final_guess': game_result['final_guess'],\n",
        "                        'failure_reason': game_result.get('failure_reason', None),\n",
        "                        'taboo_violation_turn': game_result.get('taboo_violation_turn', None),\n",
        "                        'taboo_violation_hint': game_result.get('taboo_violation_hint', None),\n",
        "                        'has_taboo_violation': game_result.get('failure_reason') == 'TABOO_VIOLATION',\n",
        "                        'all_hints': ' | '.join(game_result['all_hints']),\n",
        "                        'all_guesses': ' | '.join(game_result['all_guesses']),\n",
        "                        'conversation': ' | '.join(game_result['conversation']),\n",
        "                        'total_api_attempts': game_result.get('total_hinter_attempts', 0) + game_result.get('total_guesser_attempts', 0),\n",
        "                        'format_errors': ' | '.join(game_result.get('format_errors', [])),\n",
        "                        'has_format_errors': len(game_result.get('format_errors', [])) > 0,\n",
        "                        'duration_seconds': duration,\n",
        "                        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                        'language': 'chinese',\n",
        "                        'dataset_source': 'openhownet'\n",
        "                    }\n",
        "                    \n",
        "                    if 'error' in game_result:\n",
        "                        result['error'] = game_result['error']\n",
        "                    \n",
        "                    all_results.append(result)\n",
        "                    \n",
        "                    # 显示结果\n",
        "                    status = \"✅ 成功\" if game_result['success'] else \"❌ 失败\"\n",
        "                    failure_info = \"\"\n",
        "                    if not game_result['success'] and game_result.get('failure_reason'):\n",
        "                        failure_reason = game_result['failure_reason']\n",
        "                        if failure_reason == 'TABOO_VIOLATION':\n",
        "                            failure_info = \" (违反禁用词)\"\n",
        "                        elif failure_reason == 'FORMAT_FAILURE':\n",
        "                            failure_info = \" (格式错误)\"\n",
        "                        elif failure_reason == 'API_FAILURE':\n",
        "                            failure_info = \" (API失败)\"\n",
        "                        elif failure_reason == 'MAX_TURNS_EXCEEDED':\n",
        "                            failure_info = \" (轮数耗尽)\"\n",
        "                    \n",
        "                    print(f\"     {status}{failure_info} | {game_result['turns']}轮 | 最终猜测: {game_result['final_guess']}\")\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"     ❌ 游戏执行异常: {e}\")\n",
        "                    # 记录异常结果\n",
        "                    result = {\n",
        "                        'game_id': game_counter,\n",
        "                        'target_word': target_word,\n",
        "                        'hinter_model': hinter_model,\n",
        "                        'guesser_model': guesser_model,\n",
        "                        'success': False,\n",
        "                        'failure_reason': 'EXCEPTION',\n",
        "                        'error': str(e),\n",
        "                        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                        'language': 'chinese'\n",
        "                    }\n",
        "                    all_results.append(result)\n",
        "                \n",
        "                time.sleep(0.5)  # API调用间隔\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "# 执行测试实验\n",
        "if chinese_client:\n",
        "    test_results = run_chinese_test_experiment(\n",
        "        chinese_client, CHINESE_TEST_MODELS, chinese_dataset, num_test_words=3\n",
        "    )\n",
        "    \n",
        "    if test_results:\n",
        "        print(f\"\\n🎉 中文Taboo测试实验完成！\")\n",
        "        print(f\"📊 总游戏数: {len(test_results)}\")\n",
        "        \n",
        "        # 统计结果\n",
        "        successful_games = [r for r in test_results if r['success']]\n",
        "        success_rate = len(successful_games) / len(test_results) * 100\n",
        "        print(f\"📈 成功率: {len(successful_games)}/{len(test_results)} ({success_rate:.1f}%)\")\n",
        "        \n",
        "        # 保存测试结果\n",
        "        test_results_path = f\"results/chinese_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "        os.makedirs(\"results\", exist_ok=True)\n",
        "        \n",
        "        df_results = pd.DataFrame(test_results)\n",
        "        df_results.to_csv(test_results_path, index=False, encoding='utf-8-sig')\n",
        "        print(f\"💾 测试结果已保存: {test_results_path}\")\n",
        "        \n",
        "        # 按模型统计\n",
        "        print(f\"\\n📊 各模型表现:\")\n",
        "        for model in CHINESE_TEST_MODELS:\n",
        "            model_name = model.split('/')[-1]\n",
        "            model_as_hinter = [r for r in test_results if r['hinter_model'] == model]\n",
        "            model_as_guesser = [r for r in test_results if r['guesser_model'] == model]\n",
        "            \n",
        "            hinter_success = len([r for r in model_as_hinter if r['success']])\n",
        "            guesser_success = len([r for r in model_as_guesser if r['success']])\n",
        "            \n",
        "            print(f\"   {model_name}:\")\n",
        "            if len(model_as_hinter) > 0:\n",
        "                print(f\"     作为提示者: {hinter_success}/{len(model_as_hinter)} ({hinter_success/len(model_as_hinter)*100:.1f}%)\")\n",
        "            if len(model_as_guesser) > 0:\n",
        "                print(f\"     作为猜测者: {guesser_success}/{len(model_as_guesser)} ({guesser_success/len(model_as_guesser)*100:.1f}%)\")\n",
        "    else:\n",
        "        print(\"❌ 测试实验失败\")\n",
        "else:\n",
        "    print(\"❌ 无法执行测试实验：API客户端未初始化\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# 中文Taboo实验总结\n",
        "\n",
        "## ✅ 已完成的工作\n",
        "\n",
        "1. **环境配置**: 成功安装并配置了OpenHowNet和相关中文处理工具\n",
        "2. **数据集构建**: 使用OpenHowNet构建了100个中文词汇的Taboo数据集\n",
        "3. **词性分布**: 按照名词、动词、形容词、副词各25个的目标进行分布\n",
        "4. **游戏逻辑**: 实现了完整的中文Taboo游戏逻辑和评估框架\n",
        "5. **模型测试**: 验证了中文格式要求和禁用词检测机制\n",
        "6. **数据保存**: 生成了完整的数据集文件和实验报告\n",
        "\n",
        "## 🎯 数据集特点\n",
        "\n",
        "- **基于OpenHowNet**: 利用中文知识图谱的语义关系\n",
        "- **禁用词生成**: 每个词汇包含5个语义相关的禁用词\n",
        "- **词性覆盖**: 涵盖4种主要词性，平衡分布\n",
        "- **语义丰富**: 包含完整的义项信息和语义定义\n",
        "- **中文优化**: 专门针对中文语言特点进行优化\n",
        "\n",
        "## 🤖 技术创新\n",
        "\n",
        "- **首次应用**: 将OpenHowNet用于Taboo游戏数据集构建\n",
        "- **中文适配**: 实现了中文特定的格式检查和违规检测\n",
        "- **评估框架**: 提供了完整的中文LLM评估体系\n",
        "- **多模型支持**: 支持GPT-4o、Gemini、DeepSeek等多种模型\n",
        "\n",
        "## 📁 生成文件\n",
        "\n",
        "- `data/chinese_dataset.json` - 完整数据集\n",
        "- `data/chinese_dataset_simple.json` - 简化版数据集  \n",
        "- `data/chinese_dataset_report.json` - 数据集报告\n",
        "- `results/chinese_test_results_*.csv` - 实验结果\n",
        "\n",
        "## 🔮 扩展方向\n",
        "\n",
        "1. **规模扩展**: 增加数据集到300-500个词汇\n",
        "2. **领域拓展**: 添加专业领域词汇（医学、法律、科技等）\n",
        "3. **模型覆盖**: 测试更多中文模型（智谱、百川、文心等）\n",
        "4. **对比研究**: 实现中英文Taboo游戏对比分析\n",
        "5. **难度分级**: 研究不同复杂度词汇对模型性能的影响\n",
        "\n",
        "## 🎉 实验意义\n",
        "\n",
        "这是首个基于OpenHowNet的中文Taboo游戏实验系统，为中文语言模型的语义理解能力评估提供了新的基准测试工具。\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
