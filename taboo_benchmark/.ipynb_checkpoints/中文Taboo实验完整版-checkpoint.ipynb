{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ä¸­æ–‡Tabooå®éªŒå®Œæ•´ç‰ˆ\n",
        "\n",
        "> åŸºäºbase_test.ipynbæ¡†æ¶ï¼Œä½¿ç”¨OpenHowNetæ„å»ºä¸­æ–‡æ•°æ®é›†çš„å®Œæ•´å®éªŒç³»ç»Ÿ  \n",
        "> **å‚è€ƒæ•°æ®**ï¼šé¡¹ç›®æ•°æ®é›†å·²åœ¨dataæ–‡ä»¶å¤¹ä¸­é¢„ç”Ÿæˆ [[memory:2991732]]\n",
        "\n",
        "## ğŸ¯ å®éªŒç›®æ ‡\n",
        "\n",
        "1. **æ„å»ºé«˜è´¨é‡ä¸­æ–‡Tabooæ•°æ®é›†**ï¼šä½¿ç”¨OpenHowNetæ„å»º100ä¸ªä¸­æ–‡è¯æ±‡çš„è¯­ä¹‰æ•°æ®é›†\n",
        "2. **è¯„ä¼°ä¸­æ–‡LLMè¡¨ç°**ï¼šæµ‹è¯•å¤šä¸ªè¯­è¨€æ¨¡å‹åœ¨ä¸­æ–‡çº¦æŸæ¡ä»¶ä¸‹çš„æ²Ÿé€šèƒ½åŠ›  \n",
        "3. **å»ºç«‹ä¸­æ–‡è¯„ä¼°åŸºå‡†**ï¼šä¸ºä¸­æ–‡LLMèƒ½åŠ›è¯„ä¼°æä¾›æ ‡å‡†åŒ–å·¥å…·\n",
        "\n",
        "## ğŸ—ï¸ å®éªŒæ¶æ„\n",
        "\n",
        "æ­¤å®éªŒå®Œå…¨å‚ç…§base_test.ipynbçš„8ä¸ªæ¨¡å—ç»“æ„ï¼š\n",
        "1. **ç¯å¢ƒé…ç½®** - å¯¼å…¥ä¾èµ–å’Œä¸­æ–‡ç‰¹åŒ–è®¾ç½®\n",
        "2. **æ•°æ®é›†åŠ è½½** - åŠ è½½é¢„ç”Ÿæˆçš„ä¸­æ–‡æ•°æ®é›†\n",
        "3. **ç»Ÿè®¡åˆ†æ** - åˆ†æä¸­æ–‡æ•°æ®é›†ç‰¹å¾\n",
        "4. **APIè®¾ç½®** - é…ç½®ä¸­æ–‡æ¨¡å‹å®¢æˆ·ç«¯\n",
        "5. **æ¸¸æˆé€»è¾‘** - å®ç°ä¸­æ–‡Tabooæ¸¸æˆè§„åˆ™\n",
        "6. **æµ‹è¯•å®éªŒ** - å°è§„æ¨¡éªŒè¯\n",
        "7. **å…¨é‡å®éªŒ** - å®Œæ•´å®éªŒæ‰§è¡Œ\n",
        "8. **ç»“æœåˆ†æ** - æ·±åº¦åˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆ\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. ç¯å¢ƒé…ç½® (Environment Setup)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.1 æ ¸å¿ƒä¾èµ–å¯¼å…¥\n",
        "import json\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "import requests\n",
        "import os\n",
        "import jieba\n",
        "import re\n",
        "import logging\n",
        "from typing import Dict, List, Any, Tuple\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "\n",
        "# 1.2 ä¸­æ–‡ç‰¹åŒ–ä¾èµ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
        "try:\n",
        "    import OpenHowNet\n",
        "    print(\"âœ… OpenHowNetå¯ç”¨ï¼ˆç”¨äºæ•°æ®é›†æ„å»ºï¼‰\")\n",
        "except ImportError:\n",
        "    print(\"â„¹ï¸ OpenHowNetä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨é¢„ç”Ÿæˆæ•°æ®é›†\")\n",
        "    OpenHowNet = None\n",
        "\n",
        "# 1.3 ç¯å¢ƒè®¾ç½®\n",
        "random.seed(42)  # ç¡®ä¿å¯å¤ç°\n",
        "jieba.setLogLevel(logging.INFO)  # å‡å°‘åˆ†è¯æ—¥å¿—è¾“å‡º\n",
        "\n",
        "print(\"ğŸš€ ä¸­æ–‡Tabooå®éªŒç¯å¢ƒåˆå§‹åŒ–å®Œæˆ\")\n",
        "print(\"ğŸ“‹ å®éªŒç›®æ ‡: åŸºäºé¢„ç”Ÿæˆæ•°æ®é›†è¿›è¡Œä¸­æ–‡LLM Tabooæ¸¸æˆè¯„ä¼°\")\n",
        "print(\"ğŸ¯ å‚è€ƒæ¡†æ¶: base_test.ipynbæ ‡å‡†å®éªŒæ¶æ„\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. æ•°æ®é›†åŠ è½½ (Dataset Loading)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.1 åŠ è½½ä¸­æ–‡æ•°æ®é›†å‡½æ•°\n",
        "def load_chinese_dataset(dataset_path: str = \"data/chinese_dataset.json\") -> List[Dict]:\n",
        "    \"\"\"åŠ è½½ä¸­æ–‡Tabooæ•°æ®é›†\"\"\"\n",
        "    try:\n",
        "        with open(dataset_path, 'r', encoding='utf-8') as f:\n",
        "            return json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âŒ æ•°æ®é›†æ–‡ä»¶æœªæ‰¾åˆ°: {dataset_path}\")\n",
        "        print(\"ğŸ’¡ è¯·ç¡®ä¿å·²è¿è¡Œæ•°æ®é›†æ„å»ºæ­¥éª¤æˆ–ä½¿ç”¨é¢„ç”Ÿæˆæ•°æ®é›†\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}\")\n",
        "        return []\n",
        "\n",
        "# 2.2 åŠ è½½ä¸­æ–‡æ•°æ®é›†\n",
        "print(\"ğŸ“š æ­£åœ¨åŠ è½½ä¸­æ–‡æ•°æ®é›†...\")\n",
        "chinese_dataset = load_chinese_dataset()\n",
        "\n",
        "if chinese_dataset:\n",
        "    print(f\"âœ… ä¸­æ–‡æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…±{len(chinese_dataset)}æ¡è®°å½•\")\n",
        "    \n",
        "    # æ˜¾ç¤ºæ•°æ®é›†æ ·æœ¬\n",
        "    print(\"\\nğŸ“‹ ä¸­æ–‡æ•°æ®é›†æ ·æœ¬:\")\n",
        "    if len(chinese_dataset) > 0:\n",
        "        sample = random.choice(chinese_dataset)\n",
        "        print(f\"   ç›®æ ‡è¯: {sample['target']}\")\n",
        "        print(f\"   è¯æ€§: {sample.get('part_of_speech', 'unknown')}\")\n",
        "        print(f\"   ç±»åˆ«: {sample.get('category', 'unknown')}\")\n",
        "        print(f\"   ç¦ç”¨è¯: {sample['taboo']}\")\n",
        "        print(f\"   è¯ä¹‰æ•°: {len(sample.get('senses', []))}\")\n",
        "else:\n",
        "    print(\"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. æ•°æ®é›†ç»Ÿè®¡åˆ†æ (Dataset Analysis)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.1 ä¸­æ–‡æ•°æ®é›†ç»Ÿè®¡åˆ†æ\n",
        "if chinese_dataset:\n",
        "    print(\"ğŸ“Š ä¸­æ–‡æ•°æ®é›†åŸºæœ¬ç»Ÿè®¡:\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    # è¯æ€§åˆ†å¸ƒç»Ÿè®¡\n",
        "    pos_counts = {}\n",
        "    taboo_counts = []\n",
        "    sense_counts = []\n",
        "    categories = {}\n",
        "    \n",
        "    for item in chinese_dataset:\n",
        "        # ç»Ÿè®¡è¯æ€§\n",
        "        pos = item.get('part_of_speech', 'unknown')\n",
        "        pos_counts[pos] = pos_counts.get(pos, 0) + 1\n",
        "        \n",
        "        # ç»Ÿè®¡ç±»åˆ«\n",
        "        category = item.get('category', 'unknown')\n",
        "        categories[category] = categories.get(category, 0) + 1\n",
        "        \n",
        "        # ç»Ÿè®¡ç¦ç”¨è¯æ•°é‡\n",
        "        taboo_counts.append(len(item.get('taboo', [])))\n",
        "        \n",
        "        # ç»Ÿè®¡è¯ä¹‰æ•°é‡\n",
        "        sense_counts.append(len(item.get('senses', [])))\n",
        "    \n",
        "    print(f\"\\nğŸ·ï¸ è¯æ€§åˆ†å¸ƒ:\")\n",
        "    sorted_pos = sorted(pos_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "    for pos, count in sorted_pos:\n",
        "        percentage = count / len(chinese_dataset) * 100\n",
        "        print(f\"   {pos}: {count} ä¸ª ({percentage:.1f}%)\")\n",
        "    \n",
        "    print(f\"\\nğŸ“‚ ç±»åˆ«åˆ†å¸ƒ:\")\n",
        "    sorted_categories = sorted(categories.items(), key=lambda x: x[1], reverse=True)\n",
        "    for category, count in sorted_categories:\n",
        "        percentage = count / len(chinese_dataset) * 100\n",
        "        print(f\"   {category}: {count} ä¸ª ({percentage:.1f}%)\")\n",
        "    \n",
        "    print(f\"\\nğŸš« ç¦ç”¨è¯ç»Ÿè®¡:\")\n",
        "    print(f\"   å¹³å‡æ•°é‡: {sum(taboo_counts) / len(taboo_counts):.1f}\")\n",
        "    print(f\"   èŒƒå›´: {min(taboo_counts)} - {max(taboo_counts)}\")\n",
        "    \n",
        "    print(f\"\\nğŸ’­ è¯ä¹‰ç»Ÿè®¡:\")\n",
        "    if sense_counts and max(sense_counts) > 0:\n",
        "        print(f\"   å¹³å‡æ•°é‡: {sum(sense_counts) / len(sense_counts):.1f}\")\n",
        "        print(f\"   èŒƒå›´: {min(sense_counts)} - {max(sense_counts)}\")\n",
        "    else:\n",
        "        print(\"   è¯ä¹‰ä¿¡æ¯: æœªåŒ…å«è¯¦ç»†è¯ä¹‰æ•°æ®\")\n",
        "    \n",
        "    print(f\"\\nâœ… ä¸­æ–‡æ•°æ®é›†ç»Ÿè®¡å®Œæˆï¼Œè´¨é‡è‰¯å¥½ï¼Œå¯ç”¨äºå®éªŒ\")\n",
        "    \n",
        "    # è®¾ç½®éšæœºç§å­ç”¨äºå®éªŒ\n",
        "    random.seed(42)\n",
        "    print(\"\\nğŸ² éšæœºç§å­å·²è®¾ç½®ä¸º 42ï¼Œç¡®ä¿å®éªŒå¯å¤ç°\")\n",
        "    \n",
        "else:\n",
        "    print(\"âŒ æ— æ³•è¿›è¡Œç»Ÿè®¡åˆ†æï¼šæ•°æ®é›†æœªæˆåŠŸåŠ è½½\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. APIå®¢æˆ·ç«¯è®¾ç½® (API Configuration)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.1 åŠ è½½APIå¯†é’¥\n",
        "def load_api_keys(keys_path: str = \"api_keys.json\") -> Dict[str, str]:\n",
        "    \"\"\"åŠ è½½APIå¯†é’¥\"\"\"\n",
        "    try:\n",
        "        with open(keys_path, 'r', encoding='utf-8') as f:\n",
        "            return json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âŒ APIå¯†é’¥æ–‡ä»¶æœªæ‰¾åˆ°: {keys_path}\")\n",
        "        return {}\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ APIå¯†é’¥åŠ è½½å¤±è´¥: {e}\")\n",
        "        return {}\n",
        "\n",
        "# 4.2 ä¸­æ–‡APIå®¢æˆ·ç«¯ç±»\n",
        "class ChineseAPIClient:\n",
        "    \"\"\"æ”¯æŒä¸­æ–‡æ¨¡å‹çš„APIå®¢æˆ·ç«¯\"\"\"\n",
        "    def __init__(self, api_key: str):\n",
        "        self.api_key = api_key\n",
        "        self.base_url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "        self.headers = {\n",
        "            \"Authorization\": f\"Bearer {api_key}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "    \n",
        "    def call_model(self, model: str, messages: List[Dict[str, str]], \n",
        "                   temperature: float = 0.3) -> str:\n",
        "        \"\"\"è°ƒç”¨æ¨¡å‹APIï¼Œä¿æŒä¸­æ–‡å­—ç¬¦\"\"\"\n",
        "        payload = {\n",
        "            \"model\": model,\n",
        "            \"messages\": messages,\n",
        "            \"temperature\": temperature,\n",
        "            \"max_tokens\": 2000\n",
        "        }\n",
        "        response = requests.post(self.base_url, headers=self.headers, \n",
        "                               json=payload, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        result = response.json()\n",
        "        content = result['choices'][0]['message']['content'].strip()\n",
        "        \n",
        "        # ä¿ç•™ä¸­æ–‡å­—ç¬¦ï¼Œåªè¿‡æ»¤æ§åˆ¶å­—ç¬¦\n",
        "        content = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', '', content)\n",
        "        return content\n",
        "\n",
        "# 4.3 åˆå§‹åŒ–APIå®¢æˆ·ç«¯\n",
        "try:\n",
        "    api_keys = load_api_keys()\n",
        "    if \"OPENROUTER_API_KEY\" in api_keys:\n",
        "        chinese_client = ChineseAPIClient(api_keys[\"OPENROUTER_API_KEY\"])\n",
        "        print(\"âœ… ä¸­æ–‡APIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ\")\n",
        "    else:\n",
        "        chinese_client = None\n",
        "        print(\"âŒ ç¼ºå°‘OPENROUTER_API_KEYï¼Œæ— æ³•åˆå§‹åŒ–APIå®¢æˆ·ç«¯\")\n",
        "except Exception as e:\n",
        "    chinese_client = None\n",
        "    print(f\"âŒ APIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}\")\n",
        "\n",
        "# 4.4 å®šä¹‰ä¸­æ–‡æµ‹è¯•æ¨¡å‹\n",
        "CHINESE_MODELS = [\n",
        "    \"openai/gpt-4o\",\n",
        "    \"google/gemini-2.5-pro\", \n",
        "    \"deepseek/deepseek-chat-v3-0324\",\n",
        "    \"anthropic/claude-sonnet-4\"\n",
        "]\n",
        "\n",
        "print(f\"\\nğŸ¤– ä¸­æ–‡å®éªŒæ¨¡å‹: {len(CHINESE_MODELS)} ä¸ª\")\n",
        "for i, model in enumerate(CHINESE_MODELS, 1):\n",
        "    model_name = model.split('/')[-1]\n",
        "    print(f\"   {i}. {model_name}\")\n",
        "\n",
        "if chinese_client:\n",
        "    print(f\"\\nğŸš€ APIå®¢æˆ·ç«¯å·²å°±ç»ªï¼Œå¯ä»¥å¼€å§‹ä¸­æ–‡Tabooå®éªŒ\")\n",
        "else:\n",
        "    print(f\"\\nâš ï¸ APIå®¢æˆ·ç«¯æœªå°±ç»ªï¼Œéœ€è¦é…ç½®APIå¯†é’¥\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. ä¸­æ–‡æ¸¸æˆé€»è¾‘ (Chinese Game Logic)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5.1 ä¸­æ–‡æ–‡æœ¬å®‰å…¨æ¸…ç†\n",
        "def safe_chinese_text_cleanup(text: str, max_length: int = 200) -> str:\n",
        "    \"\"\"å®‰å…¨æ¸…ç†ä¸­æ–‡æ–‡æœ¬ï¼Œä¿ç•™ä¸­æ–‡å­—ç¬¦\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    # ä¿ç•™ä¸­æ–‡å­—ç¬¦ã€è‹±æ–‡å­—ç¬¦ã€æ•°å­—ã€åŸºæœ¬æ ‡ç‚¹\n",
        "    cleaned = re.sub(r'[^\\u4e00-\\u9fff\\w\\s\\.\\,\\!\\?\\-\\[\\]ã€ã€‘]', '', str(text))\n",
        "    if len(cleaned) > max_length:\n",
        "        cleaned = cleaned[:max_length] + \"...\"\n",
        "    return cleaned\n",
        "\n",
        "# 5.2 å¥å£®çš„ä¸­æ–‡APIè°ƒç”¨\n",
        "def robust_chinese_api_call(client, model: str, base_prompt: str, expected_prefix: str, max_retries: int = 3):\n",
        "    \"\"\"æ”¯æŒä¸­æ–‡æ ¼å¼çš„å¥å£®APIè°ƒç”¨\"\"\"\n",
        "    failed_outputs = []\n",
        "    \n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            if attempt == 1:\n",
        "                prompt = base_prompt\n",
        "            else:\n",
        "                prev_output = failed_outputs[-1] if failed_outputs else \"æœªçŸ¥\"\n",
        "                format_reminder = f\"\"\"\n",
        "\n",
        "âš ï¸ æ ¼å¼é”™è¯¯ âš ï¸\n",
        "ä½ çš„ä¸Šä¸€æ¬¡å›ç­”æ˜¯: \"{prev_output}\"\n",
        "\n",
        "å¿…éœ€æ ¼å¼:\n",
        "- ä½ å¿…é¡»ä»¥ '{expected_prefix}' å¼€å¤´ï¼ˆåŒ…æ‹¬æ–¹æ‹¬å·ï¼‰\n",
        "- ä¸è¦åœ¨ {expected_prefix} ä¹‹å‰æ·»åŠ ä»»ä½•æ–‡å­—\n",
        "\n",
        "è¯·æŒ‰æ­£ç¡®æ ¼å¼é‡æ–°å›ç­”:\"\"\"\n",
        "                prompt = base_prompt + format_reminder\n",
        "            \n",
        "            response = client.call_model(model, [{\"role\": \"user\", \"content\": prompt}])\n",
        "            cleaned_response = safe_chinese_text_cleanup(response)\n",
        "            \n",
        "            if cleaned_response.strip().startswith(expected_prefix):\n",
        "                content = cleaned_response.strip()[len(expected_prefix):].strip()\n",
        "                return {\n",
        "                    'success': True,\n",
        "                    'content': content,\n",
        "                    'attempts': attempt,\n",
        "                    'raw_response': response\n",
        "                }\n",
        "            else:\n",
        "                failed_outputs.append(cleaned_response[:50])\n",
        "                if attempt == max_retries:\n",
        "                    return {\n",
        "                        'success': False,\n",
        "                        'content': f\"æ ¼å¼éªŒè¯å¤±è´¥: {cleaned_response[:100]}\",\n",
        "                        'attempts': attempt,\n",
        "                        'raw_response': response\n",
        "                    }\n",
        "                \n",
        "        except Exception as e:\n",
        "            if attempt == max_retries:\n",
        "                return {\n",
        "                    'success': False,\n",
        "                    'content': f\"APIè°ƒç”¨å¤±è´¥: {e}\",\n",
        "                    'attempts': attempt,\n",
        "                    'raw_response': \"\"\n",
        "                }\n",
        "    \n",
        "    return {\n",
        "        'success': False,\n",
        "        'content': \"é‡è¯•æ¬¡æ•°è¶…é™\",\n",
        "        'attempts': max_retries,\n",
        "        'raw_response': \"\"\n",
        "    }\n",
        "\n",
        "print(\"âœ… ä¸­æ–‡æ–‡æœ¬å¤„ç†å’ŒAPIè°ƒç”¨å‡½æ•°å·²å®šä¹‰\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5.3 ä¸­æ–‡ç¦ç”¨è¯æ£€æµ‹\n",
        "def check_chinese_taboo_violation(text: str, taboo_words: List[str]) -> bool:\n",
        "    \"\"\"æ£€æŸ¥ä¸­æ–‡æ–‡æœ¬æ˜¯å¦è¿åç¦ç”¨è¯è§„åˆ™\"\"\"\n",
        "    if not text or not taboo_words:\n",
        "        return False\n",
        "    \n",
        "    # ä½¿ç”¨jiebaåˆ†è¯\n",
        "    words = list(jieba.cut(text, cut_all=False))\n",
        "    text_words = set(words)\n",
        "    \n",
        "    # æ£€æŸ¥ç›´æ¥åŒ…å«å’Œåˆ†è¯ååŒ…å«\n",
        "    for taboo in taboo_words:\n",
        "        if taboo in text or taboo in text_words:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# 5.4 ä¸­æ–‡è¯æ±‡åŒ¹é…æ£€æŸ¥\n",
        "def check_chinese_word_match(guess: str, target: str) -> bool:\n",
        "    \"\"\"æ£€æŸ¥ä¸­æ–‡è¯æ±‡æ˜¯å¦åŒ¹é…\"\"\"\n",
        "    if not guess or not target:\n",
        "        return False\n",
        "    \n",
        "    # ç§»é™¤æ ¼å¼æ ‡è®°å’Œç©ºæ ¼\n",
        "    guess_clean = re.sub(r'[\\[\\]ã€ã€‘\\s]', '', guess).strip()\n",
        "    target_clean = re.sub(r'[\\[\\]ã€ã€‘\\s]', '', target).strip()\n",
        "    \n",
        "    # ç›´æ¥åŒ¹é…æˆ–åŒ…å«åŒ¹é…\n",
        "    return guess_clean == target_clean or target_clean in guess_clean\n",
        "\n",
        "# 5.5 åˆ›å»ºæ¸¸æˆç»“æœ\n",
        "def create_chinese_game_result(success: bool, turns: int, final_guess: str, \n",
        "                              failure_reason: str, conversation_history: List[Dict]) -> Dict:\n",
        "    \"\"\"åˆ›å»ºä¸­æ–‡æ¸¸æˆç»“æœè®°å½•\"\"\"\n",
        "    return {\n",
        "        'success': success,\n",
        "        'turns': turns,\n",
        "        'final_guess': final_guess,\n",
        "        'failure_reason': failure_reason,\n",
        "        'conversation_history': conversation_history\n",
        "    }\n",
        "\n",
        "print(\"âœ… ä¸­æ–‡æ¸¸æˆéªŒè¯å’Œç»“æœå‡½æ•°å·²å®šä¹‰\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5.6 æ ¸å¿ƒä¸­æ–‡Tabooæ¸¸æˆé€»è¾‘\n",
        "def chinese_taboo_game(client, hinter_model: str, guesser_model: str, \n",
        "                      target_word: str, taboo_words: List[str], max_turns: int = 5) -> Dict:\n",
        "    \"\"\"ä¸­æ–‡Tabooæ¸¸æˆæ ¸å¿ƒé€»è¾‘\"\"\"\n",
        "    \n",
        "    # ä¸­æ–‡æç¤ºæ¨¡æ¿\n",
        "    hinter_base_prompt = f\"\"\"ä½ æ­£åœ¨ç©ä¸­æ–‡Tabooæ¸¸æˆã€‚ä½ çš„ä»»åŠ¡æ˜¯è®©é˜Ÿå‹çŒœå‡ºç›®æ ‡è¯æ±‡ï¼Œä½†ä¸èƒ½ä½¿ç”¨ç¦ç”¨è¯ã€‚\n",
        "\n",
        "ç›®æ ‡è¯æ±‡: {target_word}\n",
        "ç¦ç”¨è¯æ±‡: {', '.join(taboo_words)}\n",
        "\n",
        "è§„åˆ™:\n",
        "1. ä½ éœ€è¦ç»™å‡ºçº¿ç´¢è®©é˜Ÿå‹çŒœå‡ºç›®æ ‡è¯æ±‡\n",
        "2. ä½ çš„çº¿ç´¢ä¸­ä¸èƒ½åŒ…å«ä»»ä½•ç¦ç”¨è¯æ±‡\n",
        "3. è¯·ç”¨æ ¼å¼ [çº¿ç´¢] å¼€å§‹ä½ çš„å›ç­”\n",
        "4. çº¿ç´¢è¦ç®€æ´æ˜äº†ï¼Œä¸è¶…è¿‡20ä¸ªå­—\n",
        "\n",
        "è¯·ç»™å‡ºä½ çš„çº¿ç´¢:\"\"\"\n",
        "\n",
        "    conversation_history = []\n",
        "    \n",
        "    for turn in range(1, max_turns + 1):\n",
        "        # Hinterç»™å‡ºçº¿ç´¢\n",
        "        hinter_response = robust_chinese_api_call(\n",
        "            client, hinter_model, hinter_base_prompt, \"[çº¿ç´¢]\"\n",
        "        )\n",
        "        \n",
        "        if not hinter_response['success']:\n",
        "            return create_chinese_game_result(\n",
        "                False, turn, None, f\"çº¿ç´¢ç”Ÿæˆå¤±è´¥: {hinter_response['content']}\", \n",
        "                conversation_history\n",
        "            )\n",
        "        \n",
        "        hint = hinter_response['content']\n",
        "        \n",
        "        # æ£€æŸ¥ç¦ç”¨è¯è¿è§„\n",
        "        if check_chinese_taboo_violation(hint, taboo_words):\n",
        "            violated_words = [word for word in taboo_words if word in hint]\n",
        "            return create_chinese_game_result(\n",
        "                False, turn, None, f\"è¿åç¦ç”¨è¯è§„åˆ™: {violated_words}\", \n",
        "                conversation_history\n",
        "            )\n",
        "        \n",
        "        # Guesserè¿›è¡ŒçŒœæµ‹\n",
        "        guesser_prompt = f\"\"\"ä½ æ­£åœ¨ç©ä¸­æ–‡Tabooæ¸¸æˆã€‚æ ¹æ®é˜Ÿå‹ç»™å‡ºçš„çº¿ç´¢ï¼ŒçŒœå‡ºç›®æ ‡è¯æ±‡ã€‚\n",
        "\n",
        "ç¦ç”¨è¯æ±‡: {', '.join(taboo_words)}\n",
        "\n",
        "è§„åˆ™:\n",
        "1. æ ¹æ®é˜Ÿå‹çš„çº¿ç´¢çŒœå‡ºç›®æ ‡è¯æ±‡\n",
        "2. è¯·ç”¨æ ¼å¼ [çŒœæµ‹] å¼€å§‹ä½ çš„å›ç­”\n",
        "3. åªè¯´å‡ºä½ è®¤ä¸ºçš„ç­”æ¡ˆï¼Œä¸è¦è§£é‡Š\n",
        "\n",
        "é˜Ÿå‹çš„çº¿ç´¢æ˜¯: {hint}\n",
        "\n",
        "ä½ çš„çŒœæµ‹æ˜¯:\"\"\"\n",
        "        \n",
        "        guesser_response = robust_chinese_api_call(\n",
        "            client, guesser_model, guesser_prompt, \"[çŒœæµ‹]\"\n",
        "        )\n",
        "        \n",
        "        if not guesser_response['success']:\n",
        "            return create_chinese_game_result(\n",
        "                False, turn, None, f\"çŒœæµ‹ç”Ÿæˆå¤±è´¥: {guesser_response['content']}\", \n",
        "                conversation_history\n",
        "            )\n",
        "        \n",
        "        guess = guesser_response['content']\n",
        "        \n",
        "        # è®°å½•å¯¹è¯\n",
        "        conversation_history.append({\n",
        "            'turn': turn,\n",
        "            'hint': hint,\n",
        "            'guess': guess,\n",
        "            'hinter_attempts': hinter_response['attempts'],\n",
        "            'guesser_attempts': guesser_response['attempts']\n",
        "        })\n",
        "        \n",
        "        # æ£€æŸ¥æ˜¯å¦çŒœä¸­\n",
        "        if check_chinese_word_match(guess, target_word):\n",
        "            return create_chinese_game_result(\n",
        "                True, turn, guess, \"æˆåŠŸ\", conversation_history\n",
        "            )\n",
        "        \n",
        "        # æ›´æ–°hinterçš„æç¤ºï¼ŒåŒ…å«ä¹‹å‰çš„å†å²\n",
        "        previous_hints = [conv['hint'] for conv in conversation_history]\n",
        "        previous_guesses = [conv['guess'] for conv in conversation_history]\n",
        "        \n",
        "        hinter_base_prompt = f\"\"\"ä½ æ­£åœ¨ç©ä¸­æ–‡Tabooæ¸¸æˆã€‚ä½ çš„ä»»åŠ¡æ˜¯è®©é˜Ÿå‹çŒœå‡ºç›®æ ‡è¯æ±‡ï¼Œä½†ä¸èƒ½ä½¿ç”¨ç¦ç”¨è¯ã€‚\n",
        "\n",
        "ç›®æ ‡è¯æ±‡: {target_word}\n",
        "ç¦ç”¨è¯æ±‡: {', '.join(taboo_words)}\n",
        "\n",
        "ä¹‹å‰çš„çº¿ç´¢: {'; '.join(previous_hints)}\n",
        "é˜Ÿå‹çš„çŒœæµ‹: {'; '.join(previous_guesses)}\n",
        "\n",
        "é˜Ÿå‹è¿˜æ²¡æœ‰çŒœä¸­ã€‚è¯·ç»™å‡ºæ–°çš„çº¿ç´¢:\n",
        "1. ä½ çš„çº¿ç´¢ä¸­ä¸èƒ½åŒ…å«ä»»ä½•ç¦ç”¨è¯æ±‡\n",
        "2. è¯·ç”¨æ ¼å¼ [çº¿ç´¢] å¼€å§‹ä½ çš„å›ç­”\n",
        "3. çº¿ç´¢è¦ç®€æ´æ˜äº†ï¼Œä¸è¶…è¿‡20ä¸ªå­—\n",
        "4. å°è¯•ä»ä¸åŒè§’åº¦ç»™å‡ºçº¿ç´¢\n",
        "\n",
        "è¯·ç»™å‡ºä½ çš„æ–°çº¿ç´¢:\"\"\"\n",
        "    \n",
        "    # è¶…è¿‡æœ€å¤§è½®æ•°\n",
        "    final_guess = conversation_history[-1]['guess'] if conversation_history else None\n",
        "    return create_chinese_game_result(\n",
        "        False, max_turns, final_guess, \"è½®æ•°è€—å°½\", conversation_history\n",
        "    )\n",
        "\n",
        "print(\"âœ… ä¸­æ–‡Tabooæ¸¸æˆæ ¸å¿ƒé€»è¾‘å·²å®šä¹‰\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. æµ‹è¯•å®éªŒ (Test Experiment)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6.1 æµ‹è¯•å®éªŒå‡½æ•°\n",
        "def run_chinese_test_experiment(client, models, dataset, test_word_data=None):\n",
        "    \"\"\"è¿è¡Œä¸­æ–‡Tabooæµ‹è¯•å®éªŒ\"\"\"\n",
        "    \n",
        "    if not client:\n",
        "        print(\"âŒ APIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ‰§è¡Œæµ‹è¯•å®éªŒ\")\n",
        "        return None\n",
        "    \n",
        "    if not dataset:\n",
        "        print(\"âŒ æ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•æ‰§è¡Œæµ‹è¯•å®éªŒ\")\n",
        "        return None\n",
        "    \n",
        "    # é€‰æ‹©æµ‹è¯•è¯æ±‡\n",
        "    if test_word_data is None:\n",
        "        test_word_data = random.choice(dataset)\n",
        "    \n",
        "    target_word = test_word_data['target']\n",
        "    taboo_words = test_word_data['taboo']\n",
        "    pos = test_word_data.get('part_of_speech', 'unknown')\n",
        "    \n",
        "    print(f\"ğŸ§ª å¼€å§‹ä¸­æ–‡Tabooæµ‹è¯•å®éªŒ...\")\n",
        "    print(f\"ğŸ¯ æµ‹è¯•è¯: {target_word} ({pos})\")\n",
        "    print(f\"ğŸš« ç¦ç”¨è¯: {taboo_words}\")\n",
        "    print(f\"ğŸ¤– æµ‹è¯•æ¨¡å‹: {len(models)} ä¸ª\")\n",
        "    \n",
        "    total_games = len(models) ** 2\n",
        "    print(f\"ğŸ“Š æ€»æ¸¸æˆæ•°: {total_games}\")\n",
        "    \n",
        "    all_results = []\n",
        "    game_counter = 0\n",
        "    \n",
        "    # è¿è¡Œæ‰€æœ‰æ¨¡å‹ç»„åˆ\n",
        "    for hinter_model in models:\n",
        "        for guesser_model in models:\n",
        "            game_counter += 1\n",
        "            hinter_name = hinter_model.split('/')[-1]\n",
        "            guesser_name = guesser_model.split('/')[-1]\n",
        "            pair_name = f\"{hinter_name}â†’{guesser_name}\"\n",
        "            \n",
        "            print(f\"\\\\nğŸ”„ æ¸¸æˆ {game_counter}/{total_games}: {pair_name}\")\n",
        "            \n",
        "            start_time = time.time()\n",
        "            \n",
        "            # æ‰§è¡Œæ¸¸æˆ\n",
        "            game_result = chinese_taboo_game(\n",
        "                client, hinter_model, guesser_model, \n",
        "                target_word, taboo_words, max_turns=5\n",
        "            )\n",
        "            \n",
        "            duration = round(time.time() - start_time, 2)\n",
        "            \n",
        "            # è®°å½•ç»“æœ\n",
        "            result = {\n",
        "                'game_id': game_counter,\n",
        "                'target_word': target_word,\n",
        "                'part_of_speech': pos,\n",
        "                'hinter_model': hinter_model,\n",
        "                'guesser_model': guesser_model,\n",
        "                'success': game_result['success'],\n",
        "                'turns_used': game_result['turns'],\n",
        "                'final_guess': game_result.get('final_guess', ''),\n",
        "                'failure_reason': game_result.get('failure_reason', ''),\n",
        "                'duration_seconds': duration,\n",
        "                'taboo_words': '|'.join(taboo_words)\n",
        "            }\n",
        "            all_results.append(result)\n",
        "            \n",
        "            # æ˜¾ç¤ºç»“æœ\n",
        "            if game_result['success']:\n",
        "                print(f\"   âœ… æˆåŠŸ | {game_result['turns']}è½® | æœ€ç»ˆçŒœæµ‹: {game_result['final_guess']}\")\n",
        "            else:\n",
        "                print(f\"   âŒ å¤±è´¥ | {game_result['turns']}è½® | åŸå› : {game_result['failure_reason']}\")\n",
        "    \n",
        "    # è½¬æ¢ä¸ºDataFrame\n",
        "    results_df = pd.DataFrame(all_results)\n",
        "    \n",
        "    return results_df\n",
        "\n",
        "print(\"âœ… ä¸­æ–‡æµ‹è¯•å®éªŒå‡½æ•°å·²å®šä¹‰\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6.2 æ‰§è¡Œæµ‹è¯•å®éªŒ\n",
        "if chinese_client and chinese_dataset:\n",
        "    print(\"ğŸ§ª æ‰§è¡Œä¸­æ–‡Tabooæµ‹è¯•å®éªŒ...\")\n",
        "    \n",
        "    # é€‰æ‹©ä¸€ä¸ªæµ‹è¯•è¯æ±‡\n",
        "    test_word = random.choice(chinese_dataset)\n",
        "    \n",
        "    # ä½¿ç”¨å‰2ä¸ªæ¨¡å‹è¿›è¡Œå¿«é€Ÿæµ‹è¯•\n",
        "    test_models = CHINESE_MODELS[:2]\n",
        "    \n",
        "    # è¿è¡Œæµ‹è¯•\n",
        "    test_results = run_chinese_test_experiment(\n",
        "        chinese_client, test_models, chinese_dataset, test_word\n",
        "    )\n",
        "    \n",
        "    if test_results is not None and len(test_results) > 0:\n",
        "        print(f\"\\\\nâœ… æµ‹è¯•å®éªŒå®Œæˆï¼Œå…±{len(test_results)}åœºæ¸¸æˆ\")\n",
        "        \n",
        "        # æ˜¾ç¤ºæµ‹è¯•ç»“æœç»Ÿè®¡\n",
        "        print(\"\\\\nğŸ“Š æµ‹è¯•ç»“æœç»Ÿè®¡:\")\n",
        "        for model in test_models:\n",
        "            model_name = model.split('/')[-1]\n",
        "            model_as_hinter = test_results[test_results['hinter_model'] == model]\n",
        "            model_as_guesser = test_results[test_results['guesser_model'] == model]\n",
        "            \n",
        "            hinter_success = sum(model_as_hinter['success']) if len(model_as_hinter) > 0 else 0\n",
        "            guesser_success = sum(model_as_guesser['success']) if len(model_as_guesser) > 0 else 0\n",
        "            \n",
        "            hinter_total = len(model_as_hinter)\n",
        "            guesser_total = len(model_as_guesser)\n",
        "            \n",
        "            print(f\"   {model_name}:\")\n",
        "            print(f\"     ä½œä¸ºçº¿ç´¢ç»™å‡ºè€…: {hinter_success}/{hinter_total}\")\n",
        "            print(f\"     ä½œä¸ºçŒœæµ‹è€…: {guesser_success}/{guesser_total}\")\n",
        "        \n",
        "        # ä¿å­˜æµ‹è¯•ç»“æœ\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        test_output_file = f\"results/chinese_test_results_{timestamp}.csv\"\n",
        "        os.makedirs(\"results\", exist_ok=True)\n",
        "        test_results.to_csv(test_output_file, index=False, encoding='utf-8-sig')\n",
        "        print(f\"\\\\nğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜è‡³: {test_output_file}\")\n",
        "        \n",
        "        print(\"\\\\nğŸ’¡ æµ‹è¯•æˆåŠŸï¼å¯ä»¥è¿›è¡Œå®Œæ•´å®éªŒ\")\n",
        "    else:\n",
        "        print(\"âŒ æµ‹è¯•å®éªŒå¤±è´¥\")\n",
        "        \n",
        "else:\n",
        "    print(\"âŒ ç¼ºå°‘å¿…è¦ç»„ä»¶ï¼š\")\n",
        "    if not chinese_client:\n",
        "        print(\"   - APIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–\")\n",
        "    if not chinese_dataset:\n",
        "        print(\"   - ä¸­æ–‡æ•°æ®é›†æœªåŠ è½½\")\n",
        "    print(\"ğŸ’¡ è¯·å…ˆè¿è¡Œå‰é¢çš„æ­¥éª¤æ¥åˆå§‹åŒ–è¿™äº›ç»„ä»¶\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. å…¨é‡å®éªŒ (Full Experiment)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7.1 å…¨é‡å®éªŒå‡½æ•°\n",
        "def run_chinese_full_experiment(client, models, dataset, sample_size=None):\n",
        "    \"\"\"è¿è¡Œä¸­æ–‡Tabooå…¨é‡å®éªŒ\"\"\"\n",
        "    \n",
        "    if not client:\n",
        "        print(\"âŒ APIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ‰§è¡Œå…¨é‡å®éªŒ\")\n",
        "        return None\n",
        "    \n",
        "    if not dataset:\n",
        "        print(\"âŒ æ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•æ‰§è¡Œå…¨é‡å®éªŒ\")\n",
        "        return None\n",
        "    \n",
        "    # ç¡®å®šå®éªŒè§„æ¨¡\n",
        "    if sample_size and sample_size < len(dataset):\n",
        "        experiment_dataset = random.sample(dataset, sample_size)\n",
        "        print(f\"ğŸ“Š é‡‡æ ·å®éªŒï¼šä»{len(dataset)}ä¸ªè¯æ±‡ä¸­é€‰æ‹©{sample_size}ä¸ª\")\n",
        "    else:\n",
        "        experiment_dataset = dataset\n",
        "        print(f\"ğŸ“Š å…¨é‡å®éªŒï¼šä½¿ç”¨å…¨éƒ¨{len(dataset)}ä¸ªè¯æ±‡\")\n",
        "    \n",
        "    # åˆ›å»ºè¾“å‡ºç›®å½•\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    experiment_dir = f\"results/chinese_full_experiment_{timestamp}\"\n",
        "    os.makedirs(experiment_dir, exist_ok=True)\n",
        "    \n",
        "    print(f\"ğŸš€ å¼€å§‹ä¸­æ–‡Tabooå…¨é‡å®éªŒ...\")\n",
        "    print(f\"ğŸ“ è¾“å‡ºç›®å½•: {experiment_dir}\")\n",
        "    print(f\"ğŸ¯ è¯æ±‡æ•°é‡: {len(experiment_dataset)}\")\n",
        "    print(f\"ğŸ¤– æ¨¡å‹æ•°é‡: {len(models)}\")\n",
        "    \n",
        "    total_games = len(experiment_dataset) * len(models) * len(models)\n",
        "    print(f\"ğŸ® æ€»æ¸¸æˆæ•°: {total_games}\")\n",
        "    print(f\"â±ï¸ é¢„è®¡æ—¶é—´: ~{total_games * 0.5 / 60:.1f} åˆ†é’Ÿ\")\n",
        "    \n",
        "    all_results = []\n",
        "    game_counter = 0\n",
        "    \n",
        "    # æŒ‰è¯æ±‡éå†\n",
        "    for word_idx, word_data in enumerate(experiment_dataset, 1):\n",
        "        target_word = word_data['target']\n",
        "        taboo_words = word_data['taboo']\n",
        "        pos = word_data.get('part_of_speech', 'unknown')\n",
        "        \n",
        "        print(f\"\\\\nğŸ¯ è¯æ±‡ {word_idx}/{len(experiment_dataset)}: {target_word} ({pos})\")\n",
        "        \n",
        "        word_success = 0\n",
        "        word_total = 0\n",
        "        \n",
        "        # è¿è¡Œæ‰€æœ‰æ¨¡å‹ç»„åˆ\n",
        "        for hinter_model in models:\n",
        "            for guesser_model in models:\n",
        "                game_counter += 1\n",
        "                word_total += 1\n",
        "                \n",
        "                hinter_name = hinter_model.split('/')[-1]\n",
        "                guesser_name = guesser_model.split('/')[-1]\n",
        "                \n",
        "                # æ‰§è¡Œæ¸¸æˆ\n",
        "                start_time = time.time()\n",
        "                game_result = chinese_taboo_game(\n",
        "                    client, hinter_model, guesser_model,\n",
        "                    target_word, taboo_words, max_turns=5\n",
        "                )\n",
        "                duration = time.time() - start_time\n",
        "                \n",
        "                # è®°å½•ç»“æœ\n",
        "                                 result = {\n",
        "                     'game_id': game_counter,\n",
        "                     'word_index': word_idx,\n",
        "                     'target_word': target_word,\n",
        "                     'part_of_speech': pos,\n",
        "                     'category': word_data.get('category', 'chinese_general'),\n",
        "                     'hinter_model': hinter_model,\n",
        "                     'guesser_model': guesser_model,\n",
        "                     'success': game_result['success'],\n",
        "                     'turns_used': game_result['turns'],\n",
        "                     'final_guess': game_result.get('final_guess', ''),\n",
        "                     'failure_reason': game_result.get('failure_reason', ''),\n",
        "                     'duration_seconds': round(duration, 2),\n",
        "                     'taboo_words': '|'.join(taboo_words)\n",
        "                 }\n",
        "                 all_results.append(result)\n",
        "                 \n",
        "                 if game_result['success']:\n",
        "                     word_success += 1\n",
        "                 \n",
        "                 # æ˜¾ç¤ºè¿›åº¦ï¼ˆæ¯10ä¸ªæ¸¸æˆæ˜¾ç¤ºä¸€æ¬¡ï¼‰\n",
        "                 if game_counter % 10 == 0 or game_counter == total_games:\n",
        "                     progress = game_counter / total_games * 100\n",
        "                     print(f\"   è¿›åº¦: {game_counter}/{total_games} ({progress:.1f}%)\")\n",
        "        \n",
        "        # æ˜¾ç¤ºå½“å‰è¯æ±‡çš„æˆåŠŸç‡\n",
        "        word_success_rate = word_success / word_total * 100 if word_total > 0 else 0\n",
        "        print(f\"   è¯æ±‡æˆåŠŸç‡: {word_success}/{word_total} ({word_success_rate:.1f}%)\")\n",
        "        \n",
        "        # æ¯20ä¸ªè¯æ±‡ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ\n",
        "        if word_idx % 20 == 0 or word_idx == len(experiment_dataset):\n",
        "            intermediate_df = pd.DataFrame(all_results)\n",
        "            intermediate_file = f\"{experiment_dir}/intermediate_results_{word_idx:06d}.csv\"\n",
        "            intermediate_df.to_csv(intermediate_file, index=False, encoding='utf-8-sig')\n",
        "            print(f\"   ğŸ’¾ ä¸­é—´ç»“æœå·²ä¿å­˜: {intermediate_file}\")\\n    \\n    # è½¬æ¢ä¸ºDataFrameå¹¶ä¿å­˜æœ€ç»ˆç»“æœ\\n    results_df = pd.DataFrame(all_results)\\n    final_output_file = f\\\"{experiment_dir}/chinese_full_results_{timestamp}.csv\\\"\\n    results_df.to_csv(final_output_file, index=False, encoding='utf-8-sig')\\n    \\n    print(f\\\"\\\\nâœ… ä¸­æ–‡å…¨é‡å®éªŒå®Œæˆï¼\\\")\\n    print(f\\\"ğŸ“Š æ€»æ¸¸æˆæ•°: {len(results_df)}\\\")\\n    print(f\\\"ğŸ’¾ æœ€ç»ˆç»“æœ: {final_output_file}\\\")\\n    \\n    return results_df\\n\\nprint(\\\"âœ… ä¸­æ–‡å…¨é‡å®éªŒå‡½æ•°å·²å®šä¹‰\\\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. ç»“æœåˆ†æ (Results Analysis)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8.1 ä¸­æ–‡å®éªŒç»“æœåˆ†æå‡½æ•°\n",
        "def analyze_chinese_experiment_results(results_df):\n",
        "    \"\"\"åˆ†æä¸­æ–‡å®éªŒç»“æœ\"\"\"\n",
        "    if results_df is None or len(results_df) == 0:\n",
        "        print(\"âŒ æ²¡æœ‰ç»“æœæ•°æ®å¯ä¾›åˆ†æ\")\n",
        "        return None\n",
        "    \n",
        "    print(\"ğŸ“Š ä¸­æ–‡Tabooå®éªŒç»“æœåˆ†æ\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # æ•´ä½“æˆåŠŸç‡\n",
        "    total_games = len(results_df)\n",
        "    successful_games = sum(results_df['success'])\n",
        "    overall_success_rate = successful_games / total_games * 100\n",
        "    \n",
        "    print(f\"ğŸ® æ€»æ¸¸æˆæ•°: {total_games}\")\n",
        "    print(f\"âœ… æˆåŠŸæ¸¸æˆ: {successful_games}\")\n",
        "    print(f\"ğŸ“ˆ æ•´ä½“æˆåŠŸç‡: {overall_success_rate:.1f}%\")\n",
        "    \n",
        "    # æŒ‰æ¨¡å‹åˆ†æ\n",
        "    print(f\"\\nğŸ¤– æ¨¡å‹è¡¨ç°åˆ†æ:\")\n",
        "    models = results_df['hinter_model'].unique()\n",
        "    \n",
        "    for model in models:\n",
        "        model_name = model.split('/')[-1]\n",
        "        \n",
        "        # ä½œä¸ºHinterçš„è¡¨ç°\n",
        "        hinter_results = results_df[results_df['hinter_model'] == model]\n",
        "        hinter_success = sum(hinter_results['success'])\n",
        "        hinter_total = len(hinter_results)\n",
        "        hinter_rate = hinter_success / hinter_total * 100 if hinter_total > 0 else 0\n",
        "        \n",
        "        # ä½œä¸ºGuesserçš„è¡¨ç°\n",
        "        guesser_results = results_df[results_df['guesser_model'] == model]\n",
        "        guesser_success = sum(guesser_results['success'])\n",
        "        guesser_total = len(guesser_results)\n",
        "        guesser_rate = guesser_success / guesser_total * 100 if guesser_total > 0 else 0\n",
        "        \n",
        "        print(f\"   {model_name}:\")\n",
        "        print(f\"     ä½œä¸ºçº¿ç´¢ç»™å‡ºè€…: {hinter_success}/{hinter_total} ({hinter_rate:.1f}%)\")\n",
        "        print(f\"     ä½œä¸ºçŒœæµ‹è€…: {guesser_success}/{guesser_total} ({guesser_rate:.1f}%)\")\n",
        "    \n",
        "    # æŒ‰è¯æ€§åˆ†æ\n",
        "    if 'part_of_speech' in results_df.columns:\n",
        "        print(f\"\\nğŸ“ è¯æ€§è¡¨ç°åˆ†æ:\")\n",
        "        pos_types = results_df['part_of_speech'].unique()\n",
        "        \n",
        "        for pos in pos_types:\n",
        "            pos_results = results_df[results_df['part_of_speech'] == pos]\n",
        "            pos_success = sum(pos_results['success'])\n",
        "            pos_total = len(pos_results)\n",
        "            pos_rate = pos_success / pos_total * 100 if pos_total > 0 else 0\n",
        "            \n",
        "            print(f\"   {pos}: {pos_success}/{pos_total} ({pos_rate:.1f}%)\")\n",
        "    \n",
        "    # å¤±è´¥åŸå› åˆ†æ\n",
        "    print(f\"\\nâŒ å¤±è´¥åŸå› åˆ†æ:\")\n",
        "    failed_results = results_df[results_df['success'] == False]\n",
        "    \n",
        "    if len(failed_results) > 0:\n",
        "        failure_reasons = failed_results['failure_reason'].value_counts()\n",
        "        \n",
        "        for reason, count in failure_reasons.items():\n",
        "            percentage = count / len(failed_results) * 100\n",
        "            print(f\"   {reason}: {count} æ¬¡ ({percentage:.1f}%)\")\n",
        "    else:\n",
        "        print(\"   ğŸ‰ æ²¡æœ‰å¤±è´¥æ¡ˆä¾‹ï¼\")\n",
        "    \n",
        "    # è½®æ•°åˆ†æ\n",
        "    print(f\"\\nğŸ”„ æ¸¸æˆè½®æ•°åˆ†æ:\")\n",
        "    successful_results = results_df[results_df['success'] == True]\n",
        "    if len(successful_results) > 0:\n",
        "        avg_turns = successful_results['turns_used'].mean()\n",
        "        print(f\"   æˆåŠŸæ¸¸æˆå¹³å‡è½®æ•°: {avg_turns:.1f}\")\n",
        "        \n",
        "        turn_distribution = successful_results['turns_used'].value_counts().sort_index()\n",
        "        for turns, count in turn_distribution.items():\n",
        "            percentage = count / len(successful_results) * 100\n",
        "            print(f\"   {turns}è½®æˆåŠŸ: {count} æ¬¡ ({percentage:.1f}%)\")\n",
        "    \n",
        "    return {\n",
        "        'overall_success_rate': overall_success_rate,\n",
        "        'total_games': total_games,\n",
        "        'successful_games': successful_games,\n",
        "        'model_analysis': {model.split('/')[-1]: {\n",
        "            'hinter_rate': sum(results_df[results_df['hinter_model'] == model]['success']) / len(results_df[results_df['hinter_model'] == model]) * 100,\n",
        "            'guesser_rate': sum(results_df[results_df['guesser_model'] == model]['success']) / len(results_df[results_df['guesser_model'] == model]) * 100\n",
        "        } for model in models}\n",
        "    }\n",
        "\n",
        "print(\"âœ… ä¸­æ–‡ç»“æœåˆ†æå‡½æ•°å·²å®šä¹‰\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8.2 å®éªŒæ‰§è¡Œé€‰é¡¹\n",
        "print(\"ğŸš€ ä¸­æ–‡Tabooå®éªŒç³»ç»Ÿå·²å®Œå…¨å‡†å¤‡å°±ç»ªï¼\")\n",
        "print(\"\\nğŸ“‹ å¯æ‰§è¡Œçš„å®éªŒé€‰é¡¹:\")\n",
        "\n",
        "if chinese_client and chinese_dataset:\n",
        "    print(\"âœ… æ‰€æœ‰ç»„ä»¶å·²å‡†å¤‡å°±ç»ª\")\n",
        "    print(\"\\nğŸ’¡ æ‰§è¡Œé€‰é¡¹:\")\n",
        "    print(\"1. å¿«é€ŸéªŒè¯æµ‹è¯•ï¼ˆæ¨èå…ˆæ‰§è¡Œï¼‰:\")\n",
        "    print(\"   # å·²åœ¨ä¸Šé¢æ‰§è¡Œè¿‡æµ‹è¯•\")\n",
        "    print(\"\\n2. å°è§„æ¨¡å®éªŒï¼ˆé‡‡æ ·10ä¸ªè¯æ±‡ï¼‰:\")\n",
        "    print(\"   # small_results = run_chinese_full_experiment(chinese_client, CHINESE_MODELS, chinese_dataset, sample_size=10)\")\n",
        "    print(\"\\n3. å®Œæ•´å®éªŒï¼ˆæ‰€æœ‰è¯æ±‡ï¼‰:\")\n",
        "    print(\"   # full_results = run_chinese_full_experiment(chinese_client, CHINESE_MODELS, chinese_dataset)\")\n",
        "    print(\"\\n4. ç»“æœåˆ†æ:\")\n",
        "    print(\"   # analysis = analyze_chinese_experiment_results(full_results)\")\n",
        "    \n",
        "    # å®éªŒè§„æ¨¡é¢„ä¼°\n",
        "    total_words = len(chinese_dataset)\n",
        "    total_models = len(CHINESE_MODELS)\n",
        "    total_games_full = total_words * total_models * total_models\n",
        "    \n",
        "    print(f\"\\nğŸ“Š å®Œæ•´å®éªŒè§„æ¨¡:\")\n",
        "    print(f\"   è¯æ±‡æ•°é‡: {total_words}\")\n",
        "    print(f\"   æ¨¡å‹æ•°é‡: {total_models}\")\n",
        "    print(f\"   æ€»æ¸¸æˆæ•°: {total_games_full:,}\")\n",
        "    print(f\"   é¢„è®¡æ—¶é—´: ~{total_games_full * 0.5 / 60:.1f} åˆ†é’Ÿ\")\n",
        "    \n",
        "    print(f\"\\nğŸ¯ å®éªŒç‰¹ç‚¹:\")\n",
        "    print(f\"   â€¢ é¦–æ¬¡ä½¿ç”¨OpenHowNetæ„å»ºçš„ä¸­æ–‡æ•°æ®é›†\")\n",
        "    print(f\"   â€¢ ä¸­æ–‡æ ¼å¼è¦æ±‚: [çº¿ç´¢] å’Œ [çŒœæµ‹]\")\n",
        "    print(f\"   â€¢ ä¸­æ–‡ç¦ç”¨è¯æ£€æµ‹: ä½¿ç”¨jiebaåˆ†è¯\")\n",
        "    print(f\"   â€¢ æ‰¹æ¬¡ä¿å­˜: æ¯20ä¸ªè¯æ±‡ä¿å­˜ä¸­é—´ç»“æœ\")\n",
        "    print(f\"   â€¢ UTF-8ç¼–ç : å®Œæ•´æ”¯æŒä¸­æ–‡å­—ç¬¦\")\n",
        "    \n",
        "else:\n",
        "    print(\"âŒ ç¼ºå°‘å¿…è¦ç»„ä»¶ï¼š\")\n",
        "    if not chinese_client:\n",
        "        print(\"   - APIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼ˆéœ€è¦é…ç½®api_keys.jsonï¼‰\")\n",
        "    if not chinese_dataset:\n",
        "        print(\"   - ä¸­æ–‡æ•°æ®é›†æœªåŠ è½½ï¼ˆæ£€æŸ¥data/chinese_dataset.jsonï¼‰\")\n",
        "    print(\"\\nğŸ’¡ è¯·å…ˆè¿è¡Œå‰é¢çš„æ­¥éª¤æ¥åˆå§‹åŒ–è¿™äº›ç»„ä»¶\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ğŸ ä¸­æ–‡Tabooå®éªŒæ¡†æ¶æ¢³ç†å®Œæˆ!\")\n",
        "print(\"ğŸ“ æ­¤å®éªŒå®Œå…¨å‚ç…§base_test.ipynbçš„8æ¨¡å—æ¶æ„\")\n",
        "print(\"ğŸ”¬ ä¸“é—¨é’ˆå¯¹ä¸­æ–‡è¯­è¨€å’ŒOpenHowNetæ•°æ®é›†ä¼˜åŒ–\")\n",
        "print(\"ğŸ“Š æä¾›å®Œæ•´çš„å®éªŒã€åˆ†æå’ŒæŠ¥å‘Šç”ŸæˆåŠŸèƒ½\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
