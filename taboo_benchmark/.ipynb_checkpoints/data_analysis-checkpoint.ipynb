{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Tabooæ¸¸æˆå®éªŒæ•°æ®åˆ†æ\n",
        "\n",
        "æœ¬notebookåˆ†æäº†å¤šä¸ªå¤§è¯­è¨€æ¨¡å‹åœ¨TabooçŒœè¯æ¸¸æˆä¸­çš„è¡¨ç°ï¼ŒåŒ…æ‹¬ï¼š\n",
        "- å„ä¸ªæ¨¡å‹çš„æˆåŠŸç‡åˆ†æ\n",
        "- ç¬¬ä¸€æ¬¡æˆåŠŸç‡åˆ†æ  \n",
        "- æŒ‰è¯æ€§çš„æˆåŠŸç‡åˆ†æ\n",
        "- æŒ‰æŠ½è±¡ç¨‹åº¦çš„æˆåŠŸç‡åˆ†æ\n",
        "- æŒ‰é¢†åŸŸçš„æˆåŠŸç‡åˆ†æ\n",
        "- é€‚åˆè®ºæ–‡å±•ç¤ºçš„å…¶ä»–å›¾è¡¨\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# è®¾ç½®ä¸­æ–‡å­—ä½“\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# è®¾ç½®å›¾è¡¨æ ·å¼\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.style.use('seaborn-v0_8')\n",
        "\n",
        "# è®¾ç½®é¢œè‰²è°ƒè‰²æ¿\n",
        "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f']\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åŠ è½½æ•°æ®é›†\n",
        "with open('data/dataset.json', 'r', encoding='utf-8') as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "# è½¬æ¢ä¸ºDataFrame\n",
        "dataset_df = pd.DataFrame(dataset)\n",
        "\n",
        "# æå–concreteness_score\n",
        "dataset_df['concreteness_score'] = dataset_df['metadata'].apply(lambda x: x.get('concreteness_score'))\n",
        "\n",
        "print(f\"æ•°æ®é›†æ€»è¯æ±‡æ•°: {len(dataset_df)}\")\n",
        "print(f\"è¯æ€§åˆ†å¸ƒ: {dataset_df['part_of_speech'].value_counts().to_dict()}\")\n",
        "print(f\"é¢†åŸŸåˆ†å¸ƒ: {dataset_df['category'].value_counts().to_dict()}\")\n",
        "print(f\"æœ‰æŠ½è±¡ç¨‹åº¦è¯„åˆ†çš„è¯æ±‡æ•°: {dataset_df['concreteness_score'].notna().sum()}\")\n",
        "\n",
        "dataset_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åŠ è½½å®éªŒç»“æœæ•°æ®\n",
        "results_df = pd.read_csv('results/taboo_experiment_20250712_004918/complete_experiment_results.csv')\n",
        "\n",
        "print(f\"å®éªŒç»“æœæ€»æ•°: {len(results_df)}\")\n",
        "print(f\"å‚ä¸å®éªŒçš„æ¨¡å‹: {results_df['hinter_model'].unique().tolist()}\")\n",
        "print(f\"æ€»ä½“æˆåŠŸç‡: {results_df['success'].mean():.3f}\")\n",
        "\n",
        "# æ¸…ç†æ¨¡å‹åç§°ä»¥ä¾¿æ˜¾ç¤º\n",
        "model_name_mapping = {\n",
        "    'anthropic/claude-sonnet-4': 'Claude Sonnet 4',\n",
        "    'openai/gpt-4o': 'GPT-4o',\n",
        "    'google/gemini-2.5-pro': 'Gemini 2.5 Pro',\n",
        "    'deepseek/deepseek-chat-v3-0324': 'DeepSeek Chat V3'\n",
        "}\n",
        "\n",
        "results_df['hinter_model_clean'] = results_df['hinter_model'].map(model_name_mapping)\n",
        "results_df['guesser_model_clean'] = results_df['guesser_model'].map(model_name_mapping)\n",
        "\n",
        "results_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åˆå¹¶æ•°æ®é›†ä¿¡æ¯åˆ°ç»“æœä¸­\n",
        "# åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„æ•°æ®é›†æ˜ å°„\n",
        "dataset_info = dataset_df[['target', 'part_of_speech', 'category', 'concreteness_score']].copy()\n",
        "dataset_info = dataset_info.rename(columns={'target': 'target_word'})\n",
        "\n",
        "# åˆå¹¶æ•°æ®\n",
        "merged_df = results_df.merge(dataset_info, on='target_word', how='left')\n",
        "\n",
        "print(f\"åˆå¹¶åæ•°æ®é‡: {len(merged_df)}\")\n",
        "print(f\"æˆåŠŸåŒ¹é…è¯æ±‡ä¿¡æ¯çš„æ¯”ä¾‹: {merged_df['part_of_speech'].notna().mean():.3f}\")\n",
        "\n",
        "merged_df.head()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. å„ä¸ªæ¨¡å‹çš„æˆåŠŸç‡åˆ†æ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# è®¡ç®—å„æ¨¡å‹çš„æ€»ä½“æˆåŠŸç‡\n",
        "model_success = merged_df.groupby('hinter_model_clean').agg({\n",
        "    'success': ['count', 'sum', 'mean'],\n",
        "    'turns_used': 'mean',\n",
        "    'has_taboo_violation': 'mean'\n",
        "}).round(3)\n",
        "\n",
        "model_success.columns = ['æ€»æ¸¸æˆæ•°', 'æˆåŠŸæ•°', 'æˆåŠŸç‡', 'å¹³å‡è½®æ•°', 'è¿è§„ç‡']\n",
        "model_success = model_success.sort_values('æˆåŠŸç‡', ascending=False)\n",
        "\n",
        "print(\"å„æ¨¡å‹è¡¨ç°æ€»è§ˆ:\")\n",
        "print(model_success)\n",
        "\n",
        "# ç»˜åˆ¶æ¨¡å‹æˆåŠŸç‡å¯¹æ¯”å›¾\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# æˆåŠŸç‡æŸ±çŠ¶å›¾\n",
        "bars1 = ax1.bar(model_success.index, model_success['æˆåŠŸç‡'], color=colors[:len(model_success)])\n",
        "ax1.set_title('å„æ¨¡å‹æˆåŠŸç‡å¯¹æ¯”', fontsize=14, fontweight='bold')\n",
        "ax1.set_ylabel('æˆåŠŸç‡')\n",
        "ax1.set_ylim(0, 1)\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾\n",
        "for bar in bars1:\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "             f'{height:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# å¹³å‡è½®æ•°å¯¹æ¯”\n",
        "bars2 = ax2.bar(model_success.index, model_success['å¹³å‡è½®æ•°'], color=colors[:len(model_success)])\n",
        "ax2.set_title('å„æ¨¡å‹å¹³å‡è½®æ•°å¯¹æ¯”', fontsize=14, fontweight='bold')\n",
        "ax2.set_ylabel('å¹³å‡è½®æ•°')\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾\n",
        "for bar in bars2:\n",
        "    height = bar.get_height()\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
        "             f'{height:.2f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. æŒ‰æˆåŠŸè½®æ•°çš„è¯¦ç»†åˆ†æ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åˆ†ææˆåŠŸæ¡ˆä¾‹çš„è½®æ•°åˆ†å¸ƒ\n",
        "successful_games = merged_df[merged_df['success'] == True]\n",
        "\n",
        "# è®¡ç®—å„è½®æˆåŠŸçš„è¯¦ç»†åˆ†å¸ƒ\n",
        "turns_analysis = successful_games.groupby(['hinter_model_clean', 'turns_used']).size().unstack(fill_value=0)\n",
        "turns_pct = turns_analysis.div(turns_analysis.sum(axis=1), axis=0)\n",
        "\n",
        "# è®¡ç®—å„è½®æˆåŠŸç‡ï¼ˆåŒ…æ‹¬ç¬¬1è½®ã€ç¬¬2è½®ç­‰ï¼‰\n",
        "turn_success_rates = {}\n",
        "for turn in range(1, 6):  # åˆ†æå‰5è½®\n",
        "    turn_rates = successful_games.groupby('hinter_model_clean').apply(\n",
        "        lambda x: (x['turns_used'] == turn).sum() / len(x)\n",
        "    )\n",
        "    turn_success_rates[f'ç¬¬{turn}è½®æˆåŠŸç‡'] = turn_rates\n",
        "\n",
        "turn_success_df = pd.DataFrame(turn_success_rates).fillna(0)\n",
        "print(\"å„æ¨¡å‹åœ¨ä¸åŒè½®æ•°çš„æˆåŠŸç‡åˆ†å¸ƒ:\")\n",
        "print(turn_success_df.round(3))\n",
        "\n",
        "# è®¡ç®—ç´¯ç§¯æˆåŠŸç‡\n",
        "cumulative_success = {}\n",
        "for turn in range(1, 6):\n",
        "    cumulative_rates = successful_games.groupby('hinter_model_clean').apply(\n",
        "        lambda x: (x['turns_used'] <= turn).sum() / len(x)\n",
        "    )\n",
        "    cumulative_success[f'å‰{turn}è½®ç´¯ç§¯æˆåŠŸç‡'] = cumulative_rates\n",
        "\n",
        "cumulative_df = pd.DataFrame(cumulative_success).fillna(0)\n",
        "print(\"\\nå„æ¨¡å‹çš„ç´¯ç§¯æˆåŠŸç‡:\")\n",
        "print(cumulative_df.round(3))\n",
        "\n",
        "# ç»˜åˆ¶è¯¦ç»†çš„è½®æ•°åˆ†æå›¾\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 1. å„è½®æˆåŠŸç‡å¯¹æ¯”\n",
        "turn_success_df.plot(kind='bar', ax=ax1, color=colors[:len(turn_success_df.columns)])\n",
        "ax1.set_title('å„æ¨¡å‹åœ¨ä¸åŒè½®æ•°çš„æˆåŠŸç‡', fontsize=14, fontweight='bold')\n",
        "ax1.set_ylabel('æˆåŠŸç‡')\n",
        "ax1.set_xlabel('æ¨¡å‹')\n",
        "ax1.legend(title='è½®æ•°', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 2. å †ç§¯æŸ±çŠ¶å›¾æ˜¾ç¤ºè½®æ•°åˆ†å¸ƒ\n",
        "turns_pct.plot(kind='bar', stacked=True, ax=ax2, colormap='viridis', \n",
        "               legend_kws={'title': 'è½®æ•°', 'bbox_to_anchor': (1.05, 1)})\n",
        "ax2.set_title('æˆåŠŸæ¡ˆä¾‹çš„è½®æ•°åˆ†å¸ƒï¼ˆç™¾åˆ†æ¯”ï¼‰', fontsize=14, fontweight='bold')\n",
        "ax2.set_ylabel('æ¯”ä¾‹')\n",
        "ax2.set_xlabel('æ¨¡å‹')\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 3. ç´¯ç§¯æˆåŠŸç‡æ›²çº¿\n",
        "for i, model in enumerate(cumulative_df.index):\n",
        "    turns = range(1, 6)\n",
        "    rates = [cumulative_df.loc[model, f'å‰{turn}è½®ç´¯ç§¯æˆåŠŸç‡'] for turn in turns]\n",
        "    ax3.plot(turns, rates, 'o-', linewidth=2, label=model, color=colors[i])\n",
        "\n",
        "ax3.set_title('ç´¯ç§¯æˆåŠŸç‡æ›²çº¿', fontsize=14, fontweight='bold')\n",
        "ax3.set_xlabel('è½®æ•°')\n",
        "ax3.set_ylabel('ç´¯ç§¯æˆåŠŸç‡')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "ax3.set_xticks(range(1, 6))\n",
        "ax3.set_ylim(0, 1)\n",
        "\n",
        "# 4. ç¬¬1è½®vså…¶ä»–è½®æ•°æˆåŠŸç‡å¯¹æ¯”\n",
        "first_turn_vs_others = pd.DataFrame({\n",
        "    'ç¬¬1è½®æˆåŠŸç‡': turn_success_df['ç¬¬1è½®æˆåŠŸç‡'],\n",
        "    'å…¶ä»–è½®æˆåŠŸç‡': 1 - turn_success_df['ç¬¬1è½®æˆåŠŸç‡']\n",
        "})\n",
        "\n",
        "first_turn_vs_others.plot(kind='bar', ax=ax4, color=['#2ca02c', '#ff7f0e'])\n",
        "ax4.set_title('ç¬¬1è½® vs å…¶ä»–è½®æ¬¡æˆåŠŸç‡å¯¹æ¯”', fontsize=14, fontweight='bold')\n",
        "ax4.set_ylabel('æ¯”ä¾‹')\n",
        "ax4.set_xlabel('æ¨¡å‹')\n",
        "ax4.legend(title='æˆåŠŸè½®æ¬¡')\n",
        "ax4.tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# è¾“å‡ºè¯¦ç»†ç»Ÿè®¡\n",
        "print(f\"\\nğŸ“ˆ è½®æ•°åˆ†ææ€»ç»“:\")\n",
        "print(f\"  â€¢ æ€»ä½“ç¬¬1è½®æˆåŠŸç‡: {turn_success_df['ç¬¬1è½®æˆåŠŸç‡'].mean():.1%}\")\n",
        "print(f\"  â€¢ æ€»ä½“å‰3è½®ç´¯ç§¯æˆåŠŸç‡: {cumulative_df['å‰3è½®ç´¯ç§¯æˆåŠŸç‡'].mean():.1%}\")\n",
        "print(f\"  â€¢ ç¬¬1è½®æˆåŠŸç‡æœ€é«˜çš„æ¨¡å‹: {turn_success_df['ç¬¬1è½®æˆåŠŸç‡'].idxmax()} ({turn_success_df['ç¬¬1è½®æˆåŠŸç‡'].max():.1%})\")\n",
        "print(f\"  â€¢ å‰3è½®ç´¯ç§¯æˆåŠŸç‡æœ€é«˜çš„æ¨¡å‹: {cumulative_df['å‰3è½®ç´¯ç§¯æˆåŠŸç‡'].idxmax()} ({cumulative_df['å‰3è½®ç´¯ç§¯æˆåŠŸç‡'].max():.1%})\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. æŒ‰è¯æ€§çš„æˆåŠŸç‡åˆ†æ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æŒ‰è¯æ€§åˆ†ææˆåŠŸç‡\n",
        "pos_success = merged_df.groupby(['part_of_speech', 'hinter_model_clean']).agg({\n",
        "    'success': ['count', 'mean'],\n",
        "    'turns_used': 'mean'\n",
        "}).round(3)\n",
        "\n",
        "pos_success.columns = ['æ¸¸æˆæ•°', 'æˆåŠŸç‡', 'å¹³å‡è½®æ•°']\n",
        "pos_success = pos_success.reset_index()\n",
        "\n",
        "print(\"æŒ‰è¯æ€§çš„æˆåŠŸç‡åˆ†æ:\")\n",
        "pos_pivot = pos_success.pivot(index='part_of_speech', columns='hinter_model_clean', values='æˆåŠŸç‡')\n",
        "print(pos_pivot.round(3))\n",
        "\n",
        "# ç»˜åˆ¶è¯æ€§æˆåŠŸç‡çƒ­åŠ›å›¾\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
        "\n",
        "# çƒ­åŠ›å›¾\n",
        "sns.heatmap(pos_pivot, annot=True, cmap='YlOrRd', ax=ax1, \n",
        "            cbar_kws={'label': 'æˆåŠŸç‡'}, fmt='.3f')\n",
        "ax1.set_title('å„æ¨¡å‹åœ¨ä¸åŒè¯æ€§ä¸Šçš„æˆåŠŸç‡çƒ­åŠ›å›¾', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('æ¨¡å‹')\n",
        "ax1.set_ylabel('è¯æ€§')\n",
        "\n",
        "# è¯æ€§æ•´ä½“æˆåŠŸç‡\n",
        "overall_pos = merged_df.groupby('part_of_speech')['success'].mean().sort_values(ascending=True)\n",
        "bars = ax2.barh(range(len(overall_pos)), overall_pos.values, color=colors[:len(overall_pos)])\n",
        "ax2.set_yticks(range(len(overall_pos)))\n",
        "ax2.set_yticklabels(overall_pos.index)\n",
        "ax2.set_title('ä¸åŒè¯æ€§çš„æ•´ä½“æˆåŠŸç‡', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('æˆåŠŸç‡')\n",
        "\n",
        "# æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
        "for i, bar in enumerate(bars):\n",
        "    width = bar.get_width()\n",
        "    ax2.text(width + 0.01, bar.get_y() + bar.get_height()/2.,\n",
        "             f'{width:.3f}', ha='left', va='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. æŒ‰æŠ½è±¡ç¨‹åº¦çš„æˆåŠŸç‡åˆ†æ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# è¿‡æ»¤æœ‰æŠ½è±¡ç¨‹åº¦è¯„åˆ†çš„æ•°æ®\n",
        "concrete_df = merged_df[merged_df['concreteness_score'].notna()].copy()\n",
        "\n",
        "print(f\"æœ‰æŠ½è±¡ç¨‹åº¦è¯„åˆ†çš„å®éªŒæ•°æ®: {len(concrete_df)} æ¡\")\n",
        "\n",
        "# å°†æŠ½è±¡ç¨‹åº¦åˆ†ä¸ºå‡ ä¸ªåŒºé—´\n",
        "concrete_df['concreteness_level'] = pd.cut(concrete_df['concreteness_score'], \n",
        "                                           bins=[0, 2, 3, 4, 5], \n",
        "                                           labels=['é«˜æŠ½è±¡(0-2)', 'ä¸­æŠ½è±¡(2-3)', 'ä¸­å…·ä½“(3-4)', 'é«˜å…·ä½“(4-5)'])\n",
        "\n",
        "# æŒ‰æŠ½è±¡ç¨‹åº¦åˆ†æ\n",
        "concrete_success = concrete_df.groupby(['concreteness_level', 'hinter_model_clean']).agg({\n",
        "    'success': ['count', 'mean'],\n",
        "    'turns_used': 'mean'\n",
        "}).round(3)\n",
        "\n",
        "concrete_success.columns = ['æ¸¸æˆæ•°', 'æˆåŠŸç‡', 'å¹³å‡è½®æ•°']\n",
        "concrete_success = concrete_success.reset_index()\n",
        "\n",
        "print(\"æŒ‰æŠ½è±¡ç¨‹åº¦çš„æˆåŠŸç‡åˆ†æ:\")\n",
        "concrete_pivot = concrete_success.pivot(index='concreteness_level', columns='hinter_model_clean', values='æˆåŠŸç‡')\n",
        "print(concrete_pivot.round(3))\n",
        "\n",
        "# ç»˜åˆ¶æŠ½è±¡ç¨‹åº¦åˆ†æå›¾\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# ä¸åŒæ¨¡å‹åœ¨å„æŠ½è±¡ç¨‹åº¦çš„è¡¨ç°\n",
        "concrete_pivot.plot(kind='bar', ax=ax1, color=colors[:len(concrete_pivot.columns)])\n",
        "ax1.set_title('å„æ¨¡å‹åœ¨ä¸åŒæŠ½è±¡ç¨‹åº¦è¯æ±‡ä¸Šçš„æˆåŠŸç‡', fontsize=14, fontweight='bold')\n",
        "ax1.set_ylabel('æˆåŠŸç‡')\n",
        "ax1.set_xlabel('æŠ½è±¡ç¨‹åº¦')\n",
        "ax1.legend(title='æ¨¡å‹', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "ax1.set_ylim(0, 1)\n",
        "\n",
        "# æŠ½è±¡ç¨‹åº¦ä¸æˆåŠŸç‡çš„å…³ç³»æ•£ç‚¹å›¾\n",
        "for i, model in enumerate(concrete_df['hinter_model_clean'].unique()):\n",
        "    model_data = concrete_df[concrete_df['hinter_model_clean'] == model]\n",
        "    success_by_concrete = model_data.groupby('concreteness_score')['success'].mean()\n",
        "    ax2.scatter(success_by_concrete.index, success_by_concrete.values, \n",
        "               label=model, color=colors[i], alpha=0.7, s=50)\n",
        "\n",
        "ax2.set_title('æŠ½è±¡ç¨‹åº¦ä¸æˆåŠŸç‡çš„å…³ç³»', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('å…·ä½“ç¨‹åº¦è¯„åˆ† (1=æŠ½è±¡, 5=å…·ä½“)')\n",
        "ax2.set_ylabel('æˆåŠŸç‡')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. æŒ‰é¢†åŸŸçš„æˆåŠŸç‡åˆ†æ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æŒ‰é¢†åŸŸåˆ†ææˆåŠŸç‡\n",
        "category_success = merged_df.groupby(['category', 'hinter_model_clean']).agg({\n",
        "    'success': ['count', 'mean'],\n",
        "    'turns_used': 'mean'\n",
        "}).round(3)\n",
        "\n",
        "category_success.columns = ['æ¸¸æˆæ•°', 'æˆåŠŸç‡', 'å¹³å‡è½®æ•°']\n",
        "category_success = category_success.reset_index()\n",
        "\n",
        "print(\"æŒ‰é¢†åŸŸçš„æˆåŠŸç‡åˆ†æ:\")\n",
        "category_pivot = category_success.pivot(index='category', columns='hinter_model_clean', values='æˆåŠŸç‡')\n",
        "print(category_pivot.round(3))\n",
        "\n",
        "# å¦‚æœåªæœ‰generalé¢†åŸŸï¼ŒåŠ è½½ä¸“ä¸šé¢†åŸŸæ•°æ®è¿›è¡Œè¡¥å……åˆ†æ\n",
        "try:\n",
        "    # åŠ è½½ä¸“ä¸šé¢†åŸŸæ•°æ®é›†\n",
        "    domain_datasets = {}\n",
        "    domains = ['cs', 'biology', 'law', 'literature', 'medical']\n",
        "    \n",
        "    for domain in domains:\n",
        "        try:\n",
        "            with open(f'hpc_taboo/data/{domain}_wordnet_dataset.json', 'r') as f:\n",
        "                domain_datasets[domain] = json.load(f)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"æœªæ‰¾åˆ°{domain}æ•°æ®é›†\")\n",
        "    \n",
        "    print(f\"\\nåŠ è½½äº†{len(domain_datasets)}ä¸ªä¸“ä¸šé¢†åŸŸæ•°æ®é›†\")\n",
        "    for domain, data in domain_datasets.items():\n",
        "        print(f\"{domain}: {len(data)} ä¸ªè¯æ±‡\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"åŠ è½½ä¸“ä¸šé¢†åŸŸæ•°æ®æ—¶å‡ºé”™: {e}\")\n",
        "\n",
        "# ç»˜åˆ¶é¢†åŸŸåˆ†æå›¾\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
        "\n",
        "if len(category_pivot) > 1:\n",
        "    category_pivot.plot(kind='bar', ax=ax, color=colors[:len(category_pivot.columns)])\n",
        "    ax.set_title('å„æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸçš„æˆåŠŸç‡', fontsize=14, fontweight='bold')\n",
        "else:\n",
        "    # å¦‚æœåªæœ‰generalé¢†åŸŸï¼Œæ˜¾ç¤ºæ•´ä½“åˆ†å¸ƒ\n",
        "    overall_category = merged_df.groupby('hinter_model_clean')['success'].mean()\n",
        "    bars = ax.bar(overall_category.index, overall_category.values, color=colors[:len(overall_category)])\n",
        "    ax.set_title('å„æ¨¡å‹åœ¨é€šç”¨é¢†åŸŸçš„æˆåŠŸç‡', fontsize=14, fontweight='bold')\n",
        "    \n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                 f'{height:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "ax.set_ylabel('æˆåŠŸç‡')\n",
        "ax.set_xlabel('æ¨¡å‹')\n",
        "ax.tick_params(axis='x', rotation=45)\n",
        "ax.set_ylim(0, 1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. é€‚åˆè®ºæ–‡å±•ç¤ºçš„ç»¼åˆåˆ†æå›¾è¡¨\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åˆ›å»ºç»¼åˆæ€§èƒ½å¯¹æ¯”é›·è¾¾å›¾\n",
        "from math import pi\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# è®¡ç®—å„æ¨¡å‹çš„å¤šç»´æ€§èƒ½æŒ‡æ ‡\n",
        "model_metrics = merged_df.groupby('hinter_model_clean').agg({\n",
        "    'success': 'mean',\n",
        "    'turns_used': lambda x: 1/(x[merged_df.loc[x.index, 'success']].mean()),  # æ•ˆç‡æŒ‡æ ‡ï¼ˆè½®æ•°è¶Šå°‘è¶Šå¥½ï¼‰\n",
        "    'has_taboo_violation': lambda x: 1-x.mean()  # è§„åˆ™éµå®ˆæŒ‡æ ‡\n",
        "}).round(3)\n",
        "\n",
        "# æ·»åŠ ç¬¬ä¸€æ¬¡æˆåŠŸç‡\n",
        "first_success_rate = merged_df[merged_df['success'] == True].groupby('hinter_model_clean').apply(\n",
        "    lambda x: (x['turns_used'] == 1).sum() / len(x)\n",
        ")\n",
        "model_metrics['ç¬¬ä¸€æ¬¡æˆåŠŸç‡'] = first_success_rate\n",
        "\n",
        "model_metrics.columns = ['æˆåŠŸç‡', 'æ•ˆç‡æŒ‡æ ‡', 'è§„åˆ™éµå®ˆ', 'ç¬¬ä¸€æ¬¡æˆåŠŸç‡']\n",
        "\n",
        "# æ ‡å‡†åŒ–æŒ‡æ ‡åˆ°0-1èŒƒå›´\n",
        "scaler = MinMaxScaler()\n",
        "model_metrics_scaled = pd.DataFrame(\n",
        "    scaler.fit_transform(model_metrics), \n",
        "    index=model_metrics.index, \n",
        "    columns=model_metrics.columns\n",
        ")\n",
        "\n",
        "print(\"æ¨¡å‹ç»¼åˆæ€§èƒ½æŒ‡æ ‡:\")\n",
        "print(model_metrics)\n",
        "\n",
        "# ç»˜åˆ¶é›·è¾¾å›¾\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
        "\n",
        "angles = [n / float(len(model_metrics.columns)) * 2 * pi for n in range(len(model_metrics.columns))]\n",
        "angles += angles[:1]  # é—­åˆå›¾å½¢\n",
        "\n",
        "for i, (model, values) in enumerate(model_metrics_scaled.iterrows()):\n",
        "    values_list = values.tolist()\n",
        "    values_list += values_list[:1]  # é—­åˆå›¾å½¢\n",
        "    \n",
        "    ax.plot(angles, values_list, 'o-', linewidth=2, label=model, color=colors[i])\n",
        "    ax.fill(angles, values_list, alpha=0.25, color=colors[i])\n",
        "\n",
        "ax.set_xticks(angles[:-1])\n",
        "ax.set_xticklabels(model_metrics.columns)\n",
        "ax.set_ylim(0, 1)\n",
        "ax.set_title('æ¨¡å‹ç»¼åˆæ€§èƒ½é›·è¾¾å›¾', fontsize=16, fontweight='bold', pad=20)\n",
        "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
        "ax.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# é”™è¯¯åˆ†æ - å¤±è´¥åŸå› åˆ†å¸ƒ\n",
        "failure_analysis = merged_df[merged_df['success'] == False]['failure_reason'].value_counts()\n",
        "\n",
        "print(\"å¤±è´¥åŸå› åˆ†å¸ƒ:\")\n",
        "print(failure_analysis)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# å¤±è´¥åŸå› é¥¼å›¾\n",
        "ax1.pie(failure_analysis.values, labels=failure_analysis.index, autopct='%1.1f%%', \n",
        "        colors=colors[:len(failure_analysis)])\n",
        "ax1.set_title('å¤±è´¥åŸå› åˆ†å¸ƒ', fontsize=14, fontweight='bold')\n",
        "\n",
        "# å„æ¨¡å‹çš„è¿è§„ç‡å¯¹æ¯”\n",
        "violation_by_model = merged_df.groupby('hinter_model_clean')['has_taboo_violation'].mean()\n",
        "bars = ax2.bar(violation_by_model.index, violation_by_model.values, \n",
        "               color=colors[:len(violation_by_model)])\n",
        "ax2.set_title('å„æ¨¡å‹è¿è§„ç‡å¯¹æ¯”', fontsize=14, fontweight='bold')\n",
        "ax2.set_ylabel('è¿è§„ç‡')\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
        "             f'{height:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# éš¾åº¦åˆ†æ - åŸºäºæ¸¸æˆè½®æ•°çš„è¯æ±‡éš¾åº¦åˆ†å¸ƒ\n",
        "successful_games = merged_df[merged_df['success'] == True]\n",
        "word_difficulty = successful_games.groupby('target_word')['turns_used'].agg(['mean', 'count']).reset_index()\n",
        "word_difficulty = word_difficulty[word_difficulty['count'] >= 2]  # è‡³å°‘è¢«æµ‹è¯•2æ¬¡çš„è¯æ±‡\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# è¯æ±‡éš¾åº¦åˆ†å¸ƒç›´æ–¹å›¾\n",
        "ax1.hist(word_difficulty['mean'], bins=20, color=colors[0], alpha=0.7, edgecolor='black')\n",
        "ax1.set_title('è¯æ±‡éš¾åº¦åˆ†å¸ƒï¼ˆåŸºäºå¹³å‡è½®æ•°ï¼‰', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('å¹³å‡è½®æ•°')\n",
        "ax1.set_ylabel('è¯æ±‡æ•°é‡')\n",
        "ax1.axvline(word_difficulty['mean'].mean(), color='red', linestyle='--', \n",
        "            label=f'å¹³å‡éš¾åº¦: {word_difficulty[\"mean\"].mean():.2f}è½®')\n",
        "ax1.legend()\n",
        "\n",
        "# æˆåŠŸç‡ä¸å¹³å‡è½®æ•°çš„å…³ç³»\n",
        "model_turns_success = merged_df.groupby('hinter_model_clean').agg({\n",
        "    'success': 'mean',\n",
        "    'turns_used': lambda x: x[merged_df.loc[x.index, 'success']].mean()\n",
        "})\n",
        "\n",
        "ax2.scatter(model_turns_success['turns_used'], model_turns_success['success'], \n",
        "           s=100, color=colors[:len(model_turns_success)], alpha=0.7)\n",
        "\n",
        "for i, (model, data) in enumerate(model_turns_success.iterrows()):\n",
        "    ax2.annotate(model, (data['turns_used'], data['success']), \n",
        "                xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
        "\n",
        "ax2.set_title('æˆåŠŸç‡ä¸æ•ˆç‡çš„å…³ç³»', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('å¹³å‡è½®æ•°ï¼ˆæˆåŠŸæ¡ˆä¾‹ï¼‰')\n",
        "ax2.set_ylabel('æˆåŠŸç‡')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "from itertools import combinations\n",
        "\n",
        "# å¯¹æ¨¡å‹é—´æˆåŠŸç‡è¿›è¡Œç»Ÿè®¡æ£€éªŒ\n",
        "models = merged_df['hinter_model_clean'].unique()\n",
        "model_success_data = {}\n",
        "\n",
        "for model in models:\n",
        "    model_data = merged_df[merged_df['hinter_model_clean'] == model]['success']\n",
        "    model_success_data[model] = model_data\n",
        "\n",
        "print(\"æ¨¡å‹é—´æˆåŠŸç‡å·®å¼‚çš„ç»Ÿè®¡æ£€éªŒ (Chi-square test):\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "results_matrix = pd.DataFrame(index=models, columns=models, dtype=float)\n",
        "\n",
        "for model1, model2 in combinations(models, 2):\n",
        "    # åˆ›å»ºåˆ—è”è¡¨\n",
        "    data1 = model_success_data[model1]\n",
        "    data2 = model_success_data[model2]\n",
        "    \n",
        "    contingency_table = pd.crosstab(\n",
        "        pd.concat([data1, data2]), \n",
        "        pd.concat([pd.Series([model1]*len(data1)), pd.Series([model2]*len(data2))])\n",
        "    )\n",
        "    \n",
        "    chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table)\n",
        "    \n",
        "    results_matrix.loc[model1, model2] = p_value\n",
        "    results_matrix.loc[model2, model1] = p_value\n",
        "    \n",
        "    significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
        "    print(f\"{model1} vs {model2}: p = {p_value:.4f} {significance}\")\n",
        "\n",
        "print(\"\\n*** p < 0.001, ** p < 0.01, * p < 0.05, ns = not significant\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 9. æ€»ç»“æŠ¥å‘Š\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç”Ÿæˆæ€»ç»“æŠ¥å‘Š\n",
        "print(\"=\"*80)\n",
        "print(\"                          TABOOæ¸¸æˆå®éªŒæ€»ç»“æŠ¥å‘Š\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nğŸ“Š å®éªŒè§„æ¨¡:\")\n",
        "print(f\"  â€¢ æ€»æ¸¸æˆæ•°: {len(merged_df):,}\")\n",
        "print(f\"  â€¢ å‚ä¸æ¨¡å‹: {len(models)} ä¸ª\")\n",
        "print(f\"  â€¢ æµ‹è¯•è¯æ±‡: {merged_df['target_word'].nunique():,} ä¸ª\")\n",
        "print(f\"  â€¢ æ€»ä½“æˆåŠŸç‡: {merged_df['success'].mean():.1%}\")\n",
        "\n",
        "print(f\"\\nğŸ† æ¨¡å‹æ’å (æŒ‰æˆåŠŸç‡):\")\n",
        "model_ranking = model_success.sort_values('æˆåŠŸç‡', ascending=False)\n",
        "for i, (model, data) in enumerate(model_ranking.iterrows(), 1):\n",
        "    print(f\"  {i}. {model}: {data['æˆåŠŸç‡']:.1%} (å¹³å‡{data['å¹³å‡è½®æ•°']:.1f}è½®)\")\n",
        "\n",
        "print(f\"\\nâš¡ æ•ˆç‡åˆ†æ:\")\n",
        "first_turn_ranking = turn_success_df.sort_values('ç¬¬1è½®æˆåŠŸç‡', ascending=False)\n",
        "print(f\"  â€¢ ç¬¬1è½®æˆåŠŸç‡æœ€é«˜: {first_turn_ranking.index[0]} ({first_turn_ranking.iloc[0]['ç¬¬1è½®æˆåŠŸç‡']:.1%})\")\n",
        "print(f\"  â€¢ å‰3è½®ç´¯ç§¯æˆåŠŸç‡æœ€é«˜: {cumulative_df['å‰3è½®ç´¯ç§¯æˆåŠŸç‡'].idxmax()} ({cumulative_df['å‰3è½®ç´¯ç§¯æˆåŠŸç‡'].max():.1%})\")\n",
        "print(f\"  â€¢ å¹³å‡è½®æ•°æœ€å°‘: {model_success.sort_values('å¹³å‡è½®æ•°').index[0]} ({model_success.sort_values('å¹³å‡è½®æ•°').iloc[0]['å¹³å‡è½®æ•°']:.1f}è½®)\")\n",
        "\n",
        "print(f\"\\nğŸ“ è¯æ€§åˆ†æ:\")\n",
        "if 'part_of_speech' in merged_df.columns:\n",
        "    pos_overall = merged_df.groupby('part_of_speech')['success'].mean().sort_values(ascending=False)\n",
        "    print(f\"  â€¢ æœ€å®¹æ˜“çš„è¯æ€§: {pos_overall.index[0]} ({pos_overall.iloc[0]:.1%})\")\n",
        "    print(f\"  â€¢ æœ€å›°éš¾çš„è¯æ€§: {pos_overall.index[-1]} ({pos_overall.iloc[-1]:.1%})\")\n",
        "\n",
        "print(f\"\\nğŸ¯ æŠ½è±¡ç¨‹åº¦åˆ†æ:\")\n",
        "if len(concrete_df) > 0:\n",
        "    concrete_overall = concrete_df.groupby('concreteness_level')['success'].mean().sort_values(ascending=False)\n",
        "    print(f\"  â€¢ æœ€å®¹æ˜“çš„æŠ½è±¡ç¨‹åº¦: {concrete_overall.index[0]} ({concrete_overall.iloc[0]:.1%})\")\n",
        "    print(f\"  â€¢ æœ€å›°éš¾çš„æŠ½è±¡ç¨‹åº¦: {concrete_overall.index[-1]} ({concrete_overall.iloc[-1]:.1%})\")\n",
        "\n",
        "print(f\"\\nâš ï¸  è§„åˆ™éµå®ˆ:\")\n",
        "violation_ranking = merged_df.groupby('hinter_model_clean')['has_taboo_violation'].mean().sort_values()\n",
        "print(f\"  â€¢ è¿è§„ç‡æœ€ä½: {violation_ranking.index[0]} ({violation_ranking.iloc[0]:.1%})\")\n",
        "print(f\"  â€¢ è¿è§„ç‡æœ€é«˜: {violation_ranking.index[-1]} ({violation_ranking.iloc[-1]:.1%})\")\n",
        "\n",
        "print(f\"\\nğŸ” å…³é”®å‘ç°:\")\n",
        "best_model = model_ranking.index[0]\n",
        "best_success_rate = model_ranking.iloc[0]['æˆåŠŸç‡']\n",
        "worst_model = model_ranking.index[-1]\n",
        "worst_success_rate = model_ranking.iloc[-1]['æˆåŠŸç‡']\n",
        "\n",
        "print(f\"  â€¢ {best_model} è¡¨ç°æœ€ä½³ï¼ŒæˆåŠŸç‡è¾¾åˆ° {best_success_rate:.1%}\")\n",
        "print(f\"  â€¢ æœ€ä½³ä¸æœ€å·®æ¨¡å‹çš„æˆåŠŸç‡å·®è·ä¸º {best_success_rate - worst_success_rate:.1%}\")\n",
        "print(f\"  â€¢ å¹³å‡æ¸¸æˆè½®æ•°ä¸º {merged_df[merged_df['success']]['turns_used'].mean():.1f} è½®\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ä¿å­˜å…³é”®ç»“æœåˆ°æ–‡ä»¶\n",
        "import datetime\n",
        "\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# ä¿å­˜æ¨¡å‹æ€§èƒ½æ€»ç»“\n",
        "model_summary = model_success.copy()\n",
        "model_summary['ç¬¬1è½®æˆåŠŸç‡'] = model_summary.index.map(turn_success_df['ç¬¬1è½®æˆåŠŸç‡'])\n",
        "model_summary['å‰3è½®ç´¯ç§¯æˆåŠŸç‡'] = model_summary.index.map(cumulative_df['å‰3è½®ç´¯ç§¯æˆåŠŸç‡'])\n",
        "model_summary['è¿è§„ç‡'] = model_summary.index.map(violation_by_model)\n",
        "\n",
        "model_summary.to_csv(f'analysis_results_{timestamp}.csv', encoding='utf-8')\n",
        "print(f\"\\nâœ… åˆ†æç»“æœå·²ä¿å­˜åˆ°: analysis_results_{timestamp}.csv\")\n",
        "\n",
        "print(\"\\nğŸ‰ æ•°æ®åˆ†æå®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ: 300 æ¡è®°å½•\n",
            "ğŸ“ æ•°æ®é›†è·¯å¾„: data/dataset.json\n",
            "\n",
            "ğŸ“‹ æ•°æ®æ ·æœ¬:\n",
            "   ç›®æ ‡è¯: regent\n",
            "   ç¦ç”¨è¯: ['board', 'members', 'trustee', 'committee', 'governing']\n",
            "   ç±»åˆ«: general\n",
            "   å®šä¹‰: members of a governing board...\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "import time\n",
        "import requests\n",
        "import pandas as pd\n",
        "from typing import Dict, List, Any\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# åŠ è½½æ•°æ®é›†\n",
        "def load_dataset(dataset_path: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"åŠ è½½Tabooæ¸¸æˆæ•°æ®é›†\"\"\"\n",
        "    with open(dataset_path, 'r', encoding='utf-8') as f:\n",
        "        dataset = json.load(f)\n",
        "    return dataset\n",
        "\n",
        "# åŠ è½½é¢„ç”Ÿæˆçš„æ•°æ®é›†\n",
        "DATASET_PATH = \"data/dataset.json\"\n",
        "dataset = load_dataset(DATASET_PATH)\n",
        "print(f\"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ: {len(dataset)} æ¡è®°å½•\")\n",
        "print(f\"ğŸ“ æ•°æ®é›†è·¯å¾„: {DATASET_PATH}\")\n",
        "\n",
        "# æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬\n",
        "if dataset:\n",
        "    sample = dataset[0]\n",
        "    print(f\"\\nğŸ“‹ æ•°æ®æ ·æœ¬:\")\n",
        "    print(f\"   ç›®æ ‡è¯: {sample['target']}\")\n",
        "    print(f\"   ç¦ç”¨è¯: {sample['taboo']}\")\n",
        "    print(f\"   ç±»åˆ«: {sample.get('category', 'N/A')}\")\n",
        "    if sample.get('senses'):\n",
        "        print(f\"   å®šä¹‰: {sample['senses'][0].get('definition', 'N/A')[:100]}...\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
