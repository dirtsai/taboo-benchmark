{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 数据集生成脚本 V35 (完整集成版本) ===\n",
            "🔧 修复路径问题 + 🚀 增强算法 + 🤖 LLM智能补充\n",
            "✅ LLM API 已启用\n",
            "\n",
            "⏳ 步骤一：正在强制检查并下载所有必需的NLTK数据包...\n",
            "  - 下载中: 'wordnet'...\n",
            "  - 下载中: 'omw-1.4'...\n",
            "✅ NLTK依赖项已准备就绪！\n",
            "\n",
            "📦 步骤二：正在加载所有原材料（修复后的路径）...\n",
            "  - ✅ CHEMISTRY: 成功加载并解析了 1840 个候选词。\n",
            "  - ✅ CS: 成功加载并解析了 206 个候选词。\n",
            "  - ✅ FINANCE: 成功加载并解析了 110 个候选词。\n",
            "  - ✅ PHILOSOPHY: 成功加载并解析了 530 个候选词。\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /Users/czl/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /Users/czl/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  - ✅ GENERAL: 候选池已准备好 (71909 个词)。\n",
            "  - ✅ CONCRETENESS: 分数数据加载成功。\n",
            "\n",
            "🎯 步骤三：正在进行智能抽样（实时LLM质量检查）...\n",
            "\n",
            "🔍 正在为 'general' 类别智能选择 100 个词汇:\n",
            "    ✅ 接受: crotonbug\n",
            "    ✅ 接受: entitlement\n",
            "    ✅ 接受: hinge\n",
            "    ✅ 接受: cubiform\n",
            "    ✅ 接受: mantle\n",
            "    ❌ 拒绝: bullshot - 不合适:专有名词\n",
            "    ✅ 接受: evening\n",
            "    ❌ 拒绝: polybotria - 不合适:原因：可能是专有名词或虚构词汇，缺乏普遍认可的定义。\n",
            "    ✅ 接受: clitoris\n",
            "    ❌ 拒绝: zoisia - 不合适:原因\n",
            "    ✅ 接受: velcro\n",
            "    ... (后续拒绝的词汇不再显示)\n",
            "    ✅ 接受: trinuclear\n",
            "    ✅ 接受: promote\n",
            "    ✅ 接受: flagellation\n",
            "    ✅ 接受: marlinespike\n",
            "    ✅ 接受: pandemic\n",
            "    ✅ 接受: cistaceae\n",
            "    ✅ 接受: jacobinic\n",
            "  📊 'general' 完成: 成功选择 100/100, 检查了 116 个候选词, 拒绝了 16 个\n",
            "\n",
            "🔍 正在为 'chemistry' 类别智能选择 50 个词汇:\n",
            "    ✅ 接受: fumigation\n",
            "    ✅ 接受: tellurides\n",
            "    ✅ 接受: organization\n",
            "    ✅ 接受: conceptus\n",
            "    ✅ 接受: phenols\n",
            "    ❌ 拒绝: sperm - 不合适:色情内容\n",
            "    ✅ 接受: texture\n",
            "    ❌ 拒绝: microphallus - 不合适:色情内容\n",
            "    ✅ 接受: atomization\n",
            "    ✅ 接受: weber\n",
            "    ✅ 接受: zone\n",
            "    ✅ 接受: allometric\n",
            "  📊 'chemistry' 完成: 成功选择 50/50, 检查了 52 个候选词, 拒绝了 2 个\n",
            "\n",
            "🔍 正在为 'cs' 类别智能选择 50 个词汇:\n",
            "    ✅ 接受: stylus\n",
            "    ✅ 接受: software\n",
            "    ✅ 接受: binary\n",
            "    ✅ 接受: identifier\n",
            "    ✅ 接受: stack\n",
            "    ✅ 接受: socket\n",
            "    ✅ 接受: selection\n",
            "    ❌ 拒绝: terminator - 不合适:专有名词\n",
            "    ✅ 接受: syntax\n",
            "    ✅ 接受: property\n",
            "    ✅ 接受: style\n",
            "  📊 'cs' 完成: 成功选择 50/50, 检查了 51 个候选词, 拒绝了 1 个\n",
            "\n",
            "🔍 正在为 'finance' 类别智能选择 50 个词汇:\n",
            "    ✅ 接受: macroeconomics\n",
            "    ✅ 接受: entrepreneur\n",
            "    ✅ 接受: economics\n",
            "    ✅ 接受: liquidity\n",
            "    ✅ 接受: environmental\n",
            "    ✅ 接受: nominal\n",
            "    ✅ 接受: asset\n",
            "    ✅ 接受: novation\n",
            "    ❌ 拒绝: jensen - 不合适:人名\n",
            "    ❌ 拒绝: moore - 不合适:人名\n",
            "    ✅ 接受: upside\n",
            "    ✅ 接受: guarantor\n",
            "  📊 'finance' 完成: 成功选择 50/50, 检查了 52 个候选词, 拒绝了 2 个\n",
            "\n",
            "🔍 正在为 'philosophy' 类别智能选择 50 个词汇:\n",
            "    ❌ 拒绝: camus - 不合适:人名\n",
            "    ✅ 接受: combinatory\n",
            "    ✅ 接受: abduction\n",
            "    ❌ 拒绝: ross - 不合适:专有名词\n",
            "    ✅ 接受: pacifism\n",
            "    ❌ 拒绝: abortion - 不合适:涉及严重敏感内容\n",
            "    ✅ 接受: methods\n",
            "    ✅ 接受: prior\n",
            "    ... (后续拒绝的词汇不再显示)\n",
            "    ✅ 接受: discrimination\n",
            "    ✅ 接受: myths\n",
            "    ✅ 接受: humor\n",
            "    ✅ 接受: death\n",
            "    ✅ 接受: conservatism\n",
            "  📊 'philosophy' 完成: 成功选择 50/50, 检查了 67 个候选词, 拒绝了 17 个\n",
            "\n",
            "✅ 智能抽样完成! (集成了实时LLM质量检查)\n",
            "\n",
            "🏭 步骤四：正在为 300 个词进行最终数据丰富化...\n",
            "🚀 使用增强版算法 + LLM智能补充\n",
            "  - 丰富化进度 50/300: promote\n",
            "  - 丰富化进度 100/300: jacobinic\n",
            "  - 丰富化进度 150/300: allometric\n",
            "  - 丰富化进度 200/300: style\n",
            "  - 丰富化进度 250/300: guarantor\n",
            "  - 丰富化进度 300/300: conservatism\n",
            "\n",
            "--- 数据丰富化完成 ---\n",
            "✅✅✅ 您的增强版数据集已生成！文件名为: 'ENHANCED_DATASET.json'\n",
            "总词汇数: 300\n",
            "\n",
            "📊 Taboo Word 生成统计:\n",
            "  - 成功生成足够taboo words (≥5个): 300 个词汇\n",
            "  - LLM成功补充: 0 个词汇\n",
            "  - 总LLM调用次数: 0\n",
            "  - 仍然不足: 0 个词汇\n",
            "\n",
            "--- 最终文件类别分布自检 ---\n",
            "category\n",
            "general       100\n",
            "chemistry      50\n",
            "cs             50\n",
            "finance        50\n",
            "philosophy     50\n",
            "Name: count, dtype: int64\n",
            "\n",
            "📈 最终质量报告:\n",
            "  - 平均 taboo word 数量: 5.00\n",
            "  - 成功率 (≥5个taboo words): 100.0%\n",
            "\n",
            "🏆 恭喜！增强版项目已准备就绪！\n"
          ]
        }
      ],
      "source": [
        "# 数据集生成脚本 V35 - 完整集成版本 (修复路径 + 增强算法 + LLM智能补充)\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import random\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import requests\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "print(\"=== 数据集生成脚本 V35 (完整集成版本) ===\")\n",
        "print(\"🔧 修复路径问题 + 🚀 增强算法 + 🤖 LLM智能补充\")\n",
        "\n",
        "# =============================================================\n",
        "# LLM API 调用功能\n",
        "# =============================================================\n",
        "\n",
        "def load_api_key():\n",
        "    \"\"\"加载API密钥\"\"\"\n",
        "    try:\n",
        "        with open('api_keys.json', 'r') as f:\n",
        "            api_keys = json.load(f)\n",
        "        return api_keys['OPENROUTER_API_KEY']\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ 无法加载API密钥: {e}\")\n",
        "        return None\n",
        "\n",
        "def call_openrouter_api(prompt, api_key, max_retries=3):\n",
        "    \"\"\"调用OpenRouter API (DeepSeek V3)\"\"\"\n",
        "    if not api_key:\n",
        "        return None\n",
        "        \n",
        "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": \"deepseek/deepseek-chat-v3-0324:free\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "        \"max_tokens\": 150,\n",
        "        \"temperature\": 0.7\n",
        "    }\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.post(url, headers=headers, json=data, timeout=30)\n",
        "            if response.status_code == 200:\n",
        "                result = response.json()\n",
        "                return result['choices'][0]['message']['content'].strip()\n",
        "            else:\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(2 ** attempt)\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(2 ** attempt)\n",
        "    return None\n",
        "\n",
        "def generate_llm_taboo_words(target_word, existing_taboos, part_of_speech, definition, api_key):\n",
        "    \"\"\"使用LLM生成补充的taboo words\"\"\"\n",
        "    needed_count = 5 - len(existing_taboos)\n",
        "    if needed_count <= 0 or not api_key:\n",
        "        return []\n",
        "    \n",
        "    existing_str = \", \".join(existing_taboos) if existing_taboos else \"无\"\n",
        "    prompt = f\"\"\"为Taboo游戏生成禁用词。\n",
        "\n",
        "目标词: {target_word}\n",
        "词性: {part_of_speech}  \n",
        "定义: {definition}\n",
        "已有禁用词: {existing_str}\n",
        "\n",
        "请为\"{target_word}\"生成{needed_count}个新的禁用词，要求：\n",
        "1. 与目标词语义相关但不能直接说出\n",
        "2. 不能重复已有禁用词  \n",
        "3. 优先选择同义词、相关概念、上下位词\n",
        "4. 每个词用英文，单个词汇，小写\n",
        "5. 避免过于通用的词\n",
        "\n",
        "只输出{needed_count}个英文单词，用逗号分隔，不要其他内容。\"\"\"\n",
        "\n",
        "    response = call_openrouter_api(prompt, api_key)\n",
        "    if response:\n",
        "        words = [w.strip().lower() for w in response.split(',')]\n",
        "        valid_words = []\n",
        "        for word in words:\n",
        "            if (word.isalpha() and len(word) > 2 and \n",
        "                word not in existing_taboos and \n",
        "                word != target_word.lower() and\n",
        "                len(valid_words) < needed_count):\n",
        "                valid_words.append(word)\n",
        "        return valid_words\n",
        "    return []\n",
        "\n",
        "# =============================================================\n",
        "# 主要执行脚本 - 完整集成版本\n",
        "# =============================================================\n",
        "\n",
        "try:\n",
        "    # 加载API密钥\n",
        "    OPENROUTER_API_KEY = load_api_key()\n",
        "    llm_available = OPENROUTER_API_KEY is not None\n",
        "    if llm_available:\n",
        "        print(\"✅ LLM API 已启用\")\n",
        "    else:\n",
        "        print(\"⚠️ LLM API 未启用，将仅使用传统算法\")\n",
        "\n",
        "    # --- 步骤一：强制下载依赖 ---\n",
        "    packages_to_download = [\n",
        "        'wordnet', 'omw-1.4', 'punkt', \n",
        "        'averaged_perceptron_tagger', 'averaged_perceptron_tagger_eng',\n",
        "        'stopwords'\n",
        "    ]\n",
        "    print(\"\\n⏳ 步骤一：正在强制检查并下载所有必需的NLTK数据包...\")\n",
        "    for package in packages_to_download:\n",
        "        try:\n",
        "            # 尝试多种可能的路径模式\n",
        "            found = False\n",
        "            for path_pattern in [f'corpora/{package}', f'tokenizers/{package}', f'taggers/{package}']:\n",
        "                try:\n",
        "                    nltk.data.find(path_pattern)\n",
        "                    found = True\n",
        "                    break\n",
        "                except LookupError:\n",
        "                    continue\n",
        "            \n",
        "            if not found:\n",
        "                print(f\"  - 下载中: '{package}'...\")\n",
        "                nltk.download(package, quiet=False)\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"  - 尝试下载 '{package}': {e}\")\n",
        "            try:\n",
        "                nltk.download(package, quiet=False)\n",
        "            except:\n",
        "                print(f\"  - 跳过 '{package}' (可能不影响核心功能)\")\n",
        "    print(\"✅ NLTK依赖项已准备就绪！\")\n",
        "\n",
        "    # --- 步骤二：加载数据源（修复路径）---\n",
        "    print(\"\\n📦 步骤二：正在加载所有原材料（修复后的路径）...\")\n",
        "    raw_data_path = \"data/raw data/\"\n",
        "    \n",
        "    # 专有名词黑名单（常见的不适合Taboo的词汇）\n",
        "    proper_noun_blacklist = {\n",
        "        'jefferson', 'washington', 'lincoln', 'kennedy', 'roosevelt', 'adams', 'madison',\n",
        "        'aristotle', 'plato', 'socrates', 'kant', 'hegel', 'nietzsche', 'descartes',\n",
        "        'shakespeare', 'dickens', 'darwin', 'newton', 'einstein', 'galileo',\n",
        "        'america', 'europe', 'asia', 'africa', 'australia', 'antarctica',\n",
        "        'london', 'paris', 'tokyo', 'beijing', 'moscow', 'berlin',\n",
        "        'christianity', 'buddhism', 'hinduism', 'islam', 'judaism',\n",
        "        'english', 'chinese', 'japanese', 'german', 'french', 'spanish',\n",
        "        'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday',\n",
        "        'january', 'february', 'march', 'april', 'may', 'june',\n",
        "        'july', 'august', 'september', 'october', 'november', 'december'\n",
        "    }\n",
        "    \n",
        "    def parse_chemistry_json(filename):\n",
        "        full_path = os.path.join(raw_data_path, filename)\n",
        "        with open(full_path, 'r', encoding='utf-8') as f: \n",
        "            data = json.load(f)\n",
        "        words = set()\n",
        "        term_dict = data.get('terms', {}).get('list', {})\n",
        "        for key, value in term_dict.items():\n",
        "            if isinstance(value, dict):\n",
        "                term = value.get('title', '').lower()\n",
        "                if term: words.add(term)\n",
        "        return words\n",
        "\n",
        "    def parse_cs_json(filename):\n",
        "        full_path = os.path.join(raw_data_path, filename)\n",
        "        with open(full_path, 'r', encoding='utf-8') as f: \n",
        "            data = json.load(f)\n",
        "        words = set()\n",
        "        term_list = data.get('results', [])\n",
        "        for entry in term_list:\n",
        "            term = entry.get('value', '').lower().split('(')[0].strip()\n",
        "            if term: words.add(term)\n",
        "        return words\n",
        "\n",
        "    def parse_text_file(filename):\n",
        "        full_path = os.path.join(raw_data_path, filename)\n",
        "        with open(full_path, 'r', encoding='utf-8') as f:\n",
        "            words = set()\n",
        "            for line in f:\n",
        "                match = re.match(r'^[a-zA-Z\\s_-]+', line.strip())\n",
        "                if match: words.add(match.group(0).strip().lower())\n",
        "        return words\n",
        "\n",
        "    file_parsers = {\n",
        "        'chemistry': ('chemistry_terms.json', parse_chemistry_json), \n",
        "        'cs': ('cs_terms.json', parse_cs_json), \n",
        "        'finance': ('finance_terms.txt', parse_text_file), \n",
        "        'philosophy': ('philosophy_terms.txt', parse_text_file)\n",
        "    }\n",
        "    \n",
        "    professional_pools = {}\n",
        "    for category, (filename, parser_func) in file_parsers.items():\n",
        "        full_path = os.path.join(raw_data_path, filename)\n",
        "        if os.path.exists(full_path):\n",
        "            word_set = parser_func(filename)\n",
        "            # 更严格的词汇筛选：排除专有名词和不合适的词汇\n",
        "            def is_valid_word(word):\n",
        "                # 基本长度和字母检查\n",
        "                if not (word.isalpha() and 3 < len(word) < 25):\n",
        "                    return False\n",
        "                \n",
        "                # 检查专有名词黑名单\n",
        "                if word.lower() in proper_noun_blacklist:\n",
        "                    return False\n",
        "                \n",
        "                # 排除首字母大写的词（通常是专有名词）\n",
        "                if word[0].isupper():\n",
        "                    return False\n",
        "                \n",
        "                # 排除全大写的词\n",
        "                if word.isupper():\n",
        "                    return False\n",
        "                \n",
        "                # 排除包含常见专有名词标识的词\n",
        "                proper_noun_indicators = ['ism', 'ist', 'ian', 'ese', 'ic']\n",
        "                if any(word.endswith(indicator) for indicator in proper_noun_indicators):\n",
        "                    # 检查是否真的是专有名词相关\n",
        "                    synsets = wn.synsets(word)\n",
        "                    if synsets:\n",
        "                        # 检查词性，如果主要是名词且可能是专有名词，跳过\n",
        "                        pos_tags = [s.pos() for s in synsets]\n",
        "                        if 'n' in pos_tags:\n",
        "                            # 进一步检查定义中是否包含专有名词特征\n",
        "                            definitions = [s.definition().lower() for s in synsets]\n",
        "                            proper_keywords = ['person', 'people', 'country', 'place', 'city', 'name', 'founder', 'philosopher', 'scientist']\n",
        "                            if any(keyword in ' '.join(definitions) for keyword in proper_keywords):\n",
        "                                return False\n",
        "                \n",
        "                # 确保词汇在WordNet中存在且有有用的同义词集\n",
        "                synsets = wn.synsets(word)\n",
        "                if not synsets:\n",
        "                    return False\n",
        "                \n",
        "                # 确保至少有一个合适的词性（名词、动词、形容词）\n",
        "                valid_pos = {'n', 'v', 'a', 's'}  # noun, verb, adjective, satellite adjective\n",
        "                if not any(s.pos() in valid_pos for s in synsets):\n",
        "                    return False\n",
        "                \n",
        "                return True\n",
        "            \n",
        "            cleaned_words = {w for w in word_set if is_valid_word(w)}\n",
        "            professional_pools[category] = list(cleaned_words)\n",
        "            print(f\"  - ✅ {category.upper()}: 成功加载并解析了 {len(cleaned_words)} 个候选词。\")\n",
        "        else:\n",
        "            print(f\"  - ⚠️ 文件未找到: {full_path}，将跳过。\")\n",
        "            professional_pools[category] = []\n",
        "    \n",
        "    # 应用同样的严格筛选到通用词汇池\n",
        "    def is_valid_general_word(word):\n",
        "        # 基本条件\n",
        "        if not (word.isalpha() and '_' not in word and 3 < len(word) < 16):\n",
        "            return False\n",
        "        \n",
        "        # 检查专有名词黑名单\n",
        "        if word.lower() in proper_noun_blacklist:\n",
        "            return False\n",
        "        \n",
        "        # 排除首字母大写的词（通常是专有名词）\n",
        "        if word[0].isupper():\n",
        "            return False\n",
        "        \n",
        "        # 排除全大写的词\n",
        "        if word.isupper():\n",
        "            return False\n",
        "        \n",
        "        # 确保在WordNet中存在且有合适的词性\n",
        "        synsets = wn.synsets(word)\n",
        "        if not synsets:\n",
        "            return False\n",
        "            \n",
        "        valid_pos = {'n', 'v', 'a', 's'}  # noun, verb, adjective, satellite adjective\n",
        "        if not any(s.pos() in valid_pos for s in synsets):\n",
        "            return False\n",
        "            \n",
        "        return True\n",
        "    \n",
        "    general_pool = [w for w in wn.words() if is_valid_general_word(w)]\n",
        "    print(f\"  - ✅ GENERAL: 候选池已准备好 ({len(general_pool)} 个词)。\")\n",
        "    \n",
        "    # 加载具体性数据\n",
        "    concreteness_path = os.path.join(raw_data_path, \"concreteness_data.xlsx\")\n",
        "    if os.path.exists(concreteness_path):\n",
        "        concreteness_df = pd.read_excel(concreteness_path)\n",
        "        concreteness_lookup = pd.Series(concreteness_df['Conc.M'].values, index=concreteness_df['Word'].str.lower()).to_dict()\n",
        "        print(f\"  - ✅ CONCRETENESS: 分数数据加载成功。\")\n",
        "    else:\n",
        "        concreteness_lookup = {}\n",
        "        print(f\"  - ⚠️ 具体性数据文件未找到，将跳过。\")\n",
        "\n",
        "    # --- 步骤三：智能抽样（集成实时LLM质量检查）---\n",
        "    print(\"\\n🎯 步骤三：正在进行智能抽样（实时LLM质量检查）...\")\n",
        "    quotas = {'general': 100, 'chemistry': 50, 'cs': 50, 'finance': 50, 'philosophy': 50}\n",
        "    selected_words = {}\n",
        "    word_tracker = set()\n",
        "    all_pools = {'general': general_pool, **professional_pools}\n",
        "    \n",
        "    def check_single_word_appropriateness(word, category, api_key):\n",
        "        \"\"\"检查单个词汇是否适合\"\"\"\n",
        "        if not api_key:\n",
        "            return True, \"API不可用，跳过检查\"  # 如果没有API，跳过LLM检查\n",
        "        \n",
        "        prompt = f\"\"\"请检查词汇\"{word}\"是否适合用于Taboo游戏。只需要检查两个标准：\n",
        "1. 不是专有名词（人名、地名、品牌名、特定机构名等）\n",
        "2. 不涉及严重敏感内容（仅限：极端暴力、色情内容、种族歧视、极端政治敏感）\n",
        "\n",
        "注意：医学术语、疾病名称、人体解剖学术语、法律术语、商业术语、学术概念、专业技术术语、生僻词汇都是可以接受的。\n",
        "\n",
        "词汇：{word} (类别：{category})\n",
        "\n",
        "请只返回\"合适\"或\"不合适:原因\"。\"\"\"\n",
        "        \n",
        "        response = call_openrouter_api(prompt, api_key)\n",
        "        if response:\n",
        "            response = response.strip()\n",
        "            if \"不合适\" in response:\n",
        "                return False, response\n",
        "            else:\n",
        "                return True, \"合适\"\n",
        "        return True, \"API检查失败，默认接受\"  # API失败时默认接受\n",
        "    \n",
        "    for category, num_to_select in quotas.items():\n",
        "        pool = all_pools.get(category, [])\n",
        "        if not pool: \n",
        "            print(f\"  - ⚠️ 类别 '{category}' 的候选池为空, 无法抽样。\")\n",
        "            continue\n",
        "        \n",
        "        print(f\"\\n🔍 正在为 '{category}' 类别智能选择 {num_to_select} 个词汇:\")\n",
        "        \n",
        "        # 基础筛选：确保所有词汇都适合Taboo游戏\n",
        "        def is_suitable_for_taboo(word):\n",
        "            # 再次检查基本条件\n",
        "            if not word.isalpha() or len(word) <= 3:\n",
        "                return False\n",
        "            \n",
        "            # 检查是否在专有名词黑名单中\n",
        "            if word.lower() in proper_noun_blacklist:\n",
        "                return False\n",
        "            \n",
        "            # 确保不是首字母大写（专有名词）\n",
        "            if word[0].isupper():\n",
        "                return False\n",
        "                \n",
        "            synsets = wn.synsets(word)\n",
        "            if not synsets:\n",
        "                return False\n",
        "            \n",
        "            # 检查定义中是否包含明显的专有名词特征\n",
        "            for synset in synsets[:2]:  # 检查前两个最常见的含义\n",
        "                definition = synset.definition().lower()\n",
        "                if any(keyword in definition for keyword in ['person named', 'named after', 'proper name', 'surname', 'given name']):\n",
        "                    return False\n",
        "            \n",
        "            return True\n",
        "        \n",
        "        pool = [w for w in pool if wn.synsets(w) and is_suitable_for_taboo(w)]\n",
        "        random.shuffle(pool)\n",
        "        count = 0\n",
        "        checked_count = 0\n",
        "        rejected_count = 0\n",
        "        \n",
        "        for word in pool:\n",
        "            if count >= num_to_select: \n",
        "                break\n",
        "            if word in word_tracker:\n",
        "                continue\n",
        "                \n",
        "            checked_count += 1\n",
        "            \n",
        "            # 实时LLM质量检查\n",
        "            if llm_available:\n",
        "                is_appropriate, reason = check_single_word_appropriateness(word, category, OPENROUTER_API_KEY)\n",
        "                if not is_appropriate:\n",
        "                    rejected_count += 1\n",
        "                    if rejected_count <= 3:  # 只显示前3个被拒绝的词汇\n",
        "                        print(f\"    ❌ 拒绝: {word} - {reason}\")\n",
        "                    elif rejected_count == 4:\n",
        "                        print(f\"    ... (后续拒绝的词汇不再显示)\")\n",
        "                    continue\n",
        "            \n",
        "            # 接受这个词汇\n",
        "            selected_words[word] = category\n",
        "            word_tracker.add(word)\n",
        "            count += 1\n",
        "            \n",
        "            if count <= 5 or count % 10 == 0:  # 显示前5个和每10个\n",
        "                print(f\"    ✅ 接受: {word}\")\n",
        "        \n",
        "        print(f\"  📊 '{category}' 完成: 成功选择 {count}/{num_to_select}, 检查了 {checked_count} 个候选词, 拒绝了 {rejected_count} 个\")\n",
        "        \n",
        "        # 如果数量不足，从剩余池中补充（不使用LLM检查）\n",
        "        if count < num_to_select:\n",
        "            needed = num_to_select - count\n",
        "            remaining_pool = [w for w in pool if w not in word_tracker and is_suitable_for_taboo(w)]\n",
        "            random.shuffle(remaining_pool)\n",
        "            for word in remaining_pool[:needed]:\n",
        "                selected_words[word] = category\n",
        "                word_tracker.add(word)\n",
        "                count += 1\n",
        "            print(f\"  🔄 补充了 {min(needed, len(remaining_pool))} 个词汇（跳过LLM检查）\")\n",
        "    \n",
        "    print(\"\\n✅ 智能抽样完成! (集成了实时LLM质量检查)\")\n",
        "\n",
        "    # --- 步骤四：丰富化处理（增强版算法 + LLM补充）---\n",
        "    print(f\"\\n🏭 步骤四：正在为 {len(selected_words)} 个词进行最终数据丰富化...\")\n",
        "    if llm_available:\n",
        "        print(\"🚀 使用增强版算法 + LLM智能补充\")\n",
        "    else:\n",
        "        print(\"🚀 使用增强版算法\")\n",
        "    \n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    \n",
        "    def get_all_senses(word):\n",
        "        synsets = wn.synsets(word.lower())\n",
        "        return [{\n",
        "            \"name\": s.name(), \n",
        "            \"pos\": s.pos(), \n",
        "            \"definition\": s.definition(), \n",
        "            \"examples\": s.examples()\n",
        "        } for s in synsets] if synsets else []\n",
        "    \n",
        "    def generate_enhanced_taboo_words(primary_synset, target_word, all_synsets=None):\n",
        "        \"\"\"增强版 taboo word 生成器 - 多策略生成\"\"\"\n",
        "        taboo_set = set()\n",
        "        target_word_lower = target_word.lower()\n",
        "        \n",
        "        # 策略 1: 从主要同义词集获取同义词\n",
        "        for lemma in primary_synset.lemmas():\n",
        "            synonym = lemma.name().lower().replace('_', ' ')\n",
        "            if synonym != target_word_lower and len(synonym.split()) <= 2:\n",
        "                main_word = synonym.split()[0]\n",
        "                if main_word.isalpha() and len(main_word) > 2:\n",
        "                    taboo_set.add(main_word)\n",
        "                if len(synonym.split()) <= 2 and all(w.isalpha() for w in synonym.split()):\n",
        "                    taboo_set.add(synonym.replace(' ', ''))\n",
        "        \n",
        "        # 策略 2: 从所有相关同义词集获取更多同义词\n",
        "        if all_synsets:\n",
        "            for synset_info in all_synsets[:3]:\n",
        "                try:\n",
        "                    synset = wn.synset(synset_info['name'])\n",
        "                    for lemma in synset.lemmas():\n",
        "                        synonym = lemma.name().lower().replace('_', ' ')\n",
        "                        if synonym != target_word_lower:\n",
        "                            main_word = synonym.split()[0]\n",
        "                            if main_word.isalpha() and len(main_word) > 2:\n",
        "                                taboo_set.add(main_word)\n",
        "                except:\n",
        "                    continue\n",
        "        \n",
        "        # 策略 3: 从反义词获取\n",
        "        if primary_synset.lemmas():\n",
        "            for antonym_lemma in primary_synset.lemmas()[0].antonyms():\n",
        "                antonym = antonym_lemma.name().lower().replace('_', ' ')\n",
        "                main_word = antonym.split()[0]\n",
        "                if main_word.isalpha() and len(main_word) > 2:\n",
        "                    taboo_set.add(main_word)\n",
        "        \n",
        "        # 策略 4: 从定义中提取关键词（改进版）\n",
        "        definition = primary_synset.definition()\n",
        "        tokens = word_tokenize(definition)\n",
        "        tagged = pos_tag(tokens)\n",
        "        \n",
        "        for word, tag in tagged:\n",
        "            word_lower = word.lower()\n",
        "            # 优先选择名词、动词、形容词\n",
        "            if (tag.startswith(('NN', 'VB', 'JJ')) and \n",
        "                word_lower.isalpha() and \n",
        "                word_lower not in stop_words and \n",
        "                len(word_lower) > 3 and \n",
        "                word_lower != target_word_lower):\n",
        "                taboo_set.add(word_lower)\n",
        "        \n",
        "        # 策略 5: 从例句中提取关键词\n",
        "        for example in primary_synset.examples():\n",
        "            try:\n",
        "                tokens = word_tokenize(example)\n",
        "                tagged = pos_tag(tokens)\n",
        "                for word, tag in tagged:\n",
        "                    word_lower = word.lower()\n",
        "                    if (tag.startswith(('NN', 'VB', 'JJ')) and \n",
        "                        word_lower.isalpha() and \n",
        "                        word_lower not in stop_words and \n",
        "                        len(word_lower) > 3 and \n",
        "                        word_lower != target_word_lower and\n",
        "                        len(taboo_set) < 10):\n",
        "                        taboo_set.add(word_lower)\n",
        "            except:\n",
        "                continue\n",
        "        \n",
        "        # 策略 6: 获取上位词和下位词\n",
        "        try:\n",
        "            for hypernym in primary_synset.hypernyms():\n",
        "                for lemma in hypernym.lemmas():\n",
        "                    hyper_word = lemma.name().lower().replace('_', ' ').split()[0]\n",
        "                    if (hyper_word.isalpha() and \n",
        "                        len(hyper_word) > 3 and \n",
        "                        hyper_word != target_word_lower):\n",
        "                        taboo_set.add(hyper_word)\n",
        "            \n",
        "            hyponyms = primary_synset.hyponyms()[:2]\n",
        "            for hyponym in hyponyms:\n",
        "                for lemma in hyponym.lemmas():\n",
        "                    hypo_word = lemma.name().lower().replace('_', ' ').split()[0]\n",
        "                    if (hypo_word.isalpha() and \n",
        "                        len(hypo_word) > 3 and \n",
        "                        hypo_word != target_word_lower):\n",
        "                        taboo_set.add(hypo_word)\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        # 过滤和排序\n",
        "        final_taboos = []\n",
        "        taboo_list = list(taboo_set)\n",
        "        taboo_list.sort(key=lambda x: (abs(len(x) - 6), x))  # 优先6个字母左右的词\n",
        "        \n",
        "        seen = set()\n",
        "        for word in taboo_list:\n",
        "            if word not in seen and len(final_taboos) < 8:\n",
        "                final_taboos.append(word)\n",
        "                seen.add(word)\n",
        "        \n",
        "        # 策略 7: 通用词汇兜底\n",
        "        pos_specific_generics = {\n",
        "            'n': ['object', 'thing', 'item', 'entity', 'element', 'concept', 'notion'],\n",
        "            'v': ['action', 'process', 'activity', 'behavior', 'perform', 'execute'],\n",
        "            'a': ['quality', 'property', 'characteristic', 'feature', 'attribute'],\n",
        "            's': ['quality', 'property', 'characteristic', 'feature', 'attribute']\n",
        "        }\n",
        "        \n",
        "        pos = primary_synset.pos()\n",
        "        generic_words = pos_specific_generics.get(pos, ['word', 'term', 'expression'])\n",
        "        \n",
        "        for generic in generic_words:\n",
        "            if len(final_taboos) >= 5:\n",
        "                break\n",
        "            if generic not in final_taboos and generic != target_word_lower:\n",
        "                final_taboos.append(generic)\n",
        "        \n",
        "        return final_taboos[:5]\n",
        "        \n",
        "    final_json_list = []\n",
        "    taboo_stats = {\n",
        "        'sufficient_traditional': 0, \n",
        "        'llm_enhanced': 0, \n",
        "        'still_insufficient': 0, \n",
        "        'llm_calls': 0,\n",
        "        'details': []\n",
        "    }\n",
        "    \n",
        "    for i, (word, category) in enumerate(selected_words.items()):\n",
        "        if (i + 1) % 50 == 0: \n",
        "            print(f\"  - 丰富化进度 {i+1}/{len(selected_words)}: {word}\")\n",
        "        \n",
        "        senses = get_all_senses(word)\n",
        "        if not senses: continue\n",
        "        \n",
        "        primary_synset = wn.synset(senses[0]['name'])\n",
        "        pos_map = {'n':'noun','v':'verb','a':'adj','r':'adverb','s':'adj'}\n",
        "        main_pos_str = pos_map.get(senses[0]['pos'], 'other')\n",
        "        sense_count = len(senses)\n",
        "        concreteness_score = concreteness_lookup.get(word)\n",
        "        \n",
        "        # 使用增强版算法生成初始taboo words\n",
        "        taboo_words = generate_enhanced_taboo_words(primary_synset, word, senses)\n",
        "        \n",
        "        # 如果不够5个且LLM可用，使用LLM补充\n",
        "        if len(taboo_words) < 5 and llm_available:\n",
        "            llm_taboos = generate_llm_taboo_words(\n",
        "                word, \n",
        "                taboo_words, \n",
        "                main_pos_str, \n",
        "                primary_synset.definition(),\n",
        "                OPENROUTER_API_KEY\n",
        "            )\n",
        "            \n",
        "            if llm_taboos:\n",
        "                taboo_words.extend(llm_taboos)\n",
        "                taboo_stats['llm_calls'] += 1\n",
        "                taboo_stats['llm_enhanced'] += 1\n",
        "            \n",
        "            taboo_words = taboo_words[:5]\n",
        "        \n",
        "        # 统计结果\n",
        "        if len(taboo_words) >= 5:\n",
        "            taboo_stats['sufficient_traditional'] += 1\n",
        "        else:\n",
        "            taboo_stats['still_insufficient'] += 1\n",
        "            taboo_stats['details'].append(f\"{word}: {len(taboo_words)} taboos\")\n",
        "        \n",
        "        json_obj = {\n",
        "            \"target\": word,\n",
        "            \"part_of_speech\": main_pos_str,\n",
        "            \"taboo\": taboo_words,\n",
        "            \"category\": category,\n",
        "            \"senses\": senses,\n",
        "            \"metadata\": {\n",
        "                \"sense_count\": sense_count,\n",
        "                \"concreteness_score\": round(concreteness_score, 2) if concreteness_score else None,\n",
        "                \"taboo_count\": len(taboo_words)\n",
        "            }\n",
        "        }\n",
        "        final_json_list.append(json_obj)\n",
        "        \n",
        "        # API调用限制\n",
        "        if taboo_stats['llm_calls'] > 0 and taboo_stats['llm_calls'] % 10 == 0:\n",
        "            time.sleep(1)\n",
        "\n",
        "    # --- 步骤五：输出结果和统计 ---\n",
        "    print(f\"\\n--- 数据丰富化完成 ---\")\n",
        "    output_filename = \"ENHANCED_DATASET.json\"\n",
        "    with open(output_filename, 'w', encoding='utf-8') as f: \n",
        "        json.dump(final_json_list, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    print(f\"✅✅✅ 您的增强版数据集已生成！文件名为: '{output_filename}'\")\n",
        "    print(f\"总词汇数: {len(final_json_list)}\")\n",
        "    \n",
        "    # 详细统计报告\n",
        "    print(f\"\\n📊 Taboo Word 生成统计:\")\n",
        "    print(f\"  - 成功生成足够taboo words (≥5个): {taboo_stats['sufficient_traditional']} 个词汇\")\n",
        "    if llm_available:\n",
        "        print(f\"  - LLM成功补充: {taboo_stats['llm_enhanced']} 个词汇\")\n",
        "        print(f\"  - 总LLM调用次数: {taboo_stats['llm_calls']}\")\n",
        "    print(f\"  - 仍然不足: {taboo_stats['still_insufficient']} 个词汇\")\n",
        "    \n",
        "    if taboo_stats['details'][:3]:\n",
        "        print(\"  - 仍然不足的词汇:\")\n",
        "        for detail in taboo_stats['details'][:3]:\n",
        "            print(f\"    {detail}\")\n",
        "    \n",
        "    print(\"\\n--- 最终文件类别分布自检 ---\")\n",
        "    final_df = pd.DataFrame(final_json_list)\n",
        "    print(final_df['category'].value_counts())\n",
        "    \n",
        "    # 验证最终质量\n",
        "    avg_taboo_count = final_df['metadata'].apply(lambda x: x['taboo_count']).mean()\n",
        "    success_rate = (len(final_df[final_df['metadata'].apply(lambda x: x['taboo_count']) >= 5]) / len(final_df)) * 100\n",
        "    \n",
        "    print(f\"\\n📈 最终质量报告:\")\n",
        "    print(f\"  - 平均 taboo word 数量: {avg_taboo_count:.2f}\")\n",
        "    print(f\"  - 成功率 (≥5个taboo words): {success_rate:.1f}%\")\n",
        "    \n",
        "    print(\"\\n🏆 恭喜！增强版项目已准备就绪！\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌❌❌ 脚本执行过程中出现意外错误: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_api_key():\n",
        "    \"\"\"加载API密钥\"\"\"\n",
        "    try:\n",
        "        with open('api_keys.json', 'r') as f:\n",
        "            api_keys = json.load(f)\n",
        "        return api_keys['OPENROUTER_API_KEY']\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ 无法加载API密钥: {e}\")\n",
        "        return None\n",
        "\n",
        "def call_openrouter_api(prompt, api_key, max_retries=3):\n",
        "    \"\"\"调用OpenRouter API (DeepSeek V3)\"\"\"\n",
        "    if not api_key:\n",
        "        return None\n",
        "        \n",
        "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": \"deepseek/deepseek-chat-v3-0324:free\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "        \"max_tokens\": 150,\n",
        "        \"temperature\": 0.7\n",
        "    }\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.post(url, headers=headers, json=data, timeout=30)\n",
        "            if response.status_code == 200:\n",
        "                result = response.json()\n",
        "                return result['choices'][0]['message']['content'].strip()\n",
        "            else:\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(2 ** attempt)\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(2 ** attempt)\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== LLM质量检查开始 ===\n",
            "🔍 正在检查生成的数据集中是否有不合适的target words...\n",
            "✅ 数据集已加载，共 300 个词汇\n",
            "✅ 已提取所有target words\n",
            "🤖 开始使用LLM进行质量检查...\n",
            "🔍 正在检查批次 1/15 (20 个词汇)...\n",
            "🔍 正在检查批次 2/15 (20 个词汇)...\n",
            "🔍 正在检查批次 3/15 (20 个词汇)...\n",
            "🔍 正在检查批次 4/15 (20 个词汇)...\n",
            "🔍 正在检查批次 5/15 (20 个词汇)...\n",
            "🔍 正在检查批次 6/15 (20 个词汇)...\n",
            "🔍 正在检查批次 7/15 (20 个词汇)...\n",
            "🔍 正在检查批次 8/15 (20 个词汇)...\n",
            "🔍 正在检查批次 9/15 (20 个词汇)...\n",
            "🔍 正在检查批次 10/15 (20 个词汇)...\n",
            "🔍 正在检查批次 11/15 (20 个词汇)...\n",
            "🔍 正在检查批次 12/15 (20 个词汇)...\n",
            "🔍 正在检查批次 13/15 (20 个词汇)...\n",
            "🔍 正在检查批次 14/15 (20 个词汇)...\n",
            "🔍 正在检查批次 15/15 (20 个词汇)...\n",
            "\\n📋 数据集质量检查报告\n",
            "==================================================\n",
            "总词汇数: 300\n",
            "被标记为不合适的词汇数: 0\n",
            "合格率: 100.0%\n",
            "\\n✅ 所有词汇都被认为是合适的！\n",
            "\\n=== LLM质量检查完成 ===\n"
          ]
        }
      ],
      "source": [
        "# LLM质量检查 - 检测不合适的target words\n",
        "import json\n",
        "import time\n",
        "\n",
        "def load_generated_dataset():\n",
        "    \"\"\"加载刚生成的数据集\"\"\"\n",
        "    try:\n",
        "        with open('ENHANCED_DATASET.json', 'r', encoding='utf-8') as f:\n",
        "            dataset = json.load(f)\n",
        "        return dataset\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 无法加载数据集文件: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_target_words(dataset):\n",
        "    \"\"\"提取所有target words\"\"\"\n",
        "    if not dataset:\n",
        "        return []\n",
        "    \n",
        "    target_words = []\n",
        "    for entry in dataset:\n",
        "        target_words.append({\n",
        "            'word': entry['target'],\n",
        "            'category': entry['category'],\n",
        "            'pos': entry['part_of_speech']\n",
        "        })\n",
        "    \n",
        "    return target_words\n",
        "\n",
        "def check_word_appropriateness_batch(words_info, api_key, batch_size=20):\n",
        "    \"\"\"批量检查词汇是否适合Taboo游戏\"\"\"\n",
        "    if not api_key:\n",
        "        print(\"⚠️ API密钥不可用，跳过质量检查\")\n",
        "        return []\n",
        "    \n",
        "    inappropriate_words = []\n",
        "    total_batches = (len(words_info) + batch_size - 1) // batch_size\n",
        "    \n",
        "    for batch_idx in range(0, len(words_info), batch_size):\n",
        "        batch = words_info[batch_idx:batch_idx + batch_size]\n",
        "        batch_num = batch_idx // batch_size + 1\n",
        "        \n",
        "        print(f\"🔍 正在检查批次 {batch_num}/{total_batches} ({len(batch)} 个词汇)...\")\n",
        "        \n",
        "        # 构建批量检查的prompt\n",
        "        word_list = []\n",
        "        for i, word_info in enumerate(batch):\n",
        "            word_list.append(f\"{i+1}. {word_info['word']} ({word_info['category']}, {word_info['pos']})\")\n",
        "        \n",
        "        words_text = \"\\n\".join(word_list)\n",
        "        \n",
        "        prompt = f\"\"\"请检查以下词汇是否适合用于Taboo游戏。只需要检查这两个标准：\n",
        "1. 不是专有名词（人名、地名、品牌名、特定机构名等）\n",
        "2. 不涉及严重敏感内容（仅限：极端暴力、色情内容、种族歧视、极端政治敏感）\n",
        "\n",
        "注意：以下内容都是可以接受的，不算敏感内容：\n",
        "- 医学术语、疾病名称、人体解剖学术语\n",
        "- 法律术语、商业术语  \n",
        "- 一般的学术概念\n",
        "- 专业技术术语\n",
        "- 生僻词汇\n",
        "\n",
        "待检查词汇列表：\n",
        "{words_text}\n",
        "\n",
        "对于不合适的词汇，请按以下格式返回，每行一个：\n",
        "编号:原因\n",
        "例如：\n",
        "1:专有名词-人名\n",
        "3:专有名词-地名\n",
        "7:敏感内容-政治\n",
        "\n",
        "如果所有词汇都合适，请只返回\"无\"。\"\"\"\n",
        "\n",
        "        response = call_openrouter_api(prompt, api_key)\n",
        "        if response:\n",
        "            response = response.strip()\n",
        "            if response.lower() != \"无\" and response.lower() != \"none\":\n",
        "                try:\n",
        "                    # 解析返回的编号和原因\n",
        "                    lines = response.split('\\n')\n",
        "                    for line in lines:\n",
        "                        line = line.strip()\n",
        "                        if ':' in line:\n",
        "                            parts = line.split(':', 1)\n",
        "                            if len(parts) == 2 and parts[0].strip().isdigit():\n",
        "                                idx = int(parts[0].strip()) - 1\n",
        "                                reason = parts[1].strip()\n",
        "                                if 0 <= idx < len(batch):\n",
        "                                    inappropriate_words.append({\n",
        "                                        'word': batch[idx]['word'],\n",
        "                                        'category': batch[idx]['category'],\n",
        "                                        'pos': batch[idx]['pos'],\n",
        "                                        'reason': reason\n",
        "                                    })\n",
        "                except Exception as e:\n",
        "                    print(f\"  - ⚠️ 解析LLM回复时出错: {e}\")\n",
        "                    print(f\"  - 原始回复: {response[:100]}...\")\n",
        "        \n",
        "        # 避免API调用过于频繁\n",
        "        if batch_num < total_batches:\n",
        "            time.sleep(2)\n",
        "    \n",
        "    return inappropriate_words\n",
        "\n",
        "def generate_quality_report(dataset, inappropriate_words):\n",
        "    \"\"\"生成质量报告\"\"\"\n",
        "    if not dataset:\n",
        "        return\n",
        "    \n",
        "    total_words = len(dataset)\n",
        "    inappropriate_count = len(inappropriate_words)\n",
        "    \n",
        "    print(f\"\\\\n📋 数据集质量检查报告\")\n",
        "    print(f\"=\" * 50)\n",
        "    print(f\"总词汇数: {total_words}\")\n",
        "    print(f\"被标记为不合适的词汇数: {inappropriate_count}\")\n",
        "    print(f\"合格率: {((total_words - inappropriate_count) / total_words * 100):.1f}%\")\n",
        "    \n",
        "    if inappropriate_words:\n",
        "        print(f\"\\\\n⚠️ 被标记为不合适的词汇:\")\n",
        "        by_category = {}\n",
        "        for word_info in inappropriate_words:\n",
        "            category = word_info['category']\n",
        "            if category not in by_category:\n",
        "                by_category[category] = []\n",
        "            by_category[category].append(word_info)\n",
        "        \n",
        "        for category, words in by_category.items():\n",
        "            print(f\"\\\\n  【{category.upper()}】:\")\n",
        "            for word_info in words:\n",
        "                print(f\"    - {word_info['word']} ({word_info['pos']}) - {word_info['reason']}\")\n",
        "    else:\n",
        "        print(f\"\\\\n✅ 所有词汇都被认为是合适的！\")\n",
        "\n",
        "# 执行质量检查\n",
        "print(\"=== LLM质量检查开始 ===\")\n",
        "print(\"🔍 正在检查生成的数据集中是否有不合适的target words...\")\n",
        "\n",
        "# 加载数据集和API密钥\n",
        "dataset = load_generated_dataset()\n",
        "api_key = load_api_key()\n",
        "\n",
        "if dataset and api_key:\n",
        "    print(f\"✅ 数据集已加载，共 {len(dataset)} 个词汇\")\n",
        "    \n",
        "    # 提取target words\n",
        "    target_words_info = extract_target_words(dataset)\n",
        "    print(f\"✅ 已提取所有target words\")\n",
        "    \n",
        "    # 批量检查\n",
        "    print(f\"🤖 开始使用LLM进行质量检查...\")\n",
        "    inappropriate_words = check_word_appropriateness_batch(target_words_info, api_key)\n",
        "    \n",
        "    # 生成报告\n",
        "    generate_quality_report(dataset, inappropriate_words)\n",
        "    \n",
        "    # 如果有不合适的词汇，保存到文件\n",
        "    if inappropriate_words:\n",
        "        with open('inappropriate_words_report.json', 'w', encoding='utf-8') as f:\n",
        "            json.dump(inappropriate_words, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"\\\\n📄 详细报告已保存到: inappropriate_words_report.json\")\n",
        "    \n",
        "elif not dataset:\n",
        "    print(\"❌ 无法加载数据集，请确保先运行了数据集生成代码\")\n",
        "elif not api_key:\n",
        "    print(\"❌ 无法加载API密钥，质量检查功能不可用\")\n",
        "\n",
        "print(\"\\\\n=== LLM质量检查完成 ===\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 数据集加载成功: 300 条记录\n",
            "📁 数据集路径: data/dataset.json\n",
            "\n",
            "📋 数据样本:\n",
            "   目标词: regent\n",
            "   禁用词: ['board', 'members', 'trustee', 'committee', 'governing']\n",
            "   类别: general\n",
            "   定义: members of a governing board...\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "import time\n",
        "import requests\n",
        "import pandas as pd\n",
        "from typing import Dict, List, Any\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# 加载数据集\n",
        "def load_dataset(dataset_path: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"加载Taboo游戏数据集\"\"\"\n",
        "    with open(dataset_path, 'r', encoding='utf-8') as f:\n",
        "        dataset = json.load(f)\n",
        "    return dataset\n",
        "\n",
        "# 加载预生成的数据集\n",
        "DATASET_PATH = \"data/dataset.json\"\n",
        "dataset = load_dataset(DATASET_PATH)\n",
        "print(f\"✅ 数据集加载成功: {len(dataset)} 条记录\")\n",
        "print(f\"📁 数据集路径: {DATASET_PATH}\")\n",
        "\n",
        "# 显示第一个样本\n",
        "if dataset:\n",
        "    sample = dataset[0]\n",
        "    print(f\"\\n📋 数据样本:\")\n",
        "    print(f\"   目标词: {sample['target']}\")\n",
        "    print(f\"   禁用词: {sample['taboo']}\")\n",
        "    print(f\"   类别: {sample.get('category', 'N/A')}\")\n",
        "    if sample.get('senses'):\n",
        "        print(f\"   定义: {sample['senses'][0].get('definition', 'N/A')[:100]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 377)",
          "output_type": "error",
          "traceback": [
            "  \u001b[36mFile \u001b[39m\u001b[32m<tokenize>:377\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m}\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unindent does not match any outer indentation level\n"
          ]
        }
      ],
      "source": [
        "# 英国拼写检查功能 - 使用LLM直接检查\n",
        "import json\n",
        "import time\n",
        "\n",
        "def check_spelling_with_llm(words_info, api_key, batch_size=20):\n",
        "    \"\"\"使用LLM批量检查拼写并进行转换\"\"\"\n",
        "    if not api_key:\n",
        "        print(\"❌ API密钥不可用，无法进行拼写检查\")\n",
        "        return []\n",
        "        \n",
        "    spelling_changes = []\n",
        "    total_batches = (len(words_info) + batch_size - 1) // batch_size\n",
        "    \n",
        "    for batch_idx in range(0, len(words_info), batch_size):\n",
        "        batch = words_info[batch_idx:batch_idx + batch_size]\n",
        "        batch_num = batch_idx // batch_size + 1\n",
        "        \n",
        "        print(f\"🔍 正在检查批次 {batch_num}/{total_batches} 的拼写 ({len(batch)} 个词汇)...\")\n",
        "        \n",
        "        # 构建批量检查的prompt\n",
        "        word_list = []\n",
        "        for i, word_info in enumerate(batch):\n",
        "            word_list.append(f\"{i+1}. {word_info['word']}\")\n",
        "        words_text = \"\\n\".join(word_list)\n",
        "        \n",
        "        prompt = f\"\"\"请检查以下英文词汇的拼写是否为英国英语拼写。如果是美国英语拼写，请转换为英国英语拼写。\n",
        "\n",
        "常见转换规则：\n",
        "- -ize → -ise (organize → organise)\n",
        "- -or → -our (color → colour)  \n",
        "- -er → -re (center → centre)\n",
        "- -ense → -ence (defense → defence)\n",
        "- 其他常见转换 (gray → grey, aluminum → aluminium)\n",
        "\n",
        "待检查词汇：\n",
        "{words_text}\n",
        "\n",
        "请只返回需要转换的词汇，格式为：\n",
        "编号:原词→英国拼写\n",
        "例如：\n",
        "1:organize→organise\n",
        "3:color→colour\n",
        "\n",
        "如果所有词汇都是英国拼写，请返回\"无需转换\"。\"\"\"\n",
        "\n",
        "        response = call_openrouter_api(prompt, api_key)\n",
        "        if response:\n",
        "            response = response.strip()\n",
        "            if response.lower() not in [\"无需转换\", \"无\", \"none\", \"no changes needed\"]:\n",
        "                try:\n",
        "                    # 解析LLM返回的拼写转换\n",
        "                    lines = response.split('\\n')\n",
        "                    for line in lines:\n",
        "                        line = line.strip()\n",
        "                        if ':' in line and '→' in line:\n",
        "                            parts = line.split(':', 1)\n",
        "                            if len(parts) == 2 and parts[0].strip().isdigit():\n",
        "                                idx = int(parts[0].strip()) - 1\n",
        "                                conversion = parts[1].strip()\n",
        "                                if '→' in conversion:\n",
        "                                    original, corrected = conversion.split('→', 1)\n",
        "                                    original = original.strip()\n",
        "                                    corrected = corrected.strip()\n",
        "                                    if 0 <= idx < len(batch):\n",
        "                                        spelling_changes.append({\n",
        "                                            'original': original,\n",
        "                                            'corrected': corrected,\n",
        "                                            'category': batch[idx]['category'],\n",
        "                                            'pos': batch[idx]['pos'],\n",
        "                                            'type': 'llm_conversion'\n",
        "                                        })\n",
        "                except Exception as e:\n",
        "                    print(f\"  - ⚠️ 解析LLM拼写回复时出错: {e}\")\n",
        "                    print(f\"  - 原始回复: {response[:100]}...\")\n",
        "        \n",
        "        # 显示本批次的转换结果\n",
        "        batch_changes = [c for c in spelling_changes if any(w['word'] == c['original'] for w in batch)]\n",
        "        if batch_changes:\n",
        "            print(f\"  🔄 本批次发现 {len(batch_changes)} 个拼写转换\")\n",
        "            for change in batch_changes[-3:]:  # 显示最后3个\n",
        "                print(f\"    {change['original']} → {change['corrected']}\")\n",
        "        \n",
        "        # 避免API调用过于频繁\n",
        "        if batch_num < total_batches:\n",
        "            time.sleep(1.5)\n",
        "    \n",
        "    return spelling_changes\n",
        "\n",
        "def generate_spelling_report(spelling_changes):\n",
        "    \"\"\"生成拼写检查报告\"\"\"\n",
        "    if not spelling_changes:\n",
        "        print(f\"\\n✅ 所有词汇都已经是英国拼写！\")\n",
        "        return\n",
        "    \n",
        "    print(f\"\\n📋 拼写检查报告\")\n",
        "    print(f\"=\" * 50)\n",
        "    print(f\"发现需要转换的词汇数: {len(spelling_changes)}\")\n",
        "    \n",
        "    # 按类别分组\n",
        "    by_category = {}\n",
        "    \n",
        "    for change in spelling_changes:\n",
        "        category = change['category']\n",
        "        if category not in by_category:\n",
        "            by_category[category] = []\n",
        "        by_category[category].append(change)\n",
        "    \n",
        "    print(f\"\\n🔄 拼写转换详情:\")\n",
        "    for category, changes in by_category.items():\n",
        "        print(f\"\\n  【{category.upper()}】:\")\n",
        "        for change in changes:\n",
        "            print(f\"    {change['original']} → {change['corrected']} ({change['pos']})\")\n",
        "\n",
        "def update_dataset_spelling(dataset, spelling_changes):\n",
        "    \"\"\"更新数据集中的拼写\"\"\"\n",
        "    if not spelling_changes:\n",
        "        return dataset, 0\n",
        "    \n",
        "    # 创建转换映射\n",
        "    conversion_map = {change['original']: change['corrected'] for change in spelling_changes}\n",
        "    \n",
        "    updated_count = 0\n",
        "    updated_dataset = []\n",
        "    \n",
        "    for entry in dataset:\n",
        "        updated_entry = entry.copy()\n",
        "        \n",
        "        # 检查target word\n",
        "        if entry['target'] in conversion_map:\n",
        "            updated_entry['target'] = conversion_map[entry['target']]\n",
        "            updated_count += 1\n",
        "        \n",
        "        # 检查taboo words\n",
        "        updated_taboo = []\n",
        "        for taboo_word in entry['taboo']:\n",
        "            if taboo_word in conversion_map:\n",
        "                updated_taboo.append(conversion_map[taboo_word])\n",
        "            else:\n",
        "                updated_taboo.append(taboo_word)\n",
        "        updated_entry['taboo'] = updated_taboo\n",
        "        \n",
        "        updated_dataset.append(updated_entry)\n",
        "    \n",
        "    return updated_dataset, updated_count\n",
        "\n",
        "# 执行英国拼写检查\n",
        "print(\"=== 英国拼写检查开始 ===\")\n",
        "print(\"🔍 正在使用LLM检查数据集中的拼写并转换为英国英语...\")\n",
        "\n",
        "# 加载数据集和API密钥\n",
        "dataset = load_generated_dataset()\n",
        "api_key = load_api_key()\n",
        "\n",
        "if dataset and api_key:\n",
        "    print(f\"✅ 数据集已加载，共 {len(dataset)} 个词汇\")\n",
        "    \n",
        "    # 提取target words\n",
        "    target_words_info = extract_target_words(dataset)\n",
        "    print(f\"✅ 已提取所有target words\")\n",
        "    \n",
        "    # 使用LLM批量检查拼写\n",
        "    print(f\"🤖 开始使用LLM进行拼写检查...\")\n",
        "    spelling_changes = check_spelling_with_llm(target_words_info, api_key)\n",
        "    \n",
        "    # 生成拼写报告\n",
        "    generate_spelling_report(spelling_changes)\n",
        "    \n",
        "    # 更新数据集\n",
        "    if spelling_changes:\n",
        "        updated_dataset, updated_count = update_dataset_spelling(dataset, spelling_changes)\n",
        "        \n",
        "        # 保存更新后的数据集\n",
        "        output_filename = \"BRITISH_SPELLING_DATASET.json\"\n",
        "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(updated_dataset, f, indent=2, ensure_ascii=False)\n",
        "        \n",
        "        print(f\"\\n✅ 已更新 {updated_count} 个target words的拼写\")\n",
        "        print(f\"📄 更新后的数据集已保存到: {output_filename}\")\n",
        "        \n",
        "        # 保存拼写转换报告\n",
        "        with open('spelling_conversion_report.json', 'w', encoding='utf-8') as f:\n",
        "            json.dump(spelling_changes, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"📄 拼写转换报告已保存到: spelling_conversion_report.json\")\n",
        "    else:\n",
        "        print(f\"\\n✅ 数据集拼写检查完成，无需修改\")\n",
        "        \n",
        "elif not dataset:\n",
        "    print(\"❌ 无法加载数据集\")\n",
        "elif not api_key:\n",
        "    print(\"❌ 无法加载API密钥，拼写检查功能不可用\")\n",
        "\n",
        "print(\"\\n=== 英国拼写检查完成 ===\")\n",
        "        'organization': 'organisation',\n",
        "        'realize': 'realise',\n",
        "        'recognize': 'recognise',\n",
        "        'organize': 'organise',\n",
        "        'maximize': 'maximise',\n",
        "        'minimize': 'minimise',\n",
        "        'optimize': 'optimise',\n",
        "        'analyze': 'analyse',\n",
        "        'paralyze': 'paralyse',\n",
        "        'catalyze': 'catalyse',\n",
        "        'civilize': 'civilise',\n",
        "        'modernize': 'modernise',\n",
        "        'rationalize': 'rationalise',\n",
        "        'standardize': 'standardise',\n",
        "        'categorize': 'categorise',\n",
        "        'characterize': 'characterise',\n",
        "        'democratize': 'democratise',\n",
        "        'energize': 'energise',\n",
        "        'fertilize': 'fertilise',\n",
        "        'globalize': 'globalise',\n",
        "        'hospitalize': 'hospitalise',\n",
        "        'idealize': 'idealise',\n",
        "        'legalize': 'legalise',\n",
        "        'localize': 'localise',\n",
        "        'materialize': 'materialise',\n",
        "        'nationalize': 'nationalise',\n",
        "        'personalize': 'personalise',\n",
        "        'privatize': 'privatise',\n",
        "        'publicize': 'publicise',\n",
        "        'sensitize': 'sensitise',\n",
        "        'socialize': 'socialise',\n",
        "        'specialize': 'specialise',\n",
        "        'synchronize': 'synchronise',\n",
        "        'theorize': 'theorise',\n",
        "        'utilize': 'utilise',\n",
        "        'visualize': 'visualise',\n",
        "        'vocalize': 'vocalise',\n",
        "        \n",
        "        # -or to -our\n",
        "        'color': 'colour',\n",
        "        'flavor': 'flavour',\n",
        "        'honor': 'honour',\n",
        "        'humor': 'humour',\n",
        "        'labor': 'labour',\n",
        "        'neighbor': 'neighbour',\n",
        "        'odor': 'odour',\n",
        "        'rumor': 'rumour',\n",
        "        'tumor': 'tumour',\n",
        "        'valor': 'valour',\n",
        "        'vapor': 'vapour',\n",
        "        'vigor': 'vigour',\n",
        "        'behavior': 'behaviour',\n",
        "        'endeavor': 'endeavour',\n",
        "        'parlor': 'parlour',\n",
        "        'splendor': 'splendour',\n",
        "        'candor': 'candour',\n",
        "        'clamor': 'clamour',\n",
        "        'glamor': 'glamour',\n",
        "        'harbor': 'harbour',\n",
        "        'ardor': 'ardour',\n",
        "        'fervor': 'fervour',\n",
        "        'rancor': 'rancour',\n",
        "        'rigor': 'rigour',\n",
        "        'savior': 'saviour',\n",
        "        'succor': 'succour',\n",
        "        'tremor': 'tremour',\n",
        "        \n",
        "        # -er to -re\n",
        "        'center': 'centre',\n",
        "        'theater': 'theatre',\n",
        "        'fiber': 'fibre',\n",
        "        'liter': 'litre',\n",
        "        'meter': 'metre',\n",
        "        'saber': 'sabre',\n",
        "        'specter': 'spectre',\n",
        "        'caliber': 'calibre',\n",
        "        'goiter': 'goitre',\n",
        "        'luster': 'lustre',\n",
        "        'miter': 'mitre',\n",
        "        'niter': 'nitre',\n",
        "        'ocher': 'ochre',\n",
        "        'reconnoiter': 'reconnoitre',\n",
        "        'saltpeter': 'saltpetre',\n",
        "        'scepter': 'sceptre',\n",
        "        'sepulcher': 'sepulchre',\n",
        "        'somber': 'sombre',\n",
        "        'titer': 'titre',\n",
        "        \n",
        "        # -ense to -ence\n",
        "        'defense': 'defence',\n",
        "        'offense': 'offence',\n",
        "        'license': 'licence',  # noun form\n",
        "        'pretense': 'pretence',\n",
        "        \n",
        "        # -ation to -ation (some cases)\n",
        "        'atomization': 'atomisation',\n",
        "        'carbonization': 'carbonisation',\n",
        "        'centralization': 'centralisation',\n",
        "        'computerization': 'computerisation',\n",
        "        'crystallization': 'crystallisation',\n",
        "        'fertilization': 'fertilisation',\n",
        "        'globalization': 'globalisation',\n",
        "        'hospitalization': 'hospitalisation',\n",
        "        'immunization': 'immunisation',\n",
        "        'legalization': 'legalisation',\n",
        "        'localization': 'localisation',\n",
        "        'materialization': 'materialisation',\n",
        "        'modernization': 'modernisation',\n",
        "        'nationalization': 'nationalisation',\n",
        "        'normalization': 'normalisation',\n",
        "        'optimization': 'optimisation',\n",
        "        'personalization': 'personalisation',\n",
        "        'privatization': 'privatisation',\n",
        "        'rationalization': 'rationalisation',\n",
        "        'realization': 'realisation',\n",
        "        'reorganization': 'reorganisation',\n",
        "        'socialization': 'socialisation',\n",
        "        'specialization': 'specialisation',\n",
        "        'stabilization': 'stabilisation',\n",
        "        'standardization': 'standardisation',\n",
        "        'sterilization': 'sterilisation',\n",
        "        'synchronization': 'synchronisation',\n",
        "        'urbanization': 'urbanisation',\n",
        "        'visualization': 'visualisation',\n",
        "        'vocalization': 'vocalisation',\n",
        "        \n",
        "        # Other common differences\n",
        "        'gray': 'grey',\n",
        "        'sulfur': 'sulphur',\n",
        "        'aluminum': 'aluminium',\n",
        "        'check': 'cheque',  # in financial context\n",
        "        'donut': 'doughnut',\n",
        "        'tire': 'tyre',\n",
        "        'curb': 'kerb',\n",
        "        'jewelry': 'jewellery',\n",
        "        'traveling': 'travelling',\n",
        "        'modeling': 'modelling',\n",
        "        'counselor': 'counsellor',\n",
        "        'labeling': 'labelling',\n",
        "        'leveling': 'levelling',\n",
        "        'marveling': 'marvelling',\n",
        "        'quarreling': 'quarrelling',\n",
        "        'signaling': 'signalling',\n",
        "        'tunneling': 'tunnelling',\n",
        "        'canceled': 'cancelled',\n",
        "        'enrollment': 'enrolment',\n",
        "        'fulfillment': 'fulfilment',\n",
        "        'installment': 'instalment',\n",
        "        'skillful': 'skilful',\n",
        "        'willful': 'wilful',\n",
        "        'maneuver': 'manoeuvre',\n",
        "        'estrogen': 'oestrogen',\n",
        "        'fetus': 'foetus',\n",
        "        'leukemia': 'leukaemia',\n",
        "        'anemia': 'anaemia',\n",
        "        'encyclopedia': 'encyclopaedia',\n",
        "        'pediatric': 'paediatric',\n",
        "        'anesthesia': 'anaesthesia',\n",
        "        'diarrhea': 'diarrhoea',\n",
        "        'archeology': 'archaeology',\n",
        "        'medieval': 'mediaeval',\n",
        "        'mold': 'mould',\n",
        "        'smolder': 'smoulder',\n",
        "        'plow': 'plough',\n",
        "        'airplane': 'aeroplane',\n",
        "        'draft': 'draught',\n",
        "        'skeptic': 'sceptic',\n",
        "        'ax': 'axe',\n",
        "        'cozy': 'cosy',\n",
        "        'dozy': 'dozy',\n",
        "        'aging': 'ageing',\n",
        "        'acknowledgment': 'acknowledgement',\n",
        "        'judgment': 'judgement',\n",
        "        'abridgment': 'abridgement',\n",
        "        'fledgling': 'fledgeling',\n",
        "        'inquiry': 'enquiry',\n",
        "        'insure': 'ensure',\n",
        "        'program': 'programme',\n",
        "        'story': 'storey',  # building level\n",
        "        'prolog': 'prologue',\n",
        "        'catalog': 'catalogue',\n",
        "        'dialog': 'dialogue',\n",
        "        'monolog': 'monologue',\n",
        "        'analog': 'analogue',\n",
        "    }\n",
        "    \n",
        "    return american_to_british\n",
        "\n",
        "def check_spelling_and_convert(word, conversion_dict):\n",
        "    \"\"\"检查并转换拼写\"\"\"\n",
        "    word_lower = word.lower()\n",
        "    if word_lower in conversion_dict:\n",
        "        return conversion_dict[word_lower], True\n",
        "    return word, False\n",
        "\n",
        "def check_spelling_batch(words_info, api_key, conversion_dict, batch_size=20):\n",
        "    \"\"\"批量检查拼写并进行转换\"\"\"\n",
        "    if not api_key:\n",
        "        print(\"⚠️ API密钥不可用，仅进行本地拼写检查\")\n",
        "        \n",
        "    spelling_changes = []\n",
        "    total_batches = (len(words_info) + batch_size - 1) // batch_size\n",
        "    \n",
        "    for batch_idx in range(0, len(words_info), batch_size):\n",
        "        batch = words_info[batch_idx:batch_idx + batch_size]\n",
        "        batch_num = batch_idx // batch_size + 1\n",
        "        \n",
        "        print(f\"🔍 正在检查批次 {batch_num}/{total_batches} 的拼写 ({len(batch)} 个词汇)...\")\n",
        "        \n",
        "        # 先进行本地拼写检查\n",
        "        local_changes = []\n",
        "        for i, word_info in enumerate(batch):\n",
        "            word = word_info['word']\n",
        "            british_word, changed = check_spelling_and_convert(word, conversion_dict)\n",
        "            if changed:\n",
        "                local_changes.append({\n",
        "                    'original': word,\n",
        "                    'corrected': british_word,\n",
        "                    'category': word_info['category'],\n",
        "                    'pos': word_info['pos'],\n",
        "                    'type': 'local_conversion'\n",
        "                })\n",
        "        \n",
        "                 # 如果有LLM API，进行额外的拼写检查\n",
        "         if api_key and batch:\n",
        "             # 构建批量检查的prompt\n",
        "             word_list = []\n",
        "             for i, word_info in enumerate(batch):\n",
        "                 word_list.append(f\"{i+1}. {word_info['word']}\")\n",
        "             words_text = \"\\n\".join(word_list)\n",
        "             \n",
        "             prompt = f\"\"\"请检查以下英文词汇的拼写是否为英国英语拼写。如果是美国英语拼写，请转换为英国英语拼写。\n",
        "\n",
        "常见转换规则：\n",
        "- -ize → -ise (organize → organise)\n",
        "- -or → -our (color → colour)  \n",
        "- -er → -re (center → centre)\n",
        "- -ense → -ence (defense → defence)\n",
        "- 其他常见转换 (gray → grey, aluminum → aluminium)\n",
        "\n",
        "待检查词汇：\n",
        "{words_text}\n",
        "\n",
        "请只返回需要转换的词汇，格式为：\n",
        "编号:原词→英国拼写\n",
        "例如：\n",
        "1:organize→organise\n",
        "3:color→colour\n",
        "\n",
        "如果所有词汇都是英国拼写，请返回\"无需转换\"。\"\"\"\n",
        "\n",
        "            response = call_openrouter_api(prompt, api_key)\\n            if response:\\n                response = response.strip()\\n                if response.lower() not in [\\\"无需转换\\\", \\\"无\\\", \\\"none\\\", \\\"no changes needed\\\"]:\\n                    try:\\n                        # 解析LLM返回的拼写转换\\n                        lines = response.split('\\\\n')\\n                        for line in lines:\\n                            line = line.strip()\\n                            if ':' in line and '→' in line:\\n                                parts = line.split(':', 1)\\n                                if len(parts) == 2 and parts[0].strip().isdigit():\\n                                    idx = int(parts[0].strip()) - 1\\n                                    conversion = parts[1].strip()\\n                                    if '→' in conversion:\\n                                        original, corrected = conversion.split('→', 1)\\n                                        original = original.strip()\\n                                        corrected = corrected.strip()\\n                                        if 0 <= idx < len(batch):\\n                                            # 检查是否已经在本地转换中\\n                                            already_converted = any(c['original'] == original for c in local_changes)\\n                                            if not already_converted:\\n                                                spelling_changes.append({\\n                                                    'original': original,\\n                                                    'corrected': corrected,\\n                                                    'category': batch[idx]['category'],\\n                                                    'pos': batch[idx]['pos'],\\n                                                    'type': 'llm_conversion'\\n                                                })\\n                    except Exception as e:\\n                        print(f\\\"  - ⚠️ 解析LLM拼写回复时出错: {e}\\\")\\n                        print(f\\\"  - 原始回复: {response[:100]}...\\\")\\n        \\n        # 添加本地转换结果\\n        spelling_changes.extend(local_changes)\\n        \\n        # 显示本批次的转换结果\\n        batch_changes = [c for c in spelling_changes if any(w['word'] in [c['original']] for w in batch)]\\n        if batch_changes:\\n            print(f\\\"  🔄 本批次发现 {len(batch_changes)} 个拼写转换\\\")\\n            for change in batch_changes[-3:]:  # 显示最后3个\\n                print(f\\\"    {change['original']} → {change['corrected']}\\\")\\n        \\n        # 避免API调用过于频繁\\n        if api_key and batch_num < total_batches:\\n            time.sleep(1.5)\\n    \\n    return spelling_changes\n",
        "\n",
        "def generate_spelling_report(spelling_changes):\n",
        "    \\\"\\\"\\\"生成拼写检查报告\\\"\\\"\\\"\\n    if not spelling_changes:\\n        print(f\\\"\\\\n✅ 所有词汇都已经是英国拼写！\\\")\\n        return\\n    \\n    print(f\\\"\\\\n📋 拼写检查报告\\\")\\n    print(f\\\"=\\\" * 50)\\n    print(f\\\"发现需要转换的词汇数: {len(spelling_changes)}\\\")\\n    \\n    # 按类别分组\\n    by_category = {}\\n    by_type = {'local_conversion': 0, 'llm_conversion': 0}\\n    \\n    for change in spelling_changes:\\n        category = change['category']\\n        if category not in by_category:\\n            by_category[category] = []\\n        by_category[category].append(change)\\n        \\n        change_type = change.get('type', 'unknown')\\n        if change_type in by_type:\\n            by_type[change_type] += 1\\n    \\n    print(f\\\"\\\\n📊 转换类型统计:\\\")\\n    print(f\\\"  - 本地词典转换: {by_type['local_conversion']}\\\")\\n    print(f\\\"  - LLM识别转换: {by_type['llm_conversion']}\\\")\\n    \\n    print(f\\\"\\\\n🔄 拼写转换详情:\\\")\\n    for category, changes in by_category.items():\\n        print(f\\\"\\\\n  【{category.upper()}】:\\\")\\n        for change in changes:\\n            print(f\\\"    {change['original']} → {change['corrected']} ({change['pos']})\\\")\\n\\ndef update_dataset_spelling(dataset, spelling_changes):\\n    \\\"\\\"\\\"更新数据集中的拼写\\\"\\\"\\\"\\n    if not spelling_changes:\\n        return dataset, 0\\n    \\n    # 创建转换映射\\n    conversion_map = {change['original']: change['corrected'] for change in spelling_changes}\\n    \\n    updated_count = 0\\n    updated_dataset = []\\n    \\n    for entry in dataset:\\n        updated_entry = entry.copy()\\n        \\n        # 检查target word\\n        if entry['target'] in conversion_map:\\n            updated_entry['target'] = conversion_map[entry['target']]\\n            updated_count += 1\\n        \\n        # 检查taboo words\\n        updated_taboo = []\\n        for taboo_word in entry['taboo']:\\n            if taboo_word in conversion_map:\\n                updated_taboo.append(conversion_map[taboo_word])\\n            else:\\n                updated_taboo.append(taboo_word)\\n        updated_entry['taboo'] = updated_taboo\\n        \\n        updated_dataset.append(updated_entry)\\n    \\n    return updated_dataset, updated_count\n",
        "\n",
        "# 执行英国拼写检查\n",
        "print(\\\"=== 英国拼写检查开始 ===\\\")\\nprint(\\\"🔍 正在检查数据集中的拼写并转换为英国英语...\\\")\\n\\n# 创建转换字典\\nconversion_dict = create_spelling_conversion_dict()\\nprint(f\\\"📚 已加载 {len(conversion_dict)} 个拼写转换规则\\\")\\n\\n# 加载数据集和API密钥\\ndataset = load_generated_dataset()\\napi_key = load_api_key()\\n\\nif dataset:\\n    print(f\\\"✅ 数据集已加载，共 {len(dataset)} 个词汇\\\")\\n    \\n    # 提取target words\\n    target_words_info = extract_target_words(dataset)\\n    print(f\\\"✅ 已提取所有target words\\\")\\n    \\n    # 批量检查拼写\\n    print(f\\\"🤖 开始进行拼写检查...\\\")\\n    spelling_changes = check_spelling_batch(target_words_info, api_key, conversion_dict)\\n    \\n    # 生成拼写报告\\n    generate_spelling_report(spelling_changes)\\n    \\n    # 更新数据集\\n    if spelling_changes:\\n        updated_dataset, updated_count = update_dataset_spelling(dataset, spelling_changes)\\n        \\n        # 保存更新后的数据集\\n        output_filename = \\\"BRITISH_SPELLING_DATASET.json\\\"\\n        with open(output_filename, 'w', encoding='utf-8') as f:\\n            json.dump(updated_dataset, f, indent=2, ensure_ascii=False)\\n        \\n        print(f\\\"\\\\n✅ 已更新 {updated_count} 个target words的拼写\\\")\\n        print(f\\\"📄 更新后的数据集已保存到: {output_filename}\\\")\\n        \\n        # 保存拼写转换报告\\n        with open('spelling_conversion_report.json', 'w', encoding='utf-8') as f:\\n            json.dump(spelling_changes, f, indent=2, ensure_ascii=False)\\n        print(f\\\"📄 拼写转换报告已保存到: spelling_conversion_report.json\\\")\\n    else:\\n        print(f\\\"\\\\n✅ 数据集拼写检查完成，无需修改\\\")\\n        \\nelse:\\n    print(\\\"❌ 无法加载数据集\\\")\\n\\nprint(\\\"\\\\n=== 英国拼写检查完成 ===\\\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 英国拼写检查开始 ===\n",
            "🔍 正在使用LLM检查数据集中的拼写并转换为英国英语...\n",
            "✅ 数据集已加载，共 300 个词汇\n",
            "✅ 已提取所有target words\n",
            "🤖 开始使用LLM进行拼写检查...\n",
            "🔍 正在检查批次 1/15 的拼写 (20 个词汇)...\n",
            "🔍 正在检查批次 2/15 的拼写 (20 个词汇)...\n",
            "🔍 正在检查批次 3/15 的拼写 (20 个词汇)...\n",
            "🔍 正在检查批次 4/15 的拼写 (20 个词汇)...\n",
            "🔍 正在检查批次 5/15 的拼写 (20 个词汇)...\n",
            "🔍 正在检查批次 6/15 的拼写 (20 个词汇)...\n",
            "🔍 正在检查批次 7/15 的拼写 (20 个词汇)...\n",
            "🔍 正在检查批次 8/15 的拼写 (20 个词汇)...\n",
            "🔍 正在检查批次 9/15 的拼写 (20 个词汇)...\n",
            "🔍 正在检查批次 10/15 的拼写 (20 个词汇)...\n",
            "🔍 正在检查批次 11/15 的拼写 (20 个词汇)...\n",
            "🔍 正在检查批次 12/15 的拼写 (20 个词汇)...\n",
            "🔍 正在检查批次 13/15 的拼写 (20 个词汇)...\n",
            "🔍 正在检查批次 14/15 的拼写 (20 个词汇)...\n",
            "🔍 正在检查批次 15/15 的拼写 (20 个词汇)...\n",
            "\n",
            "✅ 所有词汇都已经是英国拼写！\n",
            "\n",
            "✅ 数据集拼写检查完成，无需修改\n",
            "\n",
            "=== 英国拼写检查完成 ===\n"
          ]
        }
      ],
      "source": [
        "# 英国拼写检查功能 - 使用LLM直接检查\n",
        "import json\n",
        "import time\n",
        "\n",
        "def check_spelling_with_llm(words_info, api_key, batch_size=20):\n",
        "    \"\"\"使用LLM批量检查拼写并进行转换\"\"\"\n",
        "    if not api_key:\n",
        "        print(\"❌ API密钥不可用，无法进行拼写检查\")\n",
        "        return []\n",
        "        \n",
        "    spelling_changes = []\n",
        "    total_batches = (len(words_info) + batch_size - 1) // batch_size\n",
        "    \n",
        "    for batch_idx in range(0, len(words_info), batch_size):\n",
        "        batch = words_info[batch_idx:batch_idx + batch_size]\n",
        "        batch_num = batch_idx // batch_size + 1\n",
        "        \n",
        "        print(f\"🔍 正在检查批次 {batch_num}/{total_batches} 的拼写 ({len(batch)} 个词汇)...\")\n",
        "        \n",
        "        # 构建批量检查的prompt\n",
        "        word_list = []\n",
        "        for i, word_info in enumerate(batch):\n",
        "            word_list.append(f\"{i+1}. {word_info['word']}\")\n",
        "        words_text = \"\\n\".join(word_list)\n",
        "        \n",
        "        prompt = f\"\"\"请检查以下英文词汇的拼写是否为英国英语拼写。如果是美国英语拼写，请转换为英国英语拼写。\n",
        "\n",
        "常见转换规则：\n",
        "- -ize → -ise (organize → organise)\n",
        "- -or → -our (color → colour)  \n",
        "- -er → -re (center → centre)\n",
        "- -ense → -ence (defense → defence)\n",
        "- 其他常见转换 (gray → grey, aluminum → aluminium)\n",
        "\n",
        "待检查词汇：\n",
        "{words_text}\n",
        "\n",
        "请只返回需要转换的词汇，格式为：\n",
        "编号:原词→英国拼写\n",
        "例如：\n",
        "1:organize→organise\n",
        "3:color→colour\n",
        "\n",
        "如果所有词汇都是英国拼写，请返回\"无需转换\"。\"\"\"\n",
        "\n",
        "        response = call_openrouter_api(prompt, api_key)\n",
        "        if response:\n",
        "            response = response.strip()\n",
        "            if response.lower() not in [\"无需转换\", \"无\", \"none\", \"no changes needed\"]:\n",
        "                try:\n",
        "                    # 解析LLM返回的拼写转换\n",
        "                    lines = response.split('\\n')\n",
        "                    for line in lines:\n",
        "                        line = line.strip()\n",
        "                        if ':' in line and '→' in line:\n",
        "                            parts = line.split(':', 1)\n",
        "                            if len(parts) == 2 and parts[0].strip().isdigit():\n",
        "                                idx = int(parts[0].strip()) - 1\n",
        "                                conversion = parts[1].strip()\n",
        "                                if '→' in conversion:\n",
        "                                    original, corrected = conversion.split('→', 1)\n",
        "                                    original = original.strip()\n",
        "                                    corrected = corrected.strip()\n",
        "                                    if 0 <= idx < len(batch):\n",
        "                                        spelling_changes.append({\n",
        "                                            'original': original,\n",
        "                                            'corrected': corrected,\n",
        "                                            'category': batch[idx]['category'],\n",
        "                                            'pos': batch[idx]['pos'],\n",
        "                                            'type': 'llm_conversion'\n",
        "                                        })\n",
        "                except Exception as e:\n",
        "                    print(f\"  - ⚠️ 解析LLM拼写回复时出错: {e}\")\n",
        "                    print(f\"  - 原始回复: {response[:100]}...\")\n",
        "        \n",
        "        # 显示本批次的转换结果\n",
        "        batch_changes = [c for c in spelling_changes if any(w['word'] == c['original'] for w in batch)]\n",
        "        if batch_changes:\n",
        "            print(f\"  🔄 本批次发现 {len(batch_changes)} 个拼写转换\")\n",
        "            for change in batch_changes[-3:]:  # 显示最后3个\n",
        "                print(f\"    {change['original']} → {change['corrected']}\")\n",
        "        \n",
        "        # 避免API调用过于频繁\n",
        "        if batch_num < total_batches:\n",
        "            time.sleep(1.5)\n",
        "    \n",
        "    return spelling_changes\n",
        "\n",
        "def generate_spelling_report(spelling_changes):\n",
        "    \"\"\"生成拼写检查报告\"\"\"\n",
        "    if not spelling_changes:\n",
        "        print(f\"\\n✅ 所有词汇都已经是英国拼写！\")\n",
        "        return\n",
        "    \n",
        "    print(f\"\\n📋 拼写检查报告\")\n",
        "    print(f\"=\" * 50)\n",
        "    print(f\"发现需要转换的词汇数: {len(spelling_changes)}\")\n",
        "    \n",
        "    # 按类别分组\n",
        "    by_category = {}\n",
        "    \n",
        "    for change in spelling_changes:\n",
        "        category = change['category']\n",
        "        if category not in by_category:\n",
        "            by_category[category] = []\n",
        "        by_category[category].append(change)\n",
        "    \n",
        "    print(f\"\\n🔄 拼写转换详情:\")\n",
        "    for category, changes in by_category.items():\n",
        "        print(f\"\\n  【{category.upper()}】:\")\n",
        "        for change in changes:\n",
        "            print(f\"    {change['original']} → {change['corrected']} ({change['pos']})\")\n",
        "\n",
        "def update_dataset_spelling(dataset, spelling_changes):\n",
        "    \"\"\"更新数据集中的拼写\"\"\"\n",
        "    if not spelling_changes:\n",
        "        return dataset, 0\n",
        "    \n",
        "    # 创建转换映射\n",
        "    conversion_map = {change['original']: change['corrected'] for change in spelling_changes}\n",
        "    \n",
        "    updated_count = 0\n",
        "    updated_dataset = []\n",
        "    \n",
        "    for entry in dataset:\n",
        "        updated_entry = entry.copy()\n",
        "        \n",
        "        # 检查target word\n",
        "        if entry['target'] in conversion_map:\n",
        "            updated_entry['target'] = conversion_map[entry['target']]\n",
        "            updated_count += 1\n",
        "        \n",
        "        # 检查taboo words\n",
        "        updated_taboo = []\n",
        "        for taboo_word in entry['taboo']:\n",
        "            if taboo_word in conversion_map:\n",
        "                updated_taboo.append(conversion_map[taboo_word])\n",
        "            else:\n",
        "                updated_taboo.append(taboo_word)\n",
        "        updated_entry['taboo'] = updated_taboo\n",
        "        \n",
        "        updated_dataset.append(updated_entry)\n",
        "    \n",
        "    return updated_dataset, updated_count\n",
        "\n",
        "# 执行英国拼写检查\n",
        "print(\"=== 英国拼写检查开始 ===\")\n",
        "print(\"🔍 正在使用LLM检查数据集中的拼写并转换为英国英语...\")\n",
        "\n",
        "# 加载数据集和API密钥\n",
        "dataset = load_generated_dataset()\n",
        "api_key = load_api_key()\n",
        "\n",
        "if dataset and api_key:\n",
        "    print(f\"✅ 数据集已加载，共 {len(dataset)} 个词汇\")\n",
        "    \n",
        "    # 提取target words\n",
        "    target_words_info = extract_target_words(dataset)\n",
        "    print(f\"✅ 已提取所有target words\")\n",
        "    \n",
        "    # 使用LLM批量检查拼写\n",
        "    print(f\"🤖 开始使用LLM进行拼写检查...\")\n",
        "    spelling_changes = check_spelling_with_llm(target_words_info, api_key)\n",
        "    \n",
        "    # 生成拼写报告\n",
        "    generate_spelling_report(spelling_changes)\n",
        "    \n",
        "    # 更新数据集\n",
        "    if spelling_changes:\n",
        "        updated_dataset, updated_count = update_dataset_spelling(dataset, spelling_changes)\n",
        "        \n",
        "        # 保存更新后的数据集\n",
        "        output_filename = \"BRITISH_SPELLING_DATASET.json\"\n",
        "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(updated_dataset, f, indent=2, ensure_ascii=False)\n",
        "        \n",
        "        print(f\"\\n✅ 已更新 {updated_count} 个target words的拼写\")\n",
        "        print(f\"📄 更新后的数据集已保存到: {output_filename}\")\n",
        "        \n",
        "        # 保存拼写转换报告\n",
        "        with open('spelling_conversion_report.json', 'w', encoding='utf-8') as f:\n",
        "            json.dump(spelling_changes, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"📄 拼写转换报告已保存到: spelling_conversion_report.json\")\n",
        "    else:\n",
        "        print(f\"\\n✅ 数据集拼写检查完成，无需修改\")\n",
        "        \n",
        "elif not dataset:\n",
        "    print(\"❌ 无法加载数据集\")\n",
        "elif not api_key:\n",
        "    print(\"❌ 无法加载API密钥，拼写检查功能不可用\")\n",
        "\n",
        "print(\"\\n=== 英国拼写检查完成 ===\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 开始基于LLM质量检查结果重新生成数据集 ===\n",
            "📋 发现 1 个需要移除的词汇\n",
            "📊 原数据集共 300 个词汇\n",
            "\\n🗑️ 按类别移除统计:\n",
            "  - philosophy: 1 个\n",
            "\\n✅ 过滤后保留 299 个词汇\n",
            "\\n🔄 开始补充新词汇...\n",
            "\\n📝 为 philosophy 补充 1 个词汇:\n",
            "  可用候选词: 473 个\n",
            "    ✅ abilities (taboo: 5)\n",
            "  📊 成功补充: 1/1\n",
            "\\n✅ 最终数据集已保存: FINAL_DATASET.json\n",
            "📊 最终统计:\n",
            "  - 保留原词汇: 299\n",
            "  - 新增词汇: 1\n",
            "  - 总计: 300\n",
            "\\n📈 类别分布:\n",
            "  - general: 100\n",
            "  - chemistry: 50\n",
            "  - cs: 50\n",
            "  - finance: 50\n",
            "  - philosophy: 50\n",
            "\\n🏆 数据集重新生成完成！\n"
          ]
        }
      ],
      "source": [
        "# 基于LLM质量检查结果重新生成数据集\n",
        "print(\"=== 开始基于LLM质量检查结果重新生成数据集 ===\")\n",
        "\n",
        "# 加载不合适词汇报告\n",
        "try:\n",
        "    with open('inappropriate_words_report.json', 'r', encoding='utf-8') as f:\n",
        "        inappropriate_words = json.load(f)\n",
        "    print(f\"📋 发现 {len(inappropriate_words)} 个需要移除的词汇\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ 无法加载不合适词汇报告: {e}\")\n",
        "    inappropriate_words = []\n",
        "\n",
        "if inappropriate_words:\n",
        "    # 加载原数据集\n",
        "    original_dataset = load_generated_dataset()\n",
        "    if not original_dataset:\n",
        "        print(\"❌ 无法加载原数据集\")\n",
        "    else:\n",
        "        print(f\"📊 原数据集共 {len(original_dataset)} 个词汇\")\n",
        "        \n",
        "        # 创建黑名单\n",
        "        inappropriate_word_set = {item['word'] for item in inappropriate_words}\n",
        "        \n",
        "        # 统计需要移除的词汇按类别分布\n",
        "        removal_by_category = {}\n",
        "        for item in inappropriate_words:\n",
        "            category = item['category'] \n",
        "            removal_by_category[category] = removal_by_category.get(category, 0) + 1\n",
        "        \n",
        "        print(f\"\\\\n🗑️ 按类别移除统计:\")\n",
        "        for category, count in removal_by_category.items():\n",
        "            print(f\"  - {category}: {count} 个\")\n",
        "        \n",
        "        # 过滤数据集\n",
        "        filtered_dataset = [entry for entry in original_dataset if entry['target'] not in inappropriate_word_set]\n",
        "        print(f\"\\\\n✅ 过滤后保留 {len(filtered_dataset)} 个词汇\")\n",
        "        \n",
        "        # 从现有候选池补充新词汇\n",
        "        print(f\"\\\\n🔄 开始补充新词汇...\")\n",
        "        new_entries = []\n",
        "        \n",
        "        # 创建黑名单（包含已移除和已存在的词汇）\n",
        "        existing_words = {entry['target'] for entry in filtered_dataset}\n",
        "        blacklist = inappropriate_word_set | existing_words\n",
        "        \n",
        "        # 为每个类别补充词汇\n",
        "        for category, needed_count in removal_by_category.items():\n",
        "            print(f\"\\\\n📝 为 {category} 补充 {needed_count} 个词汇:\")\n",
        "            \n",
        "            # 选择候选池\n",
        "            if category == 'general':\n",
        "                candidates = [w for w in wn.words() \n",
        "                            if (w.isalpha() and '_' not in w and 3 < len(w) < 16 and \n",
        "                                w not in blacklist and wn.synsets(w))]\n",
        "            else:\n",
        "                # 从专业词汇池选择\n",
        "                candidates = []\n",
        "                if category in professional_pools:\n",
        "                    candidates = [w for w in professional_pools[category] \n",
        "                                if w not in blacklist and wn.synsets(w)]\n",
        "            \n",
        "            print(f\"  可用候选词: {len(candidates)} 个\")\n",
        "            random.shuffle(candidates)\n",
        "            \n",
        "            added = 0\n",
        "            for word in candidates[:needed_count * 3]:  # 尝试更多候选词\n",
        "                if added >= needed_count:\n",
        "                    break\n",
        "                    \n",
        "                try:\n",
        "                    # 生成完整词汇条目\n",
        "                    senses = get_all_senses(word)\n",
        "                    if not senses:\n",
        "                        continue\n",
        "                        \n",
        "                    primary_synset = wn.synset(senses[0]['name'])\n",
        "                    pos_map = {'n': 'noun', 'v': 'verb', 'a': 'adj', 'r': 'adverb', 's': 'adj'}\n",
        "                    main_pos = pos_map.get(senses[0]['pos'], 'other')\n",
        "                    \n",
        "                    # 生成taboo words\n",
        "                    taboo_words = generate_enhanced_taboo_words(primary_synset, word, senses)\n",
        "                    \n",
        "                    # LLM补充（如果需要）\n",
        "                    if len(taboo_words) < 5 and OPENROUTER_API_KEY:\n",
        "                        llm_taboos = generate_llm_taboo_words(\n",
        "                            word, taboo_words, main_pos, \n",
        "                            primary_synset.definition(), OPENROUTER_API_KEY\n",
        "                        )\n",
        "                        if llm_taboos:\n",
        "                            taboo_words.extend(llm_taboos)\n",
        "                        taboo_words = taboo_words[:5]\n",
        "                    \n",
        "                    # 创建条目\n",
        "                    new_entry = {\n",
        "                        \"target\": word,\n",
        "                        \"part_of_speech\": main_pos,\n",
        "                        \"taboo\": taboo_words,\n",
        "                        \"category\": category,\n",
        "                        \"senses\": senses,\n",
        "                        \"metadata\": {\n",
        "                            \"sense_count\": len(senses),\n",
        "                            \"concreteness_score\": round(concreteness_lookup.get(word, 0), 2) if concreteness_lookup.get(word) else None,\n",
        "                            \"taboo_count\": len(taboo_words)\n",
        "                        }\n",
        "                    }\n",
        "                    \n",
        "                    new_entries.append(new_entry)\n",
        "                    blacklist.add(word)\n",
        "                    added += 1\n",
        "                    \n",
        "                    print(f\"    ✅ {word} (taboo: {len(taboo_words)})\")\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"    ⚠️ 跳过 {word}: {e}\")\n",
        "                    continue\n",
        "            \n",
        "            print(f\"  📊 成功补充: {added}/{needed_count}\")\n",
        "        \n",
        "        # 合并数据集\n",
        "        final_dataset = filtered_dataset + new_entries\n",
        "        \n",
        "        # 保存最终数据集\n",
        "        with open('FINAL_DATASET.json', 'w', encoding='utf-8') as f:\n",
        "            json.dump(final_dataset, f, indent=2, ensure_ascii=False)\n",
        "        \n",
        "        print(f\"\\\\n✅ 最终数据集已保存: FINAL_DATASET.json\")\n",
        "        print(f\"📊 最终统计:\")\n",
        "        print(f\"  - 保留原词汇: {len(filtered_dataset)}\")\n",
        "        print(f\"  - 新增词汇: {len(new_entries)}\")\n",
        "        print(f\"  - 总计: {len(final_dataset)}\")\n",
        "        \n",
        "        # 类别分布\n",
        "        final_df = pd.DataFrame(final_dataset)\n",
        "        print(f\"\\\\n📈 类别分布:\")\n",
        "        for category, count in final_df['category'].value_counts().items():\n",
        "            print(f\"  - {category}: {count}\")\n",
        "        \n",
        "        print(f\"\\\\n🏆 数据集重新生成完成！\")\n",
        "else:\n",
        "    print(\"✅ 没有发现需要移除的词汇，数据集质量很好！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "300个词的词性分布:\n",
            "noun: 256 (85.3%)\n",
            "adj: 32 (10.7%)\n",
            "verb: 12 (4.0%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 20010 (\\N{CJK UNIFIED IDEOGRAPH-4E2A}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/opt/homebrew/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 35789 (\\N{CJK UNIFIED IDEOGRAPH-8BCD}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/opt/homebrew/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 30340 (\\N{CJK UNIFIED IDEOGRAPH-7684}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/opt/homebrew/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 24615 (\\N{CJK UNIFIED IDEOGRAPH-6027}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/opt/homebrew/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 20998 (\\N{CJK UNIFIED IDEOGRAPH-5206}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/opt/homebrew/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 24067 (\\N{CJK UNIFIED IDEOGRAPH-5E03}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 统计并可视化300个词的词性分布\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 加载数据集\n",
        "try:\n",
        "    with open('ENHANCED_DATASET.json', 'r', encoding='utf-8') as f:\n",
        "        dataset = json.load(f)\n",
        "except FileNotFoundError:\n",
        "    with open('data/dataset.json', 'r', encoding='utf-8') as f:\n",
        "        dataset = json.load(f)\n",
        "\n",
        "df = pd.DataFrame(dataset)\n",
        "pos_counts = df['part_of_speech'].value_counts()\n",
        "pos_perc = df['part_of_speech'].value_counts(normalize=True) * 100\n",
        "\n",
        "print('300个词的词性分布:')\n",
        "for pos in pos_counts.index:\n",
        "    print(f\"{pos}: {pos_counts[pos]} ({pos_perc[pos]:.1f}%)\")\n",
        "\n",
        "# 饼图可视化\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.pie(pos_counts, labels=[f\"{k} ({v}个)\" for k,v in pos_counts.items()], autopct='%1.1f%%', startangle=90)\n",
        "plt.title('300个词的词性分布')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "quick80数据集已生成，共80个词。已保存为quick80_dataset.json\n",
            "\n",
            "【noun】\n",
            "  从300词数据集抽取（20）：\n",
            "    behaviorism, carcharhinus, aphrodisiac, futures, stylus, strength, authenticity, liquidity, wasteland, bevy, debenture, substance, architecture, perfectionism, virus, psittacidae, unemployment, short, necrosis, parentage\n",
            "  从WordNet补充（0）：\n",
            "    \n",
            "\n",
            "【adj】\n",
            "  从300词数据集抽取（20）：\n",
            "    biological, scandent, immutable, epideictical, friendless, trinuclear, many, environmental, exilic, cubiform, quaggy, livable, coital, convincible, biomedical, allometric, ungovernable, nomothetic, combinatory, stomatal\n",
            "  从WordNet补充（0）：\n",
            "    \n",
            "\n",
            "【adv】\n",
            "  从300词数据集抽取（0）：\n",
            "    \n",
            "  从WordNet补充（20）：\n",
            "    obtrusively, permissively, nervily, past, chattily, forbiddingly, sneakingly, wickedly, tonight, concisely, p.m., insanely, capably, low, kinda, inquiringly, often, con, centennially, unmemorably\n",
            "\n",
            "【verb】\n",
            "  从300词数据集抽取（12）：\n",
            "    dishearten, depend, calumniate, promote, blinded, lignify, inoculate, atomize, unzipping, booting, backtracking, underwriting\n",
            "  从WordNet补充（8）：\n",
            "    dispensed, bevel, mushroom, believing, caulk, ranking, farm, brooks\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "import pandas as pd\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "# ========== LLM相关函数 ==========\n",
        "def load_api_key():\n",
        "    try:\n",
        "        with open('api_keys.json', 'r') as f:\n",
        "            api_keys = json.load(f)\n",
        "        return api_keys.get('OPENROUTER_API_KEY', None)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def call_openrouter_api(prompt, api_key, max_retries=3):\n",
        "    import requests, time\n",
        "    if not api_key:\n",
        "        return None\n",
        "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": \"deepseek/deepseek-chat-v3-0324:free\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "        \"max_tokens\": 150,\n",
        "        \"temperature\": 0.7\n",
        "    }\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.post(url, headers=headers, json=data, timeout=30)\n",
        "            if response.status_code == 200:\n",
        "                result = response.json()\n",
        "                return result['choices'][0]['message']['content'].strip()\n",
        "            else:\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(2 ** attempt)\n",
        "        except Exception:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(2 ** attempt)\n",
        "    return None\n",
        "\n",
        "def generate_llm_taboo_words(target_word, existing_taboos, part_of_speech, definition, api_key):\n",
        "    needed_count = 5 - len(existing_taboos)\n",
        "    if needed_count <= 0 or not api_key:\n",
        "        return []\n",
        "    existing_str = \", \".join(existing_taboos) if existing_taboos else \"无\"\n",
        "    prompt = f\"\"\"为Taboo游戏生成禁用词。\n",
        "\n",
        "目标词: {target_word}\n",
        "词性: {part_of_speech}  \n",
        "定义: {definition}\n",
        "已有禁用词: {existing_str}\n",
        "\n",
        "请为\"{target_word}\"生成{needed_count}个新的禁用词，要求：\n",
        "1. 与目标词语义相关但不能直接说出\n",
        "2. 不能重复已有禁用词  \n",
        "3. 优先选择同义词、相关概念、上下位词\n",
        "4. 每个词用英文，单个词汇，小写\n",
        "5. 避免过于通用的词\n",
        "\n",
        "只输出{needed_count}个英文单词，用逗号分隔，不要其他内容。\"\"\"\n",
        "    response = call_openrouter_api(prompt, api_key)\n",
        "    if response:\n",
        "        words = [w.strip().lower() for w in response.split(',')]\n",
        "        valid_words = []\n",
        "        for word in words:\n",
        "            if (word.isalpha() and len(word) > 2 and \n",
        "                word not in existing_taboos and \n",
        "                word != target_word.lower() and\n",
        "                len(valid_words) < needed_count):\n",
        "                valid_words.append(word)\n",
        "        return valid_words\n",
        "    return []\n",
        "\n",
        "# ========== 加载数据 ==========\n",
        "try:\n",
        "    with open('ENHANCED_DATASET.json', 'r', encoding='utf-8') as f:\n",
        "        dataset = json.load(f)\n",
        "except FileNotFoundError:\n",
        "    with open('data/dataset.json', 'r', encoding='utf-8') as f:\n",
        "        dataset = json.load(f)\n",
        "\n",
        "df = pd.DataFrame(dataset)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "api_key = load_api_key()\n",
        "\n",
        "# 目标词性\n",
        "pos_targets = {\n",
        "    'noun': ['noun', 'n'],\n",
        "    'adj': ['adj', 'a', 's'],\n",
        "    'adv': ['adverb', 'r'],\n",
        "    'verb': ['verb', 'v']\n",
        "}\n",
        "quick80 = []\n",
        "used_targets = set()\n",
        "from_300 = {k: [] for k in pos_targets}\n",
        "from_wn = {k: [] for k in pos_targets}\n",
        "\n",
        "# 增强版 taboo word 生成器\n",
        "def generate_taboo_words(syn, target_word):\n",
        "    taboo_set = set()\n",
        "    target_word_lower = target_word.lower()\n",
        "    # 同义词\n",
        "    for lemma in syn.lemmas():\n",
        "        synonym = lemma.name().lower().replace('_', ' ')\n",
        "        if synonym != target_word_lower and len(synonym.split()) <= 2:\n",
        "            main_word = synonym.split()[0]\n",
        "            if main_word.isalpha() and len(main_word) > 2:\n",
        "                taboo_set.add(main_word)\n",
        "            if len(synonym.split()) <= 2 and all(w.isalpha() for w in synonym.split()):\n",
        "                taboo_set.add(synonym.replace(' ', ''))\n",
        "    # 反义词\n",
        "    if syn.lemmas():\n",
        "        for antonym_lemma in syn.lemmas()[0].antonyms():\n",
        "            antonym = antonym_lemma.name().lower().replace('_', ' ')\n",
        "            main_word = antonym.split()[0]\n",
        "            if main_word.isalpha() and len(main_word) > 2:\n",
        "                taboo_set.add(main_word)\n",
        "    # 定义关键词\n",
        "    definition = syn.definition()\n",
        "    tokens = word_tokenize(definition)\n",
        "    tagged = pos_tag(tokens)\n",
        "    for word, tag in tagged:\n",
        "        word_lower = word.lower()\n",
        "        if (tag.startswith(('NN', 'VB', 'JJ')) and \n",
        "            word_lower.isalpha() and \n",
        "            word_lower not in stop_words and \n",
        "            len(word_lower) > 3 and \n",
        "            word_lower != target_word_lower):\n",
        "            taboo_set.add(word_lower)\n",
        "    # 上下位词\n",
        "    try:\n",
        "        for hypernym in syn.hypernyms():\n",
        "            for lemma in hypernym.lemmas():\n",
        "                hyper_word = lemma.name().lower().replace('_', ' ').split()[0]\n",
        "                if (hyper_word.isalpha() and len(hyper_word) > 3 and hyper_word != target_word_lower):\n",
        "                    taboo_set.add(hyper_word)\n",
        "        hyponyms = syn.hyponyms()[:2]\n",
        "        for hyponym in hyponyms:\n",
        "            for lemma in hyponym.lemmas():\n",
        "                hypo_word = lemma.name().lower().replace('_', ' ').split()[0]\n",
        "                if (hypo_word.isalpha() and len(hypo_word) > 3 and hypo_word != target_word_lower):\n",
        "                    taboo_set.add(hypo_word)\n",
        "    except:\n",
        "        pass\n",
        "    # 过滤和排序\n",
        "    taboo_list = list(taboo_set)\n",
        "    taboo_list.sort(key=lambda x: (abs(len(x) - 6), x))\n",
        "    final_taboos = []\n",
        "    seen = set()\n",
        "    for word in taboo_list:\n",
        "        if word not in seen and len(final_taboos) < 5:\n",
        "            final_taboos.append(word)\n",
        "            seen.add(word)\n",
        "    return final_taboos\n",
        "\n",
        "# 只对WordNet补充的词生成taboo\n",
        "def build_entry_from_wordnet(word, pos_tag):\n",
        "    synsets = wn.synsets(word)\n",
        "    if not synsets:\n",
        "        return None\n",
        "    # 取第一个匹配词性的synset\n",
        "    for s in synsets:\n",
        "        if pos_tag == 'noun' and s.pos() == 'n':\n",
        "            syn = s\n",
        "            break\n",
        "        elif pos_tag == 'adj' and s.pos() in ['a', 's']:\n",
        "            syn = s\n",
        "            break\n",
        "        elif pos_tag == 'adv' and s.pos() == 'r':\n",
        "            syn = s\n",
        "            break\n",
        "        elif pos_tag == 'verb' and s.pos() == 'v':\n",
        "            syn = s\n",
        "            break\n",
        "    else:\n",
        "        return None\n",
        "    pos_map = {'n': 'noun', 'v': 'verb', 'a': 'adj', 's': 'adj', 'r': 'adv'}\n",
        "    main_pos = pos_map.get(syn.pos(), 'other')\n",
        "    taboo = generate_taboo_words(syn, word)\n",
        "    # LLM补足\n",
        "    if len(taboo) < 5 and api_key:\n",
        "        llm_taboos = generate_llm_taboo_words(\n",
        "            word, taboo, main_pos, syn.definition(), api_key\n",
        "        )\n",
        "        taboo.extend(llm_taboos)\n",
        "    taboo = taboo[:5]\n",
        "    entry = {\n",
        "        'target': word,\n",
        "        'part_of_speech': main_pos,\n",
        "        'taboo': taboo,\n",
        "        'category': 'general',\n",
        "        'senses': [{\n",
        "            'name': syn.name(),\n",
        "            'pos': syn.pos(),\n",
        "            'definition': syn.definition(),\n",
        "            'examples': syn.examples()\n",
        "        }],\n",
        "        'metadata': {\n",
        "            'sense_count': len(synsets),\n",
        "            'concreteness_score': None,\n",
        "            'taboo_count': len(taboo)\n",
        "        }\n",
        "    }\n",
        "    return entry\n",
        "\n",
        "# 1. 先从现有数据集中抽取（直接用原taboo字段）\n",
        "for pos, pos_keys in pos_targets.items():\n",
        "    sub = df[df['part_of_speech'].isin(pos_keys)]\n",
        "    selected = sub.sample(n=min(20, len(sub)), random_state=42) if len(sub) >= 20 else sub\n",
        "    for _, row in selected.iterrows():\n",
        "        quick80.append(dict(row))\n",
        "        used_targets.add(row['target'])\n",
        "        from_300[pos].append(row['target'])\n",
        "\n",
        "# 2. 如果某词性不足20个，从WordNet补足（用增强算法+LLM补足）\n",
        "for pos in pos_targets:\n",
        "    needed = 20 - sum(1 for e in quick80 if e['part_of_speech'] in pos_targets[pos])\n",
        "    if needed > 0:\n",
        "        wn_words = [w for w in wn.words() if w not in used_targets and '_' not in w]\n",
        "        random.shuffle(wn_words)\n",
        "        count = 0\n",
        "        for w in wn_words:\n",
        "            entry = build_entry_from_wordnet(w, pos)\n",
        "            if entry:\n",
        "                quick80.append(entry)\n",
        "                used_targets.add(w)\n",
        "                from_wn[pos].append(w)\n",
        "                count += 1\n",
        "            if count >= needed:\n",
        "                break\n",
        "\n",
        "# 3. 保存\n",
        "with open('quick80_dataset.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(quick80, f, indent=2, ensure_ascii=False)\n",
        "print(f\"quick80数据集已生成，共{len(quick80)}个词。已保存为quick80_dataset.json\")\n",
        "\n",
        "# 4. 输出每个词性来源\n",
        "for pos in pos_targets:\n",
        "    print(f\"\\n【{pos}】\")\n",
        "    print(f\"  从300词数据集抽取（{len(from_300[pos])}）：\")\n",
        "    print(\"   \", ', '.join(from_300[pos]))\n",
        "    print(f\"  从WordNet补充（{len(from_wn[pos])}）：\")\n",
        "    print(\"   \", ', '.join(from_wn[pos]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 开始筛选新增样本 ===\n",
            "✅ 成功加载 300 个词作为参照基准。\n",
            "✅ 成功加载 80 个词用于筛选。\n",
            "📊 筛选完成！发现 28 个从 WordNet 新增的样本。\n",
            "🏆 成功！新数据集已保存为: 'quick80_from_wordnet_only.json'\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def create_wordnet_only_dataset():\n",
        "    \"\"\"\n",
        "    过滤 quick80_dataset.json，只保留从 WordNet 新增的词汇。\n",
        "    \"\"\"\n",
        "    # 定义文件路径\n",
        "    enhanced_dataset_path = 'data/dataset.json'\n",
        "    quick80_dataset_path = 'quick80_dataset.json'\n",
        "    output_path = 'quick80_from_wordnet_only.json'\n",
        "\n",
        "    # 检查所需文件是否存在\n",
        "    if not os.path.exists(enhanced_dataset_path):\n",
        "        print(f\"❌ 错误: 源数据集 '{enhanced_dataset_path}' 未找到。无法进行比较。\")\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(quick80_dataset_path):\n",
        "        print(f\"❌ 错误: 数据集 '{quick80_dataset_path}' 未找到。没有可供筛选的数据。\")\n",
        "        return\n",
        "\n",
        "    print(\"=== 开始筛选新增样本 ===\")\n",
        "\n",
        "    # 1. 加载300词数据集，并提取所有目标词作为“旧词”列表\n",
        "    try:\n",
        "        with open(enhanced_dataset_path, 'r', encoding='utf-8') as f:\n",
        "            enhanced_dataset = json.load(f)\n",
        "        words_from_300 = {entry['target'] for entry in enhanced_dataset}\n",
        "        print(f\"✅ 成功加载 {len(words_from_300)} 个词作为参照基准。\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 加载 '{enhanced_dataset_path}' 时出错: {e}\")\n",
        "        return\n",
        "\n",
        "    # 2. 加载 quick80 数据集\n",
        "    try:\n",
        "        with open(quick80_dataset_path, 'r', encoding='utf-8') as f:\n",
        "            quick80_dataset = json.load(f)\n",
        "        print(f\"✅ 成功加载 {len(quick80_dataset)} 个词用于筛选。\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 加载 '{quick80_dataset_path}' 时出错: {e}\")\n",
        "        return\n",
        "\n",
        "    # 3. 筛选出所有“新词”\n",
        "    new_samples_from_wordnet = []\n",
        "    for entry in quick80_dataset:\n",
        "        if entry.get('target') not in words_from_300:\n",
        "            new_samples_from_wordnet.append(entry)\n",
        "    \n",
        "    print(f\"📊 筛选完成！发现 {len(new_samples_from_wordnet)} 个从 WordNet 新增的样本。\")\n",
        "\n",
        "    # 4. 保存新数据集\n",
        "    try:\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(new_samples_from_wordnet, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"🏆 成功！新数据集已保存为: '{output_path}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 保存新数据集时出错: {e}\")\n",
        "\n",
        "# --- 直接运行函数 ---\n",
        "create_wordnet_only_dataset()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
