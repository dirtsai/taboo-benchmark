{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== æ•°æ®é›†ç”Ÿæˆè„šæœ¬ V35 (å®Œæ•´é›†æˆç‰ˆæœ¬) ===\n",
            "ğŸ”§ ä¿®å¤è·¯å¾„é—®é¢˜ + ğŸš€ å¢å¼ºç®—æ³• + ğŸ¤– LLMæ™ºèƒ½è¡¥å……\n",
            "âœ… LLM API å·²å¯ç”¨\n",
            "\n",
            "â³ æ­¥éª¤ä¸€ï¼šæ­£åœ¨å¼ºåˆ¶æ£€æŸ¥å¹¶ä¸‹è½½æ‰€æœ‰å¿…éœ€çš„NLTKæ•°æ®åŒ…...\n",
            "  - ä¸‹è½½ä¸­: 'wordnet'...\n",
            "  - ä¸‹è½½ä¸­: 'omw-1.4'...\n",
            "âœ… NLTKä¾èµ–é¡¹å·²å‡†å¤‡å°±ç»ªï¼\n",
            "\n",
            "ğŸ“¦ æ­¥éª¤äºŒï¼šæ­£åœ¨åŠ è½½æ‰€æœ‰åŸææ–™ï¼ˆä¿®å¤åçš„è·¯å¾„ï¼‰...\n",
            "  - âœ… CHEMISTRY: æˆåŠŸåŠ è½½å¹¶è§£æäº† 1840 ä¸ªå€™é€‰è¯ã€‚\n",
            "  - âœ… CS: æˆåŠŸåŠ è½½å¹¶è§£æäº† 206 ä¸ªå€™é€‰è¯ã€‚\n",
            "  - âœ… FINANCE: æˆåŠŸåŠ è½½å¹¶è§£æäº† 110 ä¸ªå€™é€‰è¯ã€‚\n",
            "  - âœ… PHILOSOPHY: æˆåŠŸåŠ è½½å¹¶è§£æäº† 530 ä¸ªå€™é€‰è¯ã€‚\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /Users/czl/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /Users/czl/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  - âœ… GENERAL: å€™é€‰æ± å·²å‡†å¤‡å¥½ (71909 ä¸ªè¯)ã€‚\n",
            "  - âœ… CONCRETENESS: åˆ†æ•°æ•°æ®åŠ è½½æˆåŠŸã€‚\n",
            "\n",
            "ğŸ¯ æ­¥éª¤ä¸‰ï¼šæ­£åœ¨è¿›è¡Œæ™ºèƒ½æŠ½æ ·ï¼ˆå®æ—¶LLMè´¨é‡æ£€æŸ¥ï¼‰...\n",
            "\n",
            "ğŸ” æ­£åœ¨ä¸º 'general' ç±»åˆ«æ™ºèƒ½é€‰æ‹© 100 ä¸ªè¯æ±‡:\n",
            "    âœ… æ¥å—: crotonbug\n",
            "    âœ… æ¥å—: entitlement\n",
            "    âœ… æ¥å—: hinge\n",
            "    âœ… æ¥å—: cubiform\n",
            "    âœ… æ¥å—: mantle\n",
            "    âŒ æ‹’ç»: bullshot - ä¸åˆé€‚:ä¸“æœ‰åè¯\n",
            "    âœ… æ¥å—: evening\n",
            "    âŒ æ‹’ç»: polybotria - ä¸åˆé€‚:åŸå› ï¼šå¯èƒ½æ˜¯ä¸“æœ‰åè¯æˆ–è™šæ„è¯æ±‡ï¼Œç¼ºä¹æ™®éè®¤å¯çš„å®šä¹‰ã€‚\n",
            "    âœ… æ¥å—: clitoris\n",
            "    âŒ æ‹’ç»: zoisia - ä¸åˆé€‚:åŸå› \n",
            "    âœ… æ¥å—: velcro\n",
            "    ... (åç»­æ‹’ç»çš„è¯æ±‡ä¸å†æ˜¾ç¤º)\n",
            "    âœ… æ¥å—: trinuclear\n",
            "    âœ… æ¥å—: promote\n",
            "    âœ… æ¥å—: flagellation\n",
            "    âœ… æ¥å—: marlinespike\n",
            "    âœ… æ¥å—: pandemic\n",
            "    âœ… æ¥å—: cistaceae\n",
            "    âœ… æ¥å—: jacobinic\n",
            "  ğŸ“Š 'general' å®Œæˆ: æˆåŠŸé€‰æ‹© 100/100, æ£€æŸ¥äº† 116 ä¸ªå€™é€‰è¯, æ‹’ç»äº† 16 ä¸ª\n",
            "\n",
            "ğŸ” æ­£åœ¨ä¸º 'chemistry' ç±»åˆ«æ™ºèƒ½é€‰æ‹© 50 ä¸ªè¯æ±‡:\n",
            "    âœ… æ¥å—: fumigation\n",
            "    âœ… æ¥å—: tellurides\n",
            "    âœ… æ¥å—: organization\n",
            "    âœ… æ¥å—: conceptus\n",
            "    âœ… æ¥å—: phenols\n",
            "    âŒ æ‹’ç»: sperm - ä¸åˆé€‚:è‰²æƒ…å†…å®¹\n",
            "    âœ… æ¥å—: texture\n",
            "    âŒ æ‹’ç»: microphallus - ä¸åˆé€‚:è‰²æƒ…å†…å®¹\n",
            "    âœ… æ¥å—: atomization\n",
            "    âœ… æ¥å—: weber\n",
            "    âœ… æ¥å—: zone\n",
            "    âœ… æ¥å—: allometric\n",
            "  ğŸ“Š 'chemistry' å®Œæˆ: æˆåŠŸé€‰æ‹© 50/50, æ£€æŸ¥äº† 52 ä¸ªå€™é€‰è¯, æ‹’ç»äº† 2 ä¸ª\n",
            "\n",
            "ğŸ” æ­£åœ¨ä¸º 'cs' ç±»åˆ«æ™ºèƒ½é€‰æ‹© 50 ä¸ªè¯æ±‡:\n",
            "    âœ… æ¥å—: stylus\n",
            "    âœ… æ¥å—: software\n",
            "    âœ… æ¥å—: binary\n",
            "    âœ… æ¥å—: identifier\n",
            "    âœ… æ¥å—: stack\n",
            "    âœ… æ¥å—: socket\n",
            "    âœ… æ¥å—: selection\n",
            "    âŒ æ‹’ç»: terminator - ä¸åˆé€‚:ä¸“æœ‰åè¯\n",
            "    âœ… æ¥å—: syntax\n",
            "    âœ… æ¥å—: property\n",
            "    âœ… æ¥å—: style\n",
            "  ğŸ“Š 'cs' å®Œæˆ: æˆåŠŸé€‰æ‹© 50/50, æ£€æŸ¥äº† 51 ä¸ªå€™é€‰è¯, æ‹’ç»äº† 1 ä¸ª\n",
            "\n",
            "ğŸ” æ­£åœ¨ä¸º 'finance' ç±»åˆ«æ™ºèƒ½é€‰æ‹© 50 ä¸ªè¯æ±‡:\n",
            "    âœ… æ¥å—: macroeconomics\n",
            "    âœ… æ¥å—: entrepreneur\n",
            "    âœ… æ¥å—: economics\n",
            "    âœ… æ¥å—: liquidity\n",
            "    âœ… æ¥å—: environmental\n",
            "    âœ… æ¥å—: nominal\n",
            "    âœ… æ¥å—: asset\n",
            "    âœ… æ¥å—: novation\n",
            "    âŒ æ‹’ç»: jensen - ä¸åˆé€‚:äººå\n",
            "    âŒ æ‹’ç»: moore - ä¸åˆé€‚:äººå\n",
            "    âœ… æ¥å—: upside\n",
            "    âœ… æ¥å—: guarantor\n",
            "  ğŸ“Š 'finance' å®Œæˆ: æˆåŠŸé€‰æ‹© 50/50, æ£€æŸ¥äº† 52 ä¸ªå€™é€‰è¯, æ‹’ç»äº† 2 ä¸ª\n",
            "\n",
            "ğŸ” æ­£åœ¨ä¸º 'philosophy' ç±»åˆ«æ™ºèƒ½é€‰æ‹© 50 ä¸ªè¯æ±‡:\n",
            "    âŒ æ‹’ç»: camus - ä¸åˆé€‚:äººå\n",
            "    âœ… æ¥å—: combinatory\n",
            "    âœ… æ¥å—: abduction\n",
            "    âŒ æ‹’ç»: ross - ä¸åˆé€‚:ä¸“æœ‰åè¯\n",
            "    âœ… æ¥å—: pacifism\n",
            "    âŒ æ‹’ç»: abortion - ä¸åˆé€‚:æ¶‰åŠä¸¥é‡æ•æ„Ÿå†…å®¹\n",
            "    âœ… æ¥å—: methods\n",
            "    âœ… æ¥å—: prior\n",
            "    ... (åç»­æ‹’ç»çš„è¯æ±‡ä¸å†æ˜¾ç¤º)\n",
            "    âœ… æ¥å—: discrimination\n",
            "    âœ… æ¥å—: myths\n",
            "    âœ… æ¥å—: humor\n",
            "    âœ… æ¥å—: death\n",
            "    âœ… æ¥å—: conservatism\n",
            "  ğŸ“Š 'philosophy' å®Œæˆ: æˆåŠŸé€‰æ‹© 50/50, æ£€æŸ¥äº† 67 ä¸ªå€™é€‰è¯, æ‹’ç»äº† 17 ä¸ª\n",
            "\n",
            "âœ… æ™ºèƒ½æŠ½æ ·å®Œæˆ! (é›†æˆäº†å®æ—¶LLMè´¨é‡æ£€æŸ¥)\n",
            "\n",
            "ğŸ­ æ­¥éª¤å››ï¼šæ­£åœ¨ä¸º 300 ä¸ªè¯è¿›è¡Œæœ€ç»ˆæ•°æ®ä¸°å¯ŒåŒ–...\n",
            "ğŸš€ ä½¿ç”¨å¢å¼ºç‰ˆç®—æ³• + LLMæ™ºèƒ½è¡¥å……\n",
            "  - ä¸°å¯ŒåŒ–è¿›åº¦ 50/300: promote\n",
            "  - ä¸°å¯ŒåŒ–è¿›åº¦ 100/300: jacobinic\n",
            "  - ä¸°å¯ŒåŒ–è¿›åº¦ 150/300: allometric\n",
            "  - ä¸°å¯ŒåŒ–è¿›åº¦ 200/300: style\n",
            "  - ä¸°å¯ŒåŒ–è¿›åº¦ 250/300: guarantor\n",
            "  - ä¸°å¯ŒåŒ–è¿›åº¦ 300/300: conservatism\n",
            "\n",
            "--- æ•°æ®ä¸°å¯ŒåŒ–å®Œæˆ ---\n",
            "âœ…âœ…âœ… æ‚¨çš„å¢å¼ºç‰ˆæ•°æ®é›†å·²ç”Ÿæˆï¼æ–‡ä»¶åä¸º: 'ENHANCED_DATASET.json'\n",
            "æ€»è¯æ±‡æ•°: 300\n",
            "\n",
            "ğŸ“Š Taboo Word ç”Ÿæˆç»Ÿè®¡:\n",
            "  - æˆåŠŸç”Ÿæˆè¶³å¤Ÿtaboo words (â‰¥5ä¸ª): 300 ä¸ªè¯æ±‡\n",
            "  - LLMæˆåŠŸè¡¥å……: 0 ä¸ªè¯æ±‡\n",
            "  - æ€»LLMè°ƒç”¨æ¬¡æ•°: 0\n",
            "  - ä»ç„¶ä¸è¶³: 0 ä¸ªè¯æ±‡\n",
            "\n",
            "--- æœ€ç»ˆæ–‡ä»¶ç±»åˆ«åˆ†å¸ƒè‡ªæ£€ ---\n",
            "category\n",
            "general       100\n",
            "chemistry      50\n",
            "cs             50\n",
            "finance        50\n",
            "philosophy     50\n",
            "Name: count, dtype: int64\n",
            "\n",
            "ğŸ“ˆ æœ€ç»ˆè´¨é‡æŠ¥å‘Š:\n",
            "  - å¹³å‡ taboo word æ•°é‡: 5.00\n",
            "  - æˆåŠŸç‡ (â‰¥5ä¸ªtaboo words): 100.0%\n",
            "\n",
            "ğŸ† æ­å–œï¼å¢å¼ºç‰ˆé¡¹ç›®å·²å‡†å¤‡å°±ç»ªï¼\n"
          ]
        }
      ],
      "source": [
        "# æ•°æ®é›†ç”Ÿæˆè„šæœ¬ V35 - å®Œæ•´é›†æˆç‰ˆæœ¬ (ä¿®å¤è·¯å¾„ + å¢å¼ºç®—æ³• + LLMæ™ºèƒ½è¡¥å……)\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import random\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import requests\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "print(\"=== æ•°æ®é›†ç”Ÿæˆè„šæœ¬ V35 (å®Œæ•´é›†æˆç‰ˆæœ¬) ===\")\n",
        "print(\"ğŸ”§ ä¿®å¤è·¯å¾„é—®é¢˜ + ğŸš€ å¢å¼ºç®—æ³• + ğŸ¤– LLMæ™ºèƒ½è¡¥å……\")\n",
        "\n",
        "# =============================================================\n",
        "# LLM API è°ƒç”¨åŠŸèƒ½\n",
        "# =============================================================\n",
        "\n",
        "def load_api_key():\n",
        "    \"\"\"åŠ è½½APIå¯†é’¥\"\"\"\n",
        "    try:\n",
        "        with open('api_keys.json', 'r') as f:\n",
        "            api_keys = json.load(f)\n",
        "        return api_keys['OPENROUTER_API_KEY']\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ æ— æ³•åŠ è½½APIå¯†é’¥: {e}\")\n",
        "        return None\n",
        "\n",
        "def call_openrouter_api(prompt, api_key, max_retries=3):\n",
        "    \"\"\"è°ƒç”¨OpenRouter API (DeepSeek V3)\"\"\"\n",
        "    if not api_key:\n",
        "        return None\n",
        "        \n",
        "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": \"deepseek/deepseek-chat-v3-0324:free\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "        \"max_tokens\": 150,\n",
        "        \"temperature\": 0.7\n",
        "    }\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.post(url, headers=headers, json=data, timeout=30)\n",
        "            if response.status_code == 200:\n",
        "                result = response.json()\n",
        "                return result['choices'][0]['message']['content'].strip()\n",
        "            else:\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(2 ** attempt)\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(2 ** attempt)\n",
        "    return None\n",
        "\n",
        "def generate_llm_taboo_words(target_word, existing_taboos, part_of_speech, definition, api_key):\n",
        "    \"\"\"ä½¿ç”¨LLMç”Ÿæˆè¡¥å……çš„taboo words\"\"\"\n",
        "    needed_count = 5 - len(existing_taboos)\n",
        "    if needed_count <= 0 or not api_key:\n",
        "        return []\n",
        "    \n",
        "    existing_str = \", \".join(existing_taboos) if existing_taboos else \"æ— \"\n",
        "    prompt = f\"\"\"ä¸ºTabooæ¸¸æˆç”Ÿæˆç¦ç”¨è¯ã€‚\n",
        "\n",
        "ç›®æ ‡è¯: {target_word}\n",
        "è¯æ€§: {part_of_speech}  \n",
        "å®šä¹‰: {definition}\n",
        "å·²æœ‰ç¦ç”¨è¯: {existing_str}\n",
        "\n",
        "è¯·ä¸º\"{target_word}\"ç”Ÿæˆ{needed_count}ä¸ªæ–°çš„ç¦ç”¨è¯ï¼Œè¦æ±‚ï¼š\n",
        "1. ä¸ç›®æ ‡è¯è¯­ä¹‰ç›¸å…³ä½†ä¸èƒ½ç›´æ¥è¯´å‡º\n",
        "2. ä¸èƒ½é‡å¤å·²æœ‰ç¦ç”¨è¯  \n",
        "3. ä¼˜å…ˆé€‰æ‹©åŒä¹‰è¯ã€ç›¸å…³æ¦‚å¿µã€ä¸Šä¸‹ä½è¯\n",
        "4. æ¯ä¸ªè¯ç”¨è‹±æ–‡ï¼Œå•ä¸ªè¯æ±‡ï¼Œå°å†™\n",
        "5. é¿å…è¿‡äºé€šç”¨çš„è¯\n",
        "\n",
        "åªè¾“å‡º{needed_count}ä¸ªè‹±æ–‡å•è¯ï¼Œç”¨é€—å·åˆ†éš”ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚\"\"\"\n",
        "\n",
        "    response = call_openrouter_api(prompt, api_key)\n",
        "    if response:\n",
        "        words = [w.strip().lower() for w in response.split(',')]\n",
        "        valid_words = []\n",
        "        for word in words:\n",
        "            if (word.isalpha() and len(word) > 2 and \n",
        "                word not in existing_taboos and \n",
        "                word != target_word.lower() and\n",
        "                len(valid_words) < needed_count):\n",
        "                valid_words.append(word)\n",
        "        return valid_words\n",
        "    return []\n",
        "\n",
        "# =============================================================\n",
        "# ä¸»è¦æ‰§è¡Œè„šæœ¬ - å®Œæ•´é›†æˆç‰ˆæœ¬\n",
        "# =============================================================\n",
        "\n",
        "try:\n",
        "    # åŠ è½½APIå¯†é’¥\n",
        "    OPENROUTER_API_KEY = load_api_key()\n",
        "    llm_available = OPENROUTER_API_KEY is not None\n",
        "    if llm_available:\n",
        "        print(\"âœ… LLM API å·²å¯ç”¨\")\n",
        "    else:\n",
        "        print(\"âš ï¸ LLM API æœªå¯ç”¨ï¼Œå°†ä»…ä½¿ç”¨ä¼ ç»Ÿç®—æ³•\")\n",
        "\n",
        "    # --- æ­¥éª¤ä¸€ï¼šå¼ºåˆ¶ä¸‹è½½ä¾èµ– ---\n",
        "    packages_to_download = [\n",
        "        'wordnet', 'omw-1.4', 'punkt', \n",
        "        'averaged_perceptron_tagger', 'averaged_perceptron_tagger_eng',\n",
        "        'stopwords'\n",
        "    ]\n",
        "    print(\"\\nâ³ æ­¥éª¤ä¸€ï¼šæ­£åœ¨å¼ºåˆ¶æ£€æŸ¥å¹¶ä¸‹è½½æ‰€æœ‰å¿…éœ€çš„NLTKæ•°æ®åŒ…...\")\n",
        "    for package in packages_to_download:\n",
        "        try:\n",
        "            # å°è¯•å¤šç§å¯èƒ½çš„è·¯å¾„æ¨¡å¼\n",
        "            found = False\n",
        "            for path_pattern in [f'corpora/{package}', f'tokenizers/{package}', f'taggers/{package}']:\n",
        "                try:\n",
        "                    nltk.data.find(path_pattern)\n",
        "                    found = True\n",
        "                    break\n",
        "                except LookupError:\n",
        "                    continue\n",
        "            \n",
        "            if not found:\n",
        "                print(f\"  - ä¸‹è½½ä¸­: '{package}'...\")\n",
        "                nltk.download(package, quiet=False)\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"  - å°è¯•ä¸‹è½½ '{package}': {e}\")\n",
        "            try:\n",
        "                nltk.download(package, quiet=False)\n",
        "            except:\n",
        "                print(f\"  - è·³è¿‡ '{package}' (å¯èƒ½ä¸å½±å“æ ¸å¿ƒåŠŸèƒ½)\")\n",
        "    print(\"âœ… NLTKä¾èµ–é¡¹å·²å‡†å¤‡å°±ç»ªï¼\")\n",
        "\n",
        "    # --- æ­¥éª¤äºŒï¼šåŠ è½½æ•°æ®æºï¼ˆä¿®å¤è·¯å¾„ï¼‰---\n",
        "    print(\"\\nğŸ“¦ æ­¥éª¤äºŒï¼šæ­£åœ¨åŠ è½½æ‰€æœ‰åŸææ–™ï¼ˆä¿®å¤åçš„è·¯å¾„ï¼‰...\")\n",
        "    raw_data_path = \"data/raw data/\"\n",
        "    \n",
        "    # ä¸“æœ‰åè¯é»‘åå•ï¼ˆå¸¸è§çš„ä¸é€‚åˆTabooçš„è¯æ±‡ï¼‰\n",
        "    proper_noun_blacklist = {\n",
        "        'jefferson', 'washington', 'lincoln', 'kennedy', 'roosevelt', 'adams', 'madison',\n",
        "        'aristotle', 'plato', 'socrates', 'kant', 'hegel', 'nietzsche', 'descartes',\n",
        "        'shakespeare', 'dickens', 'darwin', 'newton', 'einstein', 'galileo',\n",
        "        'america', 'europe', 'asia', 'africa', 'australia', 'antarctica',\n",
        "        'london', 'paris', 'tokyo', 'beijing', 'moscow', 'berlin',\n",
        "        'christianity', 'buddhism', 'hinduism', 'islam', 'judaism',\n",
        "        'english', 'chinese', 'japanese', 'german', 'french', 'spanish',\n",
        "        'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday',\n",
        "        'january', 'february', 'march', 'april', 'may', 'june',\n",
        "        'july', 'august', 'september', 'october', 'november', 'december'\n",
        "    }\n",
        "    \n",
        "    def parse_chemistry_json(filename):\n",
        "        full_path = os.path.join(raw_data_path, filename)\n",
        "        with open(full_path, 'r', encoding='utf-8') as f: \n",
        "            data = json.load(f)\n",
        "        words = set()\n",
        "        term_dict = data.get('terms', {}).get('list', {})\n",
        "        for key, value in term_dict.items():\n",
        "            if isinstance(value, dict):\n",
        "                term = value.get('title', '').lower()\n",
        "                if term: words.add(term)\n",
        "        return words\n",
        "\n",
        "    def parse_cs_json(filename):\n",
        "        full_path = os.path.join(raw_data_path, filename)\n",
        "        with open(full_path, 'r', encoding='utf-8') as f: \n",
        "            data = json.load(f)\n",
        "        words = set()\n",
        "        term_list = data.get('results', [])\n",
        "        for entry in term_list:\n",
        "            term = entry.get('value', '').lower().split('(')[0].strip()\n",
        "            if term: words.add(term)\n",
        "        return words\n",
        "\n",
        "    def parse_text_file(filename):\n",
        "        full_path = os.path.join(raw_data_path, filename)\n",
        "        with open(full_path, 'r', encoding='utf-8') as f:\n",
        "            words = set()\n",
        "            for line in f:\n",
        "                match = re.match(r'^[a-zA-Z\\s_-]+', line.strip())\n",
        "                if match: words.add(match.group(0).strip().lower())\n",
        "        return words\n",
        "\n",
        "    file_parsers = {\n",
        "        'chemistry': ('chemistry_terms.json', parse_chemistry_json), \n",
        "        'cs': ('cs_terms.json', parse_cs_json), \n",
        "        'finance': ('finance_terms.txt', parse_text_file), \n",
        "        'philosophy': ('philosophy_terms.txt', parse_text_file)\n",
        "    }\n",
        "    \n",
        "    professional_pools = {}\n",
        "    for category, (filename, parser_func) in file_parsers.items():\n",
        "        full_path = os.path.join(raw_data_path, filename)\n",
        "        if os.path.exists(full_path):\n",
        "            word_set = parser_func(filename)\n",
        "            # æ›´ä¸¥æ ¼çš„è¯æ±‡ç­›é€‰ï¼šæ’é™¤ä¸“æœ‰åè¯å’Œä¸åˆé€‚çš„è¯æ±‡\n",
        "            def is_valid_word(word):\n",
        "                # åŸºæœ¬é•¿åº¦å’Œå­—æ¯æ£€æŸ¥\n",
        "                if not (word.isalpha() and 3 < len(word) < 25):\n",
        "                    return False\n",
        "                \n",
        "                # æ£€æŸ¥ä¸“æœ‰åè¯é»‘åå•\n",
        "                if word.lower() in proper_noun_blacklist:\n",
        "                    return False\n",
        "                \n",
        "                # æ’é™¤é¦–å­—æ¯å¤§å†™çš„è¯ï¼ˆé€šå¸¸æ˜¯ä¸“æœ‰åè¯ï¼‰\n",
        "                if word[0].isupper():\n",
        "                    return False\n",
        "                \n",
        "                # æ’é™¤å…¨å¤§å†™çš„è¯\n",
        "                if word.isupper():\n",
        "                    return False\n",
        "                \n",
        "                # æ’é™¤åŒ…å«å¸¸è§ä¸“æœ‰åè¯æ ‡è¯†çš„è¯\n",
        "                proper_noun_indicators = ['ism', 'ist', 'ian', 'ese', 'ic']\n",
        "                if any(word.endswith(indicator) for indicator in proper_noun_indicators):\n",
        "                    # æ£€æŸ¥æ˜¯å¦çœŸçš„æ˜¯ä¸“æœ‰åè¯ç›¸å…³\n",
        "                    synsets = wn.synsets(word)\n",
        "                    if synsets:\n",
        "                        # æ£€æŸ¥è¯æ€§ï¼Œå¦‚æœä¸»è¦æ˜¯åè¯ä¸”å¯èƒ½æ˜¯ä¸“æœ‰åè¯ï¼Œè·³è¿‡\n",
        "                        pos_tags = [s.pos() for s in synsets]\n",
        "                        if 'n' in pos_tags:\n",
        "                            # è¿›ä¸€æ­¥æ£€æŸ¥å®šä¹‰ä¸­æ˜¯å¦åŒ…å«ä¸“æœ‰åè¯ç‰¹å¾\n",
        "                            definitions = [s.definition().lower() for s in synsets]\n",
        "                            proper_keywords = ['person', 'people', 'country', 'place', 'city', 'name', 'founder', 'philosopher', 'scientist']\n",
        "                            if any(keyword in ' '.join(definitions) for keyword in proper_keywords):\n",
        "                                return False\n",
        "                \n",
        "                # ç¡®ä¿è¯æ±‡åœ¨WordNetä¸­å­˜åœ¨ä¸”æœ‰æœ‰ç”¨çš„åŒä¹‰è¯é›†\n",
        "                synsets = wn.synsets(word)\n",
        "                if not synsets:\n",
        "                    return False\n",
        "                \n",
        "                # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªåˆé€‚çš„è¯æ€§ï¼ˆåè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ï¼‰\n",
        "                valid_pos = {'n', 'v', 'a', 's'}  # noun, verb, adjective, satellite adjective\n",
        "                if not any(s.pos() in valid_pos for s in synsets):\n",
        "                    return False\n",
        "                \n",
        "                return True\n",
        "            \n",
        "            cleaned_words = {w for w in word_set if is_valid_word(w)}\n",
        "            professional_pools[category] = list(cleaned_words)\n",
        "            print(f\"  - âœ… {category.upper()}: æˆåŠŸåŠ è½½å¹¶è§£æäº† {len(cleaned_words)} ä¸ªå€™é€‰è¯ã€‚\")\n",
        "        else:\n",
        "            print(f\"  - âš ï¸ æ–‡ä»¶æœªæ‰¾åˆ°: {full_path}ï¼Œå°†è·³è¿‡ã€‚\")\n",
        "            professional_pools[category] = []\n",
        "    \n",
        "    # åº”ç”¨åŒæ ·çš„ä¸¥æ ¼ç­›é€‰åˆ°é€šç”¨è¯æ±‡æ± \n",
        "    def is_valid_general_word(word):\n",
        "        # åŸºæœ¬æ¡ä»¶\n",
        "        if not (word.isalpha() and '_' not in word and 3 < len(word) < 16):\n",
        "            return False\n",
        "        \n",
        "        # æ£€æŸ¥ä¸“æœ‰åè¯é»‘åå•\n",
        "        if word.lower() in proper_noun_blacklist:\n",
        "            return False\n",
        "        \n",
        "        # æ’é™¤é¦–å­—æ¯å¤§å†™çš„è¯ï¼ˆé€šå¸¸æ˜¯ä¸“æœ‰åè¯ï¼‰\n",
        "        if word[0].isupper():\n",
        "            return False\n",
        "        \n",
        "        # æ’é™¤å…¨å¤§å†™çš„è¯\n",
        "        if word.isupper():\n",
        "            return False\n",
        "        \n",
        "        # ç¡®ä¿åœ¨WordNetä¸­å­˜åœ¨ä¸”æœ‰åˆé€‚çš„è¯æ€§\n",
        "        synsets = wn.synsets(word)\n",
        "        if not synsets:\n",
        "            return False\n",
        "            \n",
        "        valid_pos = {'n', 'v', 'a', 's'}  # noun, verb, adjective, satellite adjective\n",
        "        if not any(s.pos() in valid_pos for s in synsets):\n",
        "            return False\n",
        "            \n",
        "        return True\n",
        "    \n",
        "    general_pool = [w for w in wn.words() if is_valid_general_word(w)]\n",
        "    print(f\"  - âœ… GENERAL: å€™é€‰æ± å·²å‡†å¤‡å¥½ ({len(general_pool)} ä¸ªè¯)ã€‚\")\n",
        "    \n",
        "    # åŠ è½½å…·ä½“æ€§æ•°æ®\n",
        "    concreteness_path = os.path.join(raw_data_path, \"concreteness_data.xlsx\")\n",
        "    if os.path.exists(concreteness_path):\n",
        "        concreteness_df = pd.read_excel(concreteness_path)\n",
        "        concreteness_lookup = pd.Series(concreteness_df['Conc.M'].values, index=concreteness_df['Word'].str.lower()).to_dict()\n",
        "        print(f\"  - âœ… CONCRETENESS: åˆ†æ•°æ•°æ®åŠ è½½æˆåŠŸã€‚\")\n",
        "    else:\n",
        "        concreteness_lookup = {}\n",
        "        print(f\"  - âš ï¸ å…·ä½“æ€§æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œå°†è·³è¿‡ã€‚\")\n",
        "\n",
        "    # --- æ­¥éª¤ä¸‰ï¼šæ™ºèƒ½æŠ½æ ·ï¼ˆé›†æˆå®æ—¶LLMè´¨é‡æ£€æŸ¥ï¼‰---\n",
        "    print(\"\\nğŸ¯ æ­¥éª¤ä¸‰ï¼šæ­£åœ¨è¿›è¡Œæ™ºèƒ½æŠ½æ ·ï¼ˆå®æ—¶LLMè´¨é‡æ£€æŸ¥ï¼‰...\")\n",
        "    quotas = {'general': 100, 'chemistry': 50, 'cs': 50, 'finance': 50, 'philosophy': 50}\n",
        "    selected_words = {}\n",
        "    word_tracker = set()\n",
        "    all_pools = {'general': general_pool, **professional_pools}\n",
        "    \n",
        "    def check_single_word_appropriateness(word, category, api_key):\n",
        "        \"\"\"æ£€æŸ¥å•ä¸ªè¯æ±‡æ˜¯å¦é€‚åˆ\"\"\"\n",
        "        if not api_key:\n",
        "            return True, \"APIä¸å¯ç”¨ï¼Œè·³è¿‡æ£€æŸ¥\"  # å¦‚æœæ²¡æœ‰APIï¼Œè·³è¿‡LLMæ£€æŸ¥\n",
        "        \n",
        "        prompt = f\"\"\"è¯·æ£€æŸ¥è¯æ±‡\"{word}\"æ˜¯å¦é€‚åˆç”¨äºTabooæ¸¸æˆã€‚åªéœ€è¦æ£€æŸ¥ä¸¤ä¸ªæ ‡å‡†ï¼š\n",
        "1. ä¸æ˜¯ä¸“æœ‰åè¯ï¼ˆäººåã€åœ°åã€å“ç‰Œåã€ç‰¹å®šæœºæ„åç­‰ï¼‰\n",
        "2. ä¸æ¶‰åŠä¸¥é‡æ•æ„Ÿå†…å®¹ï¼ˆä»…é™ï¼šæç«¯æš´åŠ›ã€è‰²æƒ…å†…å®¹ã€ç§æ—æ­§è§†ã€æç«¯æ”¿æ²»æ•æ„Ÿï¼‰\n",
        "\n",
        "æ³¨æ„ï¼šåŒ»å­¦æœ¯è¯­ã€ç–¾ç—…åç§°ã€äººä½“è§£å‰–å­¦æœ¯è¯­ã€æ³•å¾‹æœ¯è¯­ã€å•†ä¸šæœ¯è¯­ã€å­¦æœ¯æ¦‚å¿µã€ä¸“ä¸šæŠ€æœ¯æœ¯è¯­ã€ç”Ÿåƒ»è¯æ±‡éƒ½æ˜¯å¯ä»¥æ¥å—çš„ã€‚\n",
        "\n",
        "è¯æ±‡ï¼š{word} (ç±»åˆ«ï¼š{category})\n",
        "\n",
        "è¯·åªè¿”å›\"åˆé€‚\"æˆ–\"ä¸åˆé€‚:åŸå› \"ã€‚\"\"\"\n",
        "        \n",
        "        response = call_openrouter_api(prompt, api_key)\n",
        "        if response:\n",
        "            response = response.strip()\n",
        "            if \"ä¸åˆé€‚\" in response:\n",
        "                return False, response\n",
        "            else:\n",
        "                return True, \"åˆé€‚\"\n",
        "        return True, \"APIæ£€æŸ¥å¤±è´¥ï¼Œé»˜è®¤æ¥å—\"  # APIå¤±è´¥æ—¶é»˜è®¤æ¥å—\n",
        "    \n",
        "    for category, num_to_select in quotas.items():\n",
        "        pool = all_pools.get(category, [])\n",
        "        if not pool: \n",
        "            print(f\"  - âš ï¸ ç±»åˆ« '{category}' çš„å€™é€‰æ± ä¸ºç©º, æ— æ³•æŠ½æ ·ã€‚\")\n",
        "            continue\n",
        "        \n",
        "        print(f\"\\nğŸ” æ­£åœ¨ä¸º '{category}' ç±»åˆ«æ™ºèƒ½é€‰æ‹© {num_to_select} ä¸ªè¯æ±‡:\")\n",
        "        \n",
        "        # åŸºç¡€ç­›é€‰ï¼šç¡®ä¿æ‰€æœ‰è¯æ±‡éƒ½é€‚åˆTabooæ¸¸æˆ\n",
        "        def is_suitable_for_taboo(word):\n",
        "            # å†æ¬¡æ£€æŸ¥åŸºæœ¬æ¡ä»¶\n",
        "            if not word.isalpha() or len(word) <= 3:\n",
        "                return False\n",
        "            \n",
        "            # æ£€æŸ¥æ˜¯å¦åœ¨ä¸“æœ‰åè¯é»‘åå•ä¸­\n",
        "            if word.lower() in proper_noun_blacklist:\n",
        "                return False\n",
        "            \n",
        "            # ç¡®ä¿ä¸æ˜¯é¦–å­—æ¯å¤§å†™ï¼ˆä¸“æœ‰åè¯ï¼‰\n",
        "            if word[0].isupper():\n",
        "                return False\n",
        "                \n",
        "            synsets = wn.synsets(word)\n",
        "            if not synsets:\n",
        "                return False\n",
        "            \n",
        "            # æ£€æŸ¥å®šä¹‰ä¸­æ˜¯å¦åŒ…å«æ˜æ˜¾çš„ä¸“æœ‰åè¯ç‰¹å¾\n",
        "            for synset in synsets[:2]:  # æ£€æŸ¥å‰ä¸¤ä¸ªæœ€å¸¸è§çš„å«ä¹‰\n",
        "                definition = synset.definition().lower()\n",
        "                if any(keyword in definition for keyword in ['person named', 'named after', 'proper name', 'surname', 'given name']):\n",
        "                    return False\n",
        "            \n",
        "            return True\n",
        "        \n",
        "        pool = [w for w in pool if wn.synsets(w) and is_suitable_for_taboo(w)]\n",
        "        random.shuffle(pool)\n",
        "        count = 0\n",
        "        checked_count = 0\n",
        "        rejected_count = 0\n",
        "        \n",
        "        for word in pool:\n",
        "            if count >= num_to_select: \n",
        "                break\n",
        "            if word in word_tracker:\n",
        "                continue\n",
        "                \n",
        "            checked_count += 1\n",
        "            \n",
        "            # å®æ—¶LLMè´¨é‡æ£€æŸ¥\n",
        "            if llm_available:\n",
        "                is_appropriate, reason = check_single_word_appropriateness(word, category, OPENROUTER_API_KEY)\n",
        "                if not is_appropriate:\n",
        "                    rejected_count += 1\n",
        "                    if rejected_count <= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªè¢«æ‹’ç»çš„è¯æ±‡\n",
        "                        print(f\"    âŒ æ‹’ç»: {word} - {reason}\")\n",
        "                    elif rejected_count == 4:\n",
        "                        print(f\"    ... (åç»­æ‹’ç»çš„è¯æ±‡ä¸å†æ˜¾ç¤º)\")\n",
        "                    continue\n",
        "            \n",
        "            # æ¥å—è¿™ä¸ªè¯æ±‡\n",
        "            selected_words[word] = category\n",
        "            word_tracker.add(word)\n",
        "            count += 1\n",
        "            \n",
        "            if count <= 5 or count % 10 == 0:  # æ˜¾ç¤ºå‰5ä¸ªå’Œæ¯10ä¸ª\n",
        "                print(f\"    âœ… æ¥å—: {word}\")\n",
        "        \n",
        "        print(f\"  ğŸ“Š '{category}' å®Œæˆ: æˆåŠŸé€‰æ‹© {count}/{num_to_select}, æ£€æŸ¥äº† {checked_count} ä¸ªå€™é€‰è¯, æ‹’ç»äº† {rejected_count} ä¸ª\")\n",
        "        \n",
        "        # å¦‚æœæ•°é‡ä¸è¶³ï¼Œä»å‰©ä½™æ± ä¸­è¡¥å……ï¼ˆä¸ä½¿ç”¨LLMæ£€æŸ¥ï¼‰\n",
        "        if count < num_to_select:\n",
        "            needed = num_to_select - count\n",
        "            remaining_pool = [w for w in pool if w not in word_tracker and is_suitable_for_taboo(w)]\n",
        "            random.shuffle(remaining_pool)\n",
        "            for word in remaining_pool[:needed]:\n",
        "                selected_words[word] = category\n",
        "                word_tracker.add(word)\n",
        "                count += 1\n",
        "            print(f\"  ğŸ”„ è¡¥å……äº† {min(needed, len(remaining_pool))} ä¸ªè¯æ±‡ï¼ˆè·³è¿‡LLMæ£€æŸ¥ï¼‰\")\n",
        "    \n",
        "    print(\"\\nâœ… æ™ºèƒ½æŠ½æ ·å®Œæˆ! (é›†æˆäº†å®æ—¶LLMè´¨é‡æ£€æŸ¥)\")\n",
        "\n",
        "    # --- æ­¥éª¤å››ï¼šä¸°å¯ŒåŒ–å¤„ç†ï¼ˆå¢å¼ºç‰ˆç®—æ³• + LLMè¡¥å……ï¼‰---\n",
        "    print(f\"\\nğŸ­ æ­¥éª¤å››ï¼šæ­£åœ¨ä¸º {len(selected_words)} ä¸ªè¯è¿›è¡Œæœ€ç»ˆæ•°æ®ä¸°å¯ŒåŒ–...\")\n",
        "    if llm_available:\n",
        "        print(\"ğŸš€ ä½¿ç”¨å¢å¼ºç‰ˆç®—æ³• + LLMæ™ºèƒ½è¡¥å……\")\n",
        "    else:\n",
        "        print(\"ğŸš€ ä½¿ç”¨å¢å¼ºç‰ˆç®—æ³•\")\n",
        "    \n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    \n",
        "    def get_all_senses(word):\n",
        "        synsets = wn.synsets(word.lower())\n",
        "        return [{\n",
        "            \"name\": s.name(), \n",
        "            \"pos\": s.pos(), \n",
        "            \"definition\": s.definition(), \n",
        "            \"examples\": s.examples()\n",
        "        } for s in synsets] if synsets else []\n",
        "    \n",
        "    def generate_enhanced_taboo_words(primary_synset, target_word, all_synsets=None):\n",
        "        \"\"\"å¢å¼ºç‰ˆ taboo word ç”Ÿæˆå™¨ - å¤šç­–ç•¥ç”Ÿæˆ\"\"\"\n",
        "        taboo_set = set()\n",
        "        target_word_lower = target_word.lower()\n",
        "        \n",
        "        # ç­–ç•¥ 1: ä»ä¸»è¦åŒä¹‰è¯é›†è·å–åŒä¹‰è¯\n",
        "        for lemma in primary_synset.lemmas():\n",
        "            synonym = lemma.name().lower().replace('_', ' ')\n",
        "            if synonym != target_word_lower and len(synonym.split()) <= 2:\n",
        "                main_word = synonym.split()[0]\n",
        "                if main_word.isalpha() and len(main_word) > 2:\n",
        "                    taboo_set.add(main_word)\n",
        "                if len(synonym.split()) <= 2 and all(w.isalpha() for w in synonym.split()):\n",
        "                    taboo_set.add(synonym.replace(' ', ''))\n",
        "        \n",
        "        # ç­–ç•¥ 2: ä»æ‰€æœ‰ç›¸å…³åŒä¹‰è¯é›†è·å–æ›´å¤šåŒä¹‰è¯\n",
        "        if all_synsets:\n",
        "            for synset_info in all_synsets[:3]:\n",
        "                try:\n",
        "                    synset = wn.synset(synset_info['name'])\n",
        "                    for lemma in synset.lemmas():\n",
        "                        synonym = lemma.name().lower().replace('_', ' ')\n",
        "                        if synonym != target_word_lower:\n",
        "                            main_word = synonym.split()[0]\n",
        "                            if main_word.isalpha() and len(main_word) > 2:\n",
        "                                taboo_set.add(main_word)\n",
        "                except:\n",
        "                    continue\n",
        "        \n",
        "        # ç­–ç•¥ 3: ä»åä¹‰è¯è·å–\n",
        "        if primary_synset.lemmas():\n",
        "            for antonym_lemma in primary_synset.lemmas()[0].antonyms():\n",
        "                antonym = antonym_lemma.name().lower().replace('_', ' ')\n",
        "                main_word = antonym.split()[0]\n",
        "                if main_word.isalpha() and len(main_word) > 2:\n",
        "                    taboo_set.add(main_word)\n",
        "        \n",
        "        # ç­–ç•¥ 4: ä»å®šä¹‰ä¸­æå–å…³é”®è¯ï¼ˆæ”¹è¿›ç‰ˆï¼‰\n",
        "        definition = primary_synset.definition()\n",
        "        tokens = word_tokenize(definition)\n",
        "        tagged = pos_tag(tokens)\n",
        "        \n",
        "        for word, tag in tagged:\n",
        "            word_lower = word.lower()\n",
        "            # ä¼˜å…ˆé€‰æ‹©åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯\n",
        "            if (tag.startswith(('NN', 'VB', 'JJ')) and \n",
        "                word_lower.isalpha() and \n",
        "                word_lower not in stop_words and \n",
        "                len(word_lower) > 3 and \n",
        "                word_lower != target_word_lower):\n",
        "                taboo_set.add(word_lower)\n",
        "        \n",
        "        # ç­–ç•¥ 5: ä»ä¾‹å¥ä¸­æå–å…³é”®è¯\n",
        "        for example in primary_synset.examples():\n",
        "            try:\n",
        "                tokens = word_tokenize(example)\n",
        "                tagged = pos_tag(tokens)\n",
        "                for word, tag in tagged:\n",
        "                    word_lower = word.lower()\n",
        "                    if (tag.startswith(('NN', 'VB', 'JJ')) and \n",
        "                        word_lower.isalpha() and \n",
        "                        word_lower not in stop_words and \n",
        "                        len(word_lower) > 3 and \n",
        "                        word_lower != target_word_lower and\n",
        "                        len(taboo_set) < 10):\n",
        "                        taboo_set.add(word_lower)\n",
        "            except:\n",
        "                continue\n",
        "        \n",
        "        # ç­–ç•¥ 6: è·å–ä¸Šä½è¯å’Œä¸‹ä½è¯\n",
        "        try:\n",
        "            for hypernym in primary_synset.hypernyms():\n",
        "                for lemma in hypernym.lemmas():\n",
        "                    hyper_word = lemma.name().lower().replace('_', ' ').split()[0]\n",
        "                    if (hyper_word.isalpha() and \n",
        "                        len(hyper_word) > 3 and \n",
        "                        hyper_word != target_word_lower):\n",
        "                        taboo_set.add(hyper_word)\n",
        "            \n",
        "            hyponyms = primary_synset.hyponyms()[:2]\n",
        "            for hyponym in hyponyms:\n",
        "                for lemma in hyponym.lemmas():\n",
        "                    hypo_word = lemma.name().lower().replace('_', ' ').split()[0]\n",
        "                    if (hypo_word.isalpha() and \n",
        "                        len(hypo_word) > 3 and \n",
        "                        hypo_word != target_word_lower):\n",
        "                        taboo_set.add(hypo_word)\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        # è¿‡æ»¤å’Œæ’åº\n",
        "        final_taboos = []\n",
        "        taboo_list = list(taboo_set)\n",
        "        taboo_list.sort(key=lambda x: (abs(len(x) - 6), x))  # ä¼˜å…ˆ6ä¸ªå­—æ¯å·¦å³çš„è¯\n",
        "        \n",
        "        seen = set()\n",
        "        for word in taboo_list:\n",
        "            if word not in seen and len(final_taboos) < 8:\n",
        "                final_taboos.append(word)\n",
        "                seen.add(word)\n",
        "        \n",
        "        # ç­–ç•¥ 7: é€šç”¨è¯æ±‡å…œåº•\n",
        "        pos_specific_generics = {\n",
        "            'n': ['object', 'thing', 'item', 'entity', 'element', 'concept', 'notion'],\n",
        "            'v': ['action', 'process', 'activity', 'behavior', 'perform', 'execute'],\n",
        "            'a': ['quality', 'property', 'characteristic', 'feature', 'attribute'],\n",
        "            's': ['quality', 'property', 'characteristic', 'feature', 'attribute']\n",
        "        }\n",
        "        \n",
        "        pos = primary_synset.pos()\n",
        "        generic_words = pos_specific_generics.get(pos, ['word', 'term', 'expression'])\n",
        "        \n",
        "        for generic in generic_words:\n",
        "            if len(final_taboos) >= 5:\n",
        "                break\n",
        "            if generic not in final_taboos and generic != target_word_lower:\n",
        "                final_taboos.append(generic)\n",
        "        \n",
        "        return final_taboos[:5]\n",
        "        \n",
        "    final_json_list = []\n",
        "    taboo_stats = {\n",
        "        'sufficient_traditional': 0, \n",
        "        'llm_enhanced': 0, \n",
        "        'still_insufficient': 0, \n",
        "        'llm_calls': 0,\n",
        "        'details': []\n",
        "    }\n",
        "    \n",
        "    for i, (word, category) in enumerate(selected_words.items()):\n",
        "        if (i + 1) % 50 == 0: \n",
        "            print(f\"  - ä¸°å¯ŒåŒ–è¿›åº¦ {i+1}/{len(selected_words)}: {word}\")\n",
        "        \n",
        "        senses = get_all_senses(word)\n",
        "        if not senses: continue\n",
        "        \n",
        "        primary_synset = wn.synset(senses[0]['name'])\n",
        "        pos_map = {'n':'noun','v':'verb','a':'adj','r':'adverb','s':'adj'}\n",
        "        main_pos_str = pos_map.get(senses[0]['pos'], 'other')\n",
        "        sense_count = len(senses)\n",
        "        concreteness_score = concreteness_lookup.get(word)\n",
        "        \n",
        "        # ä½¿ç”¨å¢å¼ºç‰ˆç®—æ³•ç”Ÿæˆåˆå§‹taboo words\n",
        "        taboo_words = generate_enhanced_taboo_words(primary_synset, word, senses)\n",
        "        \n",
        "        # å¦‚æœä¸å¤Ÿ5ä¸ªä¸”LLMå¯ç”¨ï¼Œä½¿ç”¨LLMè¡¥å……\n",
        "        if len(taboo_words) < 5 and llm_available:\n",
        "            llm_taboos = generate_llm_taboo_words(\n",
        "                word, \n",
        "                taboo_words, \n",
        "                main_pos_str, \n",
        "                primary_synset.definition(),\n",
        "                OPENROUTER_API_KEY\n",
        "            )\n",
        "            \n",
        "            if llm_taboos:\n",
        "                taboo_words.extend(llm_taboos)\n",
        "                taboo_stats['llm_calls'] += 1\n",
        "                taboo_stats['llm_enhanced'] += 1\n",
        "            \n",
        "            taboo_words = taboo_words[:5]\n",
        "        \n",
        "        # ç»Ÿè®¡ç»“æœ\n",
        "        if len(taboo_words) >= 5:\n",
        "            taboo_stats['sufficient_traditional'] += 1\n",
        "        else:\n",
        "            taboo_stats['still_insufficient'] += 1\n",
        "            taboo_stats['details'].append(f\"{word}: {len(taboo_words)} taboos\")\n",
        "        \n",
        "        json_obj = {\n",
        "            \"target\": word,\n",
        "            \"part_of_speech\": main_pos_str,\n",
        "            \"taboo\": taboo_words,\n",
        "            \"category\": category,\n",
        "            \"senses\": senses,\n",
        "            \"metadata\": {\n",
        "                \"sense_count\": sense_count,\n",
        "                \"concreteness_score\": round(concreteness_score, 2) if concreteness_score else None,\n",
        "                \"taboo_count\": len(taboo_words)\n",
        "            }\n",
        "        }\n",
        "        final_json_list.append(json_obj)\n",
        "        \n",
        "        # APIè°ƒç”¨é™åˆ¶\n",
        "        if taboo_stats['llm_calls'] > 0 and taboo_stats['llm_calls'] % 10 == 0:\n",
        "            time.sleep(1)\n",
        "\n",
        "    # --- æ­¥éª¤äº”ï¼šè¾“å‡ºç»“æœå’Œç»Ÿè®¡ ---\n",
        "    print(f\"\\n--- æ•°æ®ä¸°å¯ŒåŒ–å®Œæˆ ---\")\n",
        "    output_filename = \"ENHANCED_DATASET.json\"\n",
        "    with open(output_filename, 'w', encoding='utf-8') as f: \n",
        "        json.dump(final_json_list, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    print(f\"âœ…âœ…âœ… æ‚¨çš„å¢å¼ºç‰ˆæ•°æ®é›†å·²ç”Ÿæˆï¼æ–‡ä»¶åä¸º: '{output_filename}'\")\n",
        "    print(f\"æ€»è¯æ±‡æ•°: {len(final_json_list)}\")\n",
        "    \n",
        "    # è¯¦ç»†ç»Ÿè®¡æŠ¥å‘Š\n",
        "    print(f\"\\nğŸ“Š Taboo Word ç”Ÿæˆç»Ÿè®¡:\")\n",
        "    print(f\"  - æˆåŠŸç”Ÿæˆè¶³å¤Ÿtaboo words (â‰¥5ä¸ª): {taboo_stats['sufficient_traditional']} ä¸ªè¯æ±‡\")\n",
        "    if llm_available:\n",
        "        print(f\"  - LLMæˆåŠŸè¡¥å……: {taboo_stats['llm_enhanced']} ä¸ªè¯æ±‡\")\n",
        "        print(f\"  - æ€»LLMè°ƒç”¨æ¬¡æ•°: {taboo_stats['llm_calls']}\")\n",
        "    print(f\"  - ä»ç„¶ä¸è¶³: {taboo_stats['still_insufficient']} ä¸ªè¯æ±‡\")\n",
        "    \n",
        "    if taboo_stats['details'][:3]:\n",
        "        print(\"  - ä»ç„¶ä¸è¶³çš„è¯æ±‡:\")\n",
        "        for detail in taboo_stats['details'][:3]:\n",
        "            print(f\"    {detail}\")\n",
        "    \n",
        "    print(\"\\n--- æœ€ç»ˆæ–‡ä»¶ç±»åˆ«åˆ†å¸ƒè‡ªæ£€ ---\")\n",
        "    final_df = pd.DataFrame(final_json_list)\n",
        "    print(final_df['category'].value_counts())\n",
        "    \n",
        "    # éªŒè¯æœ€ç»ˆè´¨é‡\n",
        "    avg_taboo_count = final_df['metadata'].apply(lambda x: x['taboo_count']).mean()\n",
        "    success_rate = (len(final_df[final_df['metadata'].apply(lambda x: x['taboo_count']) >= 5]) / len(final_df)) * 100\n",
        "    \n",
        "    print(f\"\\nğŸ“ˆ æœ€ç»ˆè´¨é‡æŠ¥å‘Š:\")\n",
        "    print(f\"  - å¹³å‡ taboo word æ•°é‡: {avg_taboo_count:.2f}\")\n",
        "    print(f\"  - æˆåŠŸç‡ (â‰¥5ä¸ªtaboo words): {success_rate:.1f}%\")\n",
        "    \n",
        "    print(\"\\nğŸ† æ­å–œï¼å¢å¼ºç‰ˆé¡¹ç›®å·²å‡†å¤‡å°±ç»ªï¼\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒâŒâŒ è„šæœ¬æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°æ„å¤–é”™è¯¯: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_api_key():\n",
        "    \"\"\"åŠ è½½APIå¯†é’¥\"\"\"\n",
        "    try:\n",
        "        with open('api_keys.json', 'r') as f:\n",
        "            api_keys = json.load(f)\n",
        "        return api_keys['OPENROUTER_API_KEY']\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ æ— æ³•åŠ è½½APIå¯†é’¥: {e}\")\n",
        "        return None\n",
        "\n",
        "def call_openrouter_api(prompt, api_key, max_retries=3):\n",
        "    \"\"\"è°ƒç”¨OpenRouter API (DeepSeek V3)\"\"\"\n",
        "    if not api_key:\n",
        "        return None\n",
        "        \n",
        "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": \"deepseek/deepseek-chat-v3-0324:free\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "        \"max_tokens\": 150,\n",
        "        \"temperature\": 0.7\n",
        "    }\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.post(url, headers=headers, json=data, timeout=30)\n",
        "            if response.status_code == 200:\n",
        "                result = response.json()\n",
        "                return result['choices'][0]['message']['content'].strip()\n",
        "            else:\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(2 ** attempt)\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(2 ** attempt)\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== LLMè´¨é‡æ£€æŸ¥å¼€å§‹ ===\n",
            "ğŸ” æ­£åœ¨æ£€æŸ¥ç”Ÿæˆçš„æ•°æ®é›†ä¸­æ˜¯å¦æœ‰ä¸åˆé€‚çš„target words...\n",
            "âœ… æ•°æ®é›†å·²åŠ è½½ï¼Œå…± 300 ä¸ªè¯æ±‡\n",
            "âœ… å·²æå–æ‰€æœ‰target words\n",
            "ğŸ¤– å¼€å§‹ä½¿ç”¨LLMè¿›è¡Œè´¨é‡æ£€æŸ¥...\n",
            "ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ 1/15 (20 ä¸ªè¯æ±‡)...\n",
            "ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ 2/15 (20 ä¸ªè¯æ±‡)...\n",
            "ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ 3/15 (20 ä¸ªè¯æ±‡)...\n",
            "ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ 4/15 (20 ä¸ªè¯æ±‡)...\n",
            "ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ 5/15 (20 ä¸ªè¯æ±‡)...\n",
            "ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ 6/15 (20 ä¸ªè¯æ±‡)...\n",
            "ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ 7/15 (20 ä¸ªè¯æ±‡)...\n",
            "ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ 8/15 (20 ä¸ªè¯æ±‡)...\n",
            "ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ 9/15 (20 ä¸ªè¯æ±‡)...\n",
            "ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ 10/15 (20 ä¸ªè¯æ±‡)...\n",
            "ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ 11/15 (20 ä¸ªè¯æ±‡)...\n",
            "ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ 12/15 (20 ä¸ªè¯æ±‡)...\n",
            "ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ 13/15 (20 ä¸ªè¯æ±‡)...\n",
            "ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ 14/15 (20 ä¸ªè¯æ±‡)...\n",
            "ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ 15/15 (20 ä¸ªè¯æ±‡)...\n",
            "\\nğŸ“‹ æ•°æ®é›†è´¨é‡æ£€æŸ¥æŠ¥å‘Š\n",
            "==================================================\n",
            "æ€»è¯æ±‡æ•°: 300\n",
            "è¢«æ ‡è®°ä¸ºä¸åˆé€‚çš„è¯æ±‡æ•°: 0\n",
            "åˆæ ¼ç‡: 100.0%\n",
            "\\nâœ… æ‰€æœ‰è¯æ±‡éƒ½è¢«è®¤ä¸ºæ˜¯åˆé€‚çš„ï¼\n",
            "\\n=== LLMè´¨é‡æ£€æŸ¥å®Œæˆ ===\n"
          ]
        }
      ],
      "source": [
        "# LLMè´¨é‡æ£€æŸ¥ - æ£€æµ‹ä¸åˆé€‚çš„target words\n",
        "import json\n",
        "import time\n",
        "\n",
        "def load_generated_dataset():\n",
        "    \"\"\"åŠ è½½åˆšç”Ÿæˆçš„æ•°æ®é›†\"\"\"\n",
        "    try:\n",
        "        with open('ENHANCED_DATASET.json', 'r', encoding='utf-8') as f:\n",
        "            dataset = json.load(f)\n",
        "        return dataset\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ æ— æ³•åŠ è½½æ•°æ®é›†æ–‡ä»¶: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_target_words(dataset):\n",
        "    \"\"\"æå–æ‰€æœ‰target words\"\"\"\n",
        "    if not dataset:\n",
        "        return []\n",
        "    \n",
        "    target_words = []\n",
        "    for entry in dataset:\n",
        "        target_words.append({\n",
        "            'word': entry['target'],\n",
        "            'category': entry['category'],\n",
        "            'pos': entry['part_of_speech']\n",
        "        })\n",
        "    \n",
        "    return target_words\n",
        "\n",
        "def check_word_appropriateness_batch(words_info, api_key, batch_size=20):\n",
        "    \"\"\"æ‰¹é‡æ£€æŸ¥è¯æ±‡æ˜¯å¦é€‚åˆTabooæ¸¸æˆ\"\"\"\n",
        "    if not api_key:\n",
        "        print(\"âš ï¸ APIå¯†é’¥ä¸å¯ç”¨ï¼Œè·³è¿‡è´¨é‡æ£€æŸ¥\")\n",
        "        return []\n",
        "    \n",
        "    inappropriate_words = []\n",
        "    total_batches = (len(words_info) + batch_size - 1) // batch_size\n",
        "    \n",
        "    for batch_idx in range(0, len(words_info), batch_size):\n",
        "        batch = words_info[batch_idx:batch_idx + batch_size]\n",
        "        batch_num = batch_idx // batch_size + 1\n",
        "        \n",
        "        print(f\"ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch)} ä¸ªè¯æ±‡)...\")\n",
        "        \n",
        "        # æ„å»ºæ‰¹é‡æ£€æŸ¥çš„prompt\n",
        "        word_list = []\n",
        "        for i, word_info in enumerate(batch):\n",
        "            word_list.append(f\"{i+1}. {word_info['word']} ({word_info['category']}, {word_info['pos']})\")\n",
        "        \n",
        "        words_text = \"\\n\".join(word_list)\n",
        "        \n",
        "        prompt = f\"\"\"è¯·æ£€æŸ¥ä»¥ä¸‹è¯æ±‡æ˜¯å¦é€‚åˆç”¨äºTabooæ¸¸æˆã€‚åªéœ€è¦æ£€æŸ¥è¿™ä¸¤ä¸ªæ ‡å‡†ï¼š\n",
        "1. ä¸æ˜¯ä¸“æœ‰åè¯ï¼ˆäººåã€åœ°åã€å“ç‰Œåã€ç‰¹å®šæœºæ„åç­‰ï¼‰\n",
        "2. ä¸æ¶‰åŠä¸¥é‡æ•æ„Ÿå†…å®¹ï¼ˆä»…é™ï¼šæç«¯æš´åŠ›ã€è‰²æƒ…å†…å®¹ã€ç§æ—æ­§è§†ã€æç«¯æ”¿æ²»æ•æ„Ÿï¼‰\n",
        "\n",
        "æ³¨æ„ï¼šä»¥ä¸‹å†…å®¹éƒ½æ˜¯å¯ä»¥æ¥å—çš„ï¼Œä¸ç®—æ•æ„Ÿå†…å®¹ï¼š\n",
        "- åŒ»å­¦æœ¯è¯­ã€ç–¾ç—…åç§°ã€äººä½“è§£å‰–å­¦æœ¯è¯­\n",
        "- æ³•å¾‹æœ¯è¯­ã€å•†ä¸šæœ¯è¯­  \n",
        "- ä¸€èˆ¬çš„å­¦æœ¯æ¦‚å¿µ\n",
        "- ä¸“ä¸šæŠ€æœ¯æœ¯è¯­\n",
        "- ç”Ÿåƒ»è¯æ±‡\n",
        "\n",
        "å¾…æ£€æŸ¥è¯æ±‡åˆ—è¡¨ï¼š\n",
        "{words_text}\n",
        "\n",
        "å¯¹äºä¸åˆé€‚çš„è¯æ±‡ï¼Œè¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¿”å›ï¼Œæ¯è¡Œä¸€ä¸ªï¼š\n",
        "ç¼–å·:åŸå› \n",
        "ä¾‹å¦‚ï¼š\n",
        "1:ä¸“æœ‰åè¯-äººå\n",
        "3:ä¸“æœ‰åè¯-åœ°å\n",
        "7:æ•æ„Ÿå†…å®¹-æ”¿æ²»\n",
        "\n",
        "å¦‚æœæ‰€æœ‰è¯æ±‡éƒ½åˆé€‚ï¼Œè¯·åªè¿”å›\"æ— \"ã€‚\"\"\"\n",
        "\n",
        "        response = call_openrouter_api(prompt, api_key)\n",
        "        if response:\n",
        "            response = response.strip()\n",
        "            if response.lower() != \"æ— \" and response.lower() != \"none\":\n",
        "                try:\n",
        "                    # è§£æè¿”å›çš„ç¼–å·å’ŒåŸå› \n",
        "                    lines = response.split('\\n')\n",
        "                    for line in lines:\n",
        "                        line = line.strip()\n",
        "                        if ':' in line:\n",
        "                            parts = line.split(':', 1)\n",
        "                            if len(parts) == 2 and parts[0].strip().isdigit():\n",
        "                                idx = int(parts[0].strip()) - 1\n",
        "                                reason = parts[1].strip()\n",
        "                                if 0 <= idx < len(batch):\n",
        "                                    inappropriate_words.append({\n",
        "                                        'word': batch[idx]['word'],\n",
        "                                        'category': batch[idx]['category'],\n",
        "                                        'pos': batch[idx]['pos'],\n",
        "                                        'reason': reason\n",
        "                                    })\n",
        "                except Exception as e:\n",
        "                    print(f\"  - âš ï¸ è§£æLLMå›å¤æ—¶å‡ºé”™: {e}\")\n",
        "                    print(f\"  - åŸå§‹å›å¤: {response[:100]}...\")\n",
        "        \n",
        "        # é¿å…APIè°ƒç”¨è¿‡äºé¢‘ç¹\n",
        "        if batch_num < total_batches:\n",
        "            time.sleep(2)\n",
        "    \n",
        "    return inappropriate_words\n",
        "\n",
        "def generate_quality_report(dataset, inappropriate_words):\n",
        "    \"\"\"ç”Ÿæˆè´¨é‡æŠ¥å‘Š\"\"\"\n",
        "    if not dataset:\n",
        "        return\n",
        "    \n",
        "    total_words = len(dataset)\n",
        "    inappropriate_count = len(inappropriate_words)\n",
        "    \n",
        "    print(f\"\\\\nğŸ“‹ æ•°æ®é›†è´¨é‡æ£€æŸ¥æŠ¥å‘Š\")\n",
        "    print(f\"=\" * 50)\n",
        "    print(f\"æ€»è¯æ±‡æ•°: {total_words}\")\n",
        "    print(f\"è¢«æ ‡è®°ä¸ºä¸åˆé€‚çš„è¯æ±‡æ•°: {inappropriate_count}\")\n",
        "    print(f\"åˆæ ¼ç‡: {((total_words - inappropriate_count) / total_words * 100):.1f}%\")\n",
        "    \n",
        "    if inappropriate_words:\n",
        "        print(f\"\\\\nâš ï¸ è¢«æ ‡è®°ä¸ºä¸åˆé€‚çš„è¯æ±‡:\")\n",
        "        by_category = {}\n",
        "        for word_info in inappropriate_words:\n",
        "            category = word_info['category']\n",
        "            if category not in by_category:\n",
        "                by_category[category] = []\n",
        "            by_category[category].append(word_info)\n",
        "        \n",
        "        for category, words in by_category.items():\n",
        "            print(f\"\\\\n  ã€{category.upper()}ã€‘:\")\n",
        "            for word_info in words:\n",
        "                print(f\"    - {word_info['word']} ({word_info['pos']}) - {word_info['reason']}\")\n",
        "    else:\n",
        "        print(f\"\\\\nâœ… æ‰€æœ‰è¯æ±‡éƒ½è¢«è®¤ä¸ºæ˜¯åˆé€‚çš„ï¼\")\n",
        "\n",
        "# æ‰§è¡Œè´¨é‡æ£€æŸ¥\n",
        "print(\"=== LLMè´¨é‡æ£€æŸ¥å¼€å§‹ ===\")\n",
        "print(\"ğŸ” æ­£åœ¨æ£€æŸ¥ç”Ÿæˆçš„æ•°æ®é›†ä¸­æ˜¯å¦æœ‰ä¸åˆé€‚çš„target words...\")\n",
        "\n",
        "# åŠ è½½æ•°æ®é›†å’ŒAPIå¯†é’¥\n",
        "dataset = load_generated_dataset()\n",
        "api_key = load_api_key()\n",
        "\n",
        "if dataset and api_key:\n",
        "    print(f\"âœ… æ•°æ®é›†å·²åŠ è½½ï¼Œå…± {len(dataset)} ä¸ªè¯æ±‡\")\n",
        "    \n",
        "    # æå–target words\n",
        "    target_words_info = extract_target_words(dataset)\n",
        "    print(f\"âœ… å·²æå–æ‰€æœ‰target words\")\n",
        "    \n",
        "    # æ‰¹é‡æ£€æŸ¥\n",
        "    print(f\"ğŸ¤– å¼€å§‹ä½¿ç”¨LLMè¿›è¡Œè´¨é‡æ£€æŸ¥...\")\n",
        "    inappropriate_words = check_word_appropriateness_batch(target_words_info, api_key)\n",
        "    \n",
        "    # ç”ŸæˆæŠ¥å‘Š\n",
        "    generate_quality_report(dataset, inappropriate_words)\n",
        "    \n",
        "    # å¦‚æœæœ‰ä¸åˆé€‚çš„è¯æ±‡ï¼Œä¿å­˜åˆ°æ–‡ä»¶\n",
        "    if inappropriate_words:\n",
        "        with open('inappropriate_words_report.json', 'w', encoding='utf-8') as f:\n",
        "            json.dump(inappropriate_words, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"\\\\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: inappropriate_words_report.json\")\n",
        "    \n",
        "elif not dataset:\n",
        "    print(\"âŒ æ— æ³•åŠ è½½æ•°æ®é›†ï¼Œè¯·ç¡®ä¿å…ˆè¿è¡Œäº†æ•°æ®é›†ç”Ÿæˆä»£ç \")\n",
        "elif not api_key:\n",
        "    print(\"âŒ æ— æ³•åŠ è½½APIå¯†é’¥ï¼Œè´¨é‡æ£€æŸ¥åŠŸèƒ½ä¸å¯ç”¨\")\n",
        "\n",
        "print(\"\\\\n=== LLMè´¨é‡æ£€æŸ¥å®Œæˆ ===\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ: 300 æ¡è®°å½•\n",
            "ğŸ“ æ•°æ®é›†è·¯å¾„: data/dataset.json\n",
            "\n",
            "ğŸ“‹ æ•°æ®æ ·æœ¬:\n",
            "   ç›®æ ‡è¯: regent\n",
            "   ç¦ç”¨è¯: ['board', 'members', 'trustee', 'committee', 'governing']\n",
            "   ç±»åˆ«: general\n",
            "   å®šä¹‰: members of a governing board...\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "import time\n",
        "import requests\n",
        "import pandas as pd\n",
        "from typing import Dict, List, Any\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# åŠ è½½æ•°æ®é›†\n",
        "def load_dataset(dataset_path: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"åŠ è½½Tabooæ¸¸æˆæ•°æ®é›†\"\"\"\n",
        "    with open(dataset_path, 'r', encoding='utf-8') as f:\n",
        "        dataset = json.load(f)\n",
        "    return dataset\n",
        "\n",
        "# åŠ è½½é¢„ç”Ÿæˆçš„æ•°æ®é›†\n",
        "DATASET_PATH = \"data/dataset.json\"\n",
        "dataset = load_dataset(DATASET_PATH)\n",
        "print(f\"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ: {len(dataset)} æ¡è®°å½•\")\n",
        "print(f\"ğŸ“ æ•°æ®é›†è·¯å¾„: {DATASET_PATH}\")\n",
        "\n",
        "# æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬\n",
        "if dataset:\n",
        "    sample = dataset[0]\n",
        "    print(f\"\\nğŸ“‹ æ•°æ®æ ·æœ¬:\")\n",
        "    print(f\"   ç›®æ ‡è¯: {sample['target']}\")\n",
        "    print(f\"   ç¦ç”¨è¯: {sample['taboo']}\")\n",
        "    print(f\"   ç±»åˆ«: {sample.get('category', 'N/A')}\")\n",
        "    if sample.get('senses'):\n",
        "        print(f\"   å®šä¹‰: {sample['senses'][0].get('definition', 'N/A')[:100]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 377)",
          "output_type": "error",
          "traceback": [
            "  \u001b[36mFile \u001b[39m\u001b[32m<tokenize>:377\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m}\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unindent does not match any outer indentation level\n"
          ]
        }
      ],
      "source": [
        "# è‹±å›½æ‹¼å†™æ£€æŸ¥åŠŸèƒ½ - ä½¿ç”¨LLMç›´æ¥æ£€æŸ¥\n",
        "import json\n",
        "import time\n",
        "\n",
        "def check_spelling_with_llm(words_info, api_key, batch_size=20):\n",
        "    \"\"\"ä½¿ç”¨LLMæ‰¹é‡æ£€æŸ¥æ‹¼å†™å¹¶è¿›è¡Œè½¬æ¢\"\"\"\n",
        "    if not api_key:\n",
        "        print(\"âŒ APIå¯†é’¥ä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œæ‹¼å†™æ£€æŸ¥\")\n",
        "        return []\n",
        "        \n",
        "    spelling_changes = []\n",
        "    total_batches = (len(words_info) + batch_size - 1) // batch_size\n",
        "    \n",
        "    for batch_idx in range(0, len(words_info), batch_size):\n",
        "        batch = words_info[batch_idx:batch_idx + batch_size]\n",
        "        batch_num = batch_idx // batch_size + 1\n",
        "        \n",
        "        print(f\"ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ {batch_num}/{total_batches} çš„æ‹¼å†™ ({len(batch)} ä¸ªè¯æ±‡)...\")\n",
        "        \n",
        "        # æ„å»ºæ‰¹é‡æ£€æŸ¥çš„prompt\n",
        "        word_list = []\n",
        "        for i, word_info in enumerate(batch):\n",
        "            word_list.append(f\"{i+1}. {word_info['word']}\")\n",
        "        words_text = \"\\n\".join(word_list)\n",
        "        \n",
        "        prompt = f\"\"\"è¯·æ£€æŸ¥ä»¥ä¸‹è‹±æ–‡è¯æ±‡çš„æ‹¼å†™æ˜¯å¦ä¸ºè‹±å›½è‹±è¯­æ‹¼å†™ã€‚å¦‚æœæ˜¯ç¾å›½è‹±è¯­æ‹¼å†™ï¼Œè¯·è½¬æ¢ä¸ºè‹±å›½è‹±è¯­æ‹¼å†™ã€‚\n",
        "\n",
        "å¸¸è§è½¬æ¢è§„åˆ™ï¼š\n",
        "- -ize â†’ -ise (organize â†’ organise)\n",
        "- -or â†’ -our (color â†’ colour)  \n",
        "- -er â†’ -re (center â†’ centre)\n",
        "- -ense â†’ -ence (defense â†’ defence)\n",
        "- å…¶ä»–å¸¸è§è½¬æ¢ (gray â†’ grey, aluminum â†’ aluminium)\n",
        "\n",
        "å¾…æ£€æŸ¥è¯æ±‡ï¼š\n",
        "{words_text}\n",
        "\n",
        "è¯·åªè¿”å›éœ€è¦è½¬æ¢çš„è¯æ±‡ï¼Œæ ¼å¼ä¸ºï¼š\n",
        "ç¼–å·:åŸè¯â†’è‹±å›½æ‹¼å†™\n",
        "ä¾‹å¦‚ï¼š\n",
        "1:organizeâ†’organise\n",
        "3:colorâ†’colour\n",
        "\n",
        "å¦‚æœæ‰€æœ‰è¯æ±‡éƒ½æ˜¯è‹±å›½æ‹¼å†™ï¼Œè¯·è¿”å›\"æ— éœ€è½¬æ¢\"ã€‚\"\"\"\n",
        "\n",
        "        response = call_openrouter_api(prompt, api_key)\n",
        "        if response:\n",
        "            response = response.strip()\n",
        "            if response.lower() not in [\"æ— éœ€è½¬æ¢\", \"æ— \", \"none\", \"no changes needed\"]:\n",
        "                try:\n",
        "                    # è§£æLLMè¿”å›çš„æ‹¼å†™è½¬æ¢\n",
        "                    lines = response.split('\\n')\n",
        "                    for line in lines:\n",
        "                        line = line.strip()\n",
        "                        if ':' in line and 'â†’' in line:\n",
        "                            parts = line.split(':', 1)\n",
        "                            if len(parts) == 2 and parts[0].strip().isdigit():\n",
        "                                idx = int(parts[0].strip()) - 1\n",
        "                                conversion = parts[1].strip()\n",
        "                                if 'â†’' in conversion:\n",
        "                                    original, corrected = conversion.split('â†’', 1)\n",
        "                                    original = original.strip()\n",
        "                                    corrected = corrected.strip()\n",
        "                                    if 0 <= idx < len(batch):\n",
        "                                        spelling_changes.append({\n",
        "                                            'original': original,\n",
        "                                            'corrected': corrected,\n",
        "                                            'category': batch[idx]['category'],\n",
        "                                            'pos': batch[idx]['pos'],\n",
        "                                            'type': 'llm_conversion'\n",
        "                                        })\n",
        "                except Exception as e:\n",
        "                    print(f\"  - âš ï¸ è§£æLLMæ‹¼å†™å›å¤æ—¶å‡ºé”™: {e}\")\n",
        "                    print(f\"  - åŸå§‹å›å¤: {response[:100]}...\")\n",
        "        \n",
        "        # æ˜¾ç¤ºæœ¬æ‰¹æ¬¡çš„è½¬æ¢ç»“æœ\n",
        "        batch_changes = [c for c in spelling_changes if any(w['word'] == c['original'] for w in batch)]\n",
        "        if batch_changes:\n",
        "            print(f\"  ğŸ”„ æœ¬æ‰¹æ¬¡å‘ç° {len(batch_changes)} ä¸ªæ‹¼å†™è½¬æ¢\")\n",
        "            for change in batch_changes[-3:]:  # æ˜¾ç¤ºæœ€å3ä¸ª\n",
        "                print(f\"    {change['original']} â†’ {change['corrected']}\")\n",
        "        \n",
        "        # é¿å…APIè°ƒç”¨è¿‡äºé¢‘ç¹\n",
        "        if batch_num < total_batches:\n",
        "            time.sleep(1.5)\n",
        "    \n",
        "    return spelling_changes\n",
        "\n",
        "def generate_spelling_report(spelling_changes):\n",
        "    \"\"\"ç”Ÿæˆæ‹¼å†™æ£€æŸ¥æŠ¥å‘Š\"\"\"\n",
        "    if not spelling_changes:\n",
        "        print(f\"\\nâœ… æ‰€æœ‰è¯æ±‡éƒ½å·²ç»æ˜¯è‹±å›½æ‹¼å†™ï¼\")\n",
        "        return\n",
        "    \n",
        "    print(f\"\\nğŸ“‹ æ‹¼å†™æ£€æŸ¥æŠ¥å‘Š\")\n",
        "    print(f\"=\" * 50)\n",
        "    print(f\"å‘ç°éœ€è¦è½¬æ¢çš„è¯æ±‡æ•°: {len(spelling_changes)}\")\n",
        "    \n",
        "    # æŒ‰ç±»åˆ«åˆ†ç»„\n",
        "    by_category = {}\n",
        "    \n",
        "    for change in spelling_changes:\n",
        "        category = change['category']\n",
        "        if category not in by_category:\n",
        "            by_category[category] = []\n",
        "        by_category[category].append(change)\n",
        "    \n",
        "    print(f\"\\nğŸ”„ æ‹¼å†™è½¬æ¢è¯¦æƒ…:\")\n",
        "    for category, changes in by_category.items():\n",
        "        print(f\"\\n  ã€{category.upper()}ã€‘:\")\n",
        "        for change in changes:\n",
        "            print(f\"    {change['original']} â†’ {change['corrected']} ({change['pos']})\")\n",
        "\n",
        "def update_dataset_spelling(dataset, spelling_changes):\n",
        "    \"\"\"æ›´æ–°æ•°æ®é›†ä¸­çš„æ‹¼å†™\"\"\"\n",
        "    if not spelling_changes:\n",
        "        return dataset, 0\n",
        "    \n",
        "    # åˆ›å»ºè½¬æ¢æ˜ å°„\n",
        "    conversion_map = {change['original']: change['corrected'] for change in spelling_changes}\n",
        "    \n",
        "    updated_count = 0\n",
        "    updated_dataset = []\n",
        "    \n",
        "    for entry in dataset:\n",
        "        updated_entry = entry.copy()\n",
        "        \n",
        "        # æ£€æŸ¥target word\n",
        "        if entry['target'] in conversion_map:\n",
        "            updated_entry['target'] = conversion_map[entry['target']]\n",
        "            updated_count += 1\n",
        "        \n",
        "        # æ£€æŸ¥taboo words\n",
        "        updated_taboo = []\n",
        "        for taboo_word in entry['taboo']:\n",
        "            if taboo_word in conversion_map:\n",
        "                updated_taboo.append(conversion_map[taboo_word])\n",
        "            else:\n",
        "                updated_taboo.append(taboo_word)\n",
        "        updated_entry['taboo'] = updated_taboo\n",
        "        \n",
        "        updated_dataset.append(updated_entry)\n",
        "    \n",
        "    return updated_dataset, updated_count\n",
        "\n",
        "# æ‰§è¡Œè‹±å›½æ‹¼å†™æ£€æŸ¥\n",
        "print(\"=== è‹±å›½æ‹¼å†™æ£€æŸ¥å¼€å§‹ ===\")\n",
        "print(\"ğŸ” æ­£åœ¨ä½¿ç”¨LLMæ£€æŸ¥æ•°æ®é›†ä¸­çš„æ‹¼å†™å¹¶è½¬æ¢ä¸ºè‹±å›½è‹±è¯­...\")\n",
        "\n",
        "# åŠ è½½æ•°æ®é›†å’ŒAPIå¯†é’¥\n",
        "dataset = load_generated_dataset()\n",
        "api_key = load_api_key()\n",
        "\n",
        "if dataset and api_key:\n",
        "    print(f\"âœ… æ•°æ®é›†å·²åŠ è½½ï¼Œå…± {len(dataset)} ä¸ªè¯æ±‡\")\n",
        "    \n",
        "    # æå–target words\n",
        "    target_words_info = extract_target_words(dataset)\n",
        "    print(f\"âœ… å·²æå–æ‰€æœ‰target words\")\n",
        "    \n",
        "    # ä½¿ç”¨LLMæ‰¹é‡æ£€æŸ¥æ‹¼å†™\n",
        "    print(f\"ğŸ¤– å¼€å§‹ä½¿ç”¨LLMè¿›è¡Œæ‹¼å†™æ£€æŸ¥...\")\n",
        "    spelling_changes = check_spelling_with_llm(target_words_info, api_key)\n",
        "    \n",
        "    # ç”Ÿæˆæ‹¼å†™æŠ¥å‘Š\n",
        "    generate_spelling_report(spelling_changes)\n",
        "    \n",
        "    # æ›´æ–°æ•°æ®é›†\n",
        "    if spelling_changes:\n",
        "        updated_dataset, updated_count = update_dataset_spelling(dataset, spelling_changes)\n",
        "        \n",
        "        # ä¿å­˜æ›´æ–°åçš„æ•°æ®é›†\n",
        "        output_filename = \"BRITISH_SPELLING_DATASET.json\"\n",
        "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(updated_dataset, f, indent=2, ensure_ascii=False)\n",
        "        \n",
        "        print(f\"\\nâœ… å·²æ›´æ–° {updated_count} ä¸ªtarget wordsçš„æ‹¼å†™\")\n",
        "        print(f\"ğŸ“„ æ›´æ–°åçš„æ•°æ®é›†å·²ä¿å­˜åˆ°: {output_filename}\")\n",
        "        \n",
        "        # ä¿å­˜æ‹¼å†™è½¬æ¢æŠ¥å‘Š\n",
        "        with open('spelling_conversion_report.json', 'w', encoding='utf-8') as f:\n",
        "            json.dump(spelling_changes, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"ğŸ“„ æ‹¼å†™è½¬æ¢æŠ¥å‘Šå·²ä¿å­˜åˆ°: spelling_conversion_report.json\")\n",
        "    else:\n",
        "        print(f\"\\nâœ… æ•°æ®é›†æ‹¼å†™æ£€æŸ¥å®Œæˆï¼Œæ— éœ€ä¿®æ”¹\")\n",
        "        \n",
        "elif not dataset:\n",
        "    print(\"âŒ æ— æ³•åŠ è½½æ•°æ®é›†\")\n",
        "elif not api_key:\n",
        "    print(\"âŒ æ— æ³•åŠ è½½APIå¯†é’¥ï¼Œæ‹¼å†™æ£€æŸ¥åŠŸèƒ½ä¸å¯ç”¨\")\n",
        "\n",
        "print(\"\\n=== è‹±å›½æ‹¼å†™æ£€æŸ¥å®Œæˆ ===\")\n",
        "        'organization': 'organisation',\n",
        "        'realize': 'realise',\n",
        "        'recognize': 'recognise',\n",
        "        'organize': 'organise',\n",
        "        'maximize': 'maximise',\n",
        "        'minimize': 'minimise',\n",
        "        'optimize': 'optimise',\n",
        "        'analyze': 'analyse',\n",
        "        'paralyze': 'paralyse',\n",
        "        'catalyze': 'catalyse',\n",
        "        'civilize': 'civilise',\n",
        "        'modernize': 'modernise',\n",
        "        'rationalize': 'rationalise',\n",
        "        'standardize': 'standardise',\n",
        "        'categorize': 'categorise',\n",
        "        'characterize': 'characterise',\n",
        "        'democratize': 'democratise',\n",
        "        'energize': 'energise',\n",
        "        'fertilize': 'fertilise',\n",
        "        'globalize': 'globalise',\n",
        "        'hospitalize': 'hospitalise',\n",
        "        'idealize': 'idealise',\n",
        "        'legalize': 'legalise',\n",
        "        'localize': 'localise',\n",
        "        'materialize': 'materialise',\n",
        "        'nationalize': 'nationalise',\n",
        "        'personalize': 'personalise',\n",
        "        'privatize': 'privatise',\n",
        "        'publicize': 'publicise',\n",
        "        'sensitize': 'sensitise',\n",
        "        'socialize': 'socialise',\n",
        "        'specialize': 'specialise',\n",
        "        'synchronize': 'synchronise',\n",
        "        'theorize': 'theorise',\n",
        "        'utilize': 'utilise',\n",
        "        'visualize': 'visualise',\n",
        "        'vocalize': 'vocalise',\n",
        "        \n",
        "        # -or to -our\n",
        "        'color': 'colour',\n",
        "        'flavor': 'flavour',\n",
        "        'honor': 'honour',\n",
        "        'humor': 'humour',\n",
        "        'labor': 'labour',\n",
        "        'neighbor': 'neighbour',\n",
        "        'odor': 'odour',\n",
        "        'rumor': 'rumour',\n",
        "        'tumor': 'tumour',\n",
        "        'valor': 'valour',\n",
        "        'vapor': 'vapour',\n",
        "        'vigor': 'vigour',\n",
        "        'behavior': 'behaviour',\n",
        "        'endeavor': 'endeavour',\n",
        "        'parlor': 'parlour',\n",
        "        'splendor': 'splendour',\n",
        "        'candor': 'candour',\n",
        "        'clamor': 'clamour',\n",
        "        'glamor': 'glamour',\n",
        "        'harbor': 'harbour',\n",
        "        'ardor': 'ardour',\n",
        "        'fervor': 'fervour',\n",
        "        'rancor': 'rancour',\n",
        "        'rigor': 'rigour',\n",
        "        'savior': 'saviour',\n",
        "        'succor': 'succour',\n",
        "        'tremor': 'tremour',\n",
        "        \n",
        "        # -er to -re\n",
        "        'center': 'centre',\n",
        "        'theater': 'theatre',\n",
        "        'fiber': 'fibre',\n",
        "        'liter': 'litre',\n",
        "        'meter': 'metre',\n",
        "        'saber': 'sabre',\n",
        "        'specter': 'spectre',\n",
        "        'caliber': 'calibre',\n",
        "        'goiter': 'goitre',\n",
        "        'luster': 'lustre',\n",
        "        'miter': 'mitre',\n",
        "        'niter': 'nitre',\n",
        "        'ocher': 'ochre',\n",
        "        'reconnoiter': 'reconnoitre',\n",
        "        'saltpeter': 'saltpetre',\n",
        "        'scepter': 'sceptre',\n",
        "        'sepulcher': 'sepulchre',\n",
        "        'somber': 'sombre',\n",
        "        'titer': 'titre',\n",
        "        \n",
        "        # -ense to -ence\n",
        "        'defense': 'defence',\n",
        "        'offense': 'offence',\n",
        "        'license': 'licence',  # noun form\n",
        "        'pretense': 'pretence',\n",
        "        \n",
        "        # -ation to -ation (some cases)\n",
        "        'atomization': 'atomisation',\n",
        "        'carbonization': 'carbonisation',\n",
        "        'centralization': 'centralisation',\n",
        "        'computerization': 'computerisation',\n",
        "        'crystallization': 'crystallisation',\n",
        "        'fertilization': 'fertilisation',\n",
        "        'globalization': 'globalisation',\n",
        "        'hospitalization': 'hospitalisation',\n",
        "        'immunization': 'immunisation',\n",
        "        'legalization': 'legalisation',\n",
        "        'localization': 'localisation',\n",
        "        'materialization': 'materialisation',\n",
        "        'modernization': 'modernisation',\n",
        "        'nationalization': 'nationalisation',\n",
        "        'normalization': 'normalisation',\n",
        "        'optimization': 'optimisation',\n",
        "        'personalization': 'personalisation',\n",
        "        'privatization': 'privatisation',\n",
        "        'rationalization': 'rationalisation',\n",
        "        'realization': 'realisation',\n",
        "        'reorganization': 'reorganisation',\n",
        "        'socialization': 'socialisation',\n",
        "        'specialization': 'specialisation',\n",
        "        'stabilization': 'stabilisation',\n",
        "        'standardization': 'standardisation',\n",
        "        'sterilization': 'sterilisation',\n",
        "        'synchronization': 'synchronisation',\n",
        "        'urbanization': 'urbanisation',\n",
        "        'visualization': 'visualisation',\n",
        "        'vocalization': 'vocalisation',\n",
        "        \n",
        "        # Other common differences\n",
        "        'gray': 'grey',\n",
        "        'sulfur': 'sulphur',\n",
        "        'aluminum': 'aluminium',\n",
        "        'check': 'cheque',  # in financial context\n",
        "        'donut': 'doughnut',\n",
        "        'tire': 'tyre',\n",
        "        'curb': 'kerb',\n",
        "        'jewelry': 'jewellery',\n",
        "        'traveling': 'travelling',\n",
        "        'modeling': 'modelling',\n",
        "        'counselor': 'counsellor',\n",
        "        'labeling': 'labelling',\n",
        "        'leveling': 'levelling',\n",
        "        'marveling': 'marvelling',\n",
        "        'quarreling': 'quarrelling',\n",
        "        'signaling': 'signalling',\n",
        "        'tunneling': 'tunnelling',\n",
        "        'canceled': 'cancelled',\n",
        "        'enrollment': 'enrolment',\n",
        "        'fulfillment': 'fulfilment',\n",
        "        'installment': 'instalment',\n",
        "        'skillful': 'skilful',\n",
        "        'willful': 'wilful',\n",
        "        'maneuver': 'manoeuvre',\n",
        "        'estrogen': 'oestrogen',\n",
        "        'fetus': 'foetus',\n",
        "        'leukemia': 'leukaemia',\n",
        "        'anemia': 'anaemia',\n",
        "        'encyclopedia': 'encyclopaedia',\n",
        "        'pediatric': 'paediatric',\n",
        "        'anesthesia': 'anaesthesia',\n",
        "        'diarrhea': 'diarrhoea',\n",
        "        'archeology': 'archaeology',\n",
        "        'medieval': 'mediaeval',\n",
        "        'mold': 'mould',\n",
        "        'smolder': 'smoulder',\n",
        "        'plow': 'plough',\n",
        "        'airplane': 'aeroplane',\n",
        "        'draft': 'draught',\n",
        "        'skeptic': 'sceptic',\n",
        "        'ax': 'axe',\n",
        "        'cozy': 'cosy',\n",
        "        'dozy': 'dozy',\n",
        "        'aging': 'ageing',\n",
        "        'acknowledgment': 'acknowledgement',\n",
        "        'judgment': 'judgement',\n",
        "        'abridgment': 'abridgement',\n",
        "        'fledgling': 'fledgeling',\n",
        "        'inquiry': 'enquiry',\n",
        "        'insure': 'ensure',\n",
        "        'program': 'programme',\n",
        "        'story': 'storey',  # building level\n",
        "        'prolog': 'prologue',\n",
        "        'catalog': 'catalogue',\n",
        "        'dialog': 'dialogue',\n",
        "        'monolog': 'monologue',\n",
        "        'analog': 'analogue',\n",
        "    }\n",
        "    \n",
        "    return american_to_british\n",
        "\n",
        "def check_spelling_and_convert(word, conversion_dict):\n",
        "    \"\"\"æ£€æŸ¥å¹¶è½¬æ¢æ‹¼å†™\"\"\"\n",
        "    word_lower = word.lower()\n",
        "    if word_lower in conversion_dict:\n",
        "        return conversion_dict[word_lower], True\n",
        "    return word, False\n",
        "\n",
        "def check_spelling_batch(words_info, api_key, conversion_dict, batch_size=20):\n",
        "    \"\"\"æ‰¹é‡æ£€æŸ¥æ‹¼å†™å¹¶è¿›è¡Œè½¬æ¢\"\"\"\n",
        "    if not api_key:\n",
        "        print(\"âš ï¸ APIå¯†é’¥ä¸å¯ç”¨ï¼Œä»…è¿›è¡Œæœ¬åœ°æ‹¼å†™æ£€æŸ¥\")\n",
        "        \n",
        "    spelling_changes = []\n",
        "    total_batches = (len(words_info) + batch_size - 1) // batch_size\n",
        "    \n",
        "    for batch_idx in range(0, len(words_info), batch_size):\n",
        "        batch = words_info[batch_idx:batch_idx + batch_size]\n",
        "        batch_num = batch_idx // batch_size + 1\n",
        "        \n",
        "        print(f\"ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ {batch_num}/{total_batches} çš„æ‹¼å†™ ({len(batch)} ä¸ªè¯æ±‡)...\")\n",
        "        \n",
        "        # å…ˆè¿›è¡Œæœ¬åœ°æ‹¼å†™æ£€æŸ¥\n",
        "        local_changes = []\n",
        "        for i, word_info in enumerate(batch):\n",
        "            word = word_info['word']\n",
        "            british_word, changed = check_spelling_and_convert(word, conversion_dict)\n",
        "            if changed:\n",
        "                local_changes.append({\n",
        "                    'original': word,\n",
        "                    'corrected': british_word,\n",
        "                    'category': word_info['category'],\n",
        "                    'pos': word_info['pos'],\n",
        "                    'type': 'local_conversion'\n",
        "                })\n",
        "        \n",
        "                 # å¦‚æœæœ‰LLM APIï¼Œè¿›è¡Œé¢å¤–çš„æ‹¼å†™æ£€æŸ¥\n",
        "         if api_key and batch:\n",
        "             # æ„å»ºæ‰¹é‡æ£€æŸ¥çš„prompt\n",
        "             word_list = []\n",
        "             for i, word_info in enumerate(batch):\n",
        "                 word_list.append(f\"{i+1}. {word_info['word']}\")\n",
        "             words_text = \"\\n\".join(word_list)\n",
        "             \n",
        "             prompt = f\"\"\"è¯·æ£€æŸ¥ä»¥ä¸‹è‹±æ–‡è¯æ±‡çš„æ‹¼å†™æ˜¯å¦ä¸ºè‹±å›½è‹±è¯­æ‹¼å†™ã€‚å¦‚æœæ˜¯ç¾å›½è‹±è¯­æ‹¼å†™ï¼Œè¯·è½¬æ¢ä¸ºè‹±å›½è‹±è¯­æ‹¼å†™ã€‚\n",
        "\n",
        "å¸¸è§è½¬æ¢è§„åˆ™ï¼š\n",
        "- -ize â†’ -ise (organize â†’ organise)\n",
        "- -or â†’ -our (color â†’ colour)  \n",
        "- -er â†’ -re (center â†’ centre)\n",
        "- -ense â†’ -ence (defense â†’ defence)\n",
        "- å…¶ä»–å¸¸è§è½¬æ¢ (gray â†’ grey, aluminum â†’ aluminium)\n",
        "\n",
        "å¾…æ£€æŸ¥è¯æ±‡ï¼š\n",
        "{words_text}\n",
        "\n",
        "è¯·åªè¿”å›éœ€è¦è½¬æ¢çš„è¯æ±‡ï¼Œæ ¼å¼ä¸ºï¼š\n",
        "ç¼–å·:åŸè¯â†’è‹±å›½æ‹¼å†™\n",
        "ä¾‹å¦‚ï¼š\n",
        "1:organizeâ†’organise\n",
        "3:colorâ†’colour\n",
        "\n",
        "å¦‚æœæ‰€æœ‰è¯æ±‡éƒ½æ˜¯è‹±å›½æ‹¼å†™ï¼Œè¯·è¿”å›\"æ— éœ€è½¬æ¢\"ã€‚\"\"\"\n",
        "\n",
        "            response = call_openrouter_api(prompt, api_key)\\n            if response:\\n                response = response.strip()\\n                if response.lower() not in [\\\"æ— éœ€è½¬æ¢\\\", \\\"æ— \\\", \\\"none\\\", \\\"no changes needed\\\"]:\\n                    try:\\n                        # è§£æLLMè¿”å›çš„æ‹¼å†™è½¬æ¢\\n                        lines = response.split('\\\\n')\\n                        for line in lines:\\n                            line = line.strip()\\n                            if ':' in line and 'â†’' in line:\\n                                parts = line.split(':', 1)\\n                                if len(parts) == 2 and parts[0].strip().isdigit():\\n                                    idx = int(parts[0].strip()) - 1\\n                                    conversion = parts[1].strip()\\n                                    if 'â†’' in conversion:\\n                                        original, corrected = conversion.split('â†’', 1)\\n                                        original = original.strip()\\n                                        corrected = corrected.strip()\\n                                        if 0 <= idx < len(batch):\\n                                            # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨æœ¬åœ°è½¬æ¢ä¸­\\n                                            already_converted = any(c['original'] == original for c in local_changes)\\n                                            if not already_converted:\\n                                                spelling_changes.append({\\n                                                    'original': original,\\n                                                    'corrected': corrected,\\n                                                    'category': batch[idx]['category'],\\n                                                    'pos': batch[idx]['pos'],\\n                                                    'type': 'llm_conversion'\\n                                                })\\n                    except Exception as e:\\n                        print(f\\\"  - âš ï¸ è§£æLLMæ‹¼å†™å›å¤æ—¶å‡ºé”™: {e}\\\")\\n                        print(f\\\"  - åŸå§‹å›å¤: {response[:100]}...\\\")\\n        \\n        # æ·»åŠ æœ¬åœ°è½¬æ¢ç»“æœ\\n        spelling_changes.extend(local_changes)\\n        \\n        # æ˜¾ç¤ºæœ¬æ‰¹æ¬¡çš„è½¬æ¢ç»“æœ\\n        batch_changes = [c for c in spelling_changes if any(w['word'] in [c['original']] for w in batch)]\\n        if batch_changes:\\n            print(f\\\"  ğŸ”„ æœ¬æ‰¹æ¬¡å‘ç° {len(batch_changes)} ä¸ªæ‹¼å†™è½¬æ¢\\\")\\n            for change in batch_changes[-3:]:  # æ˜¾ç¤ºæœ€å3ä¸ª\\n                print(f\\\"    {change['original']} â†’ {change['corrected']}\\\")\\n        \\n        # é¿å…APIè°ƒç”¨è¿‡äºé¢‘ç¹\\n        if api_key and batch_num < total_batches:\\n            time.sleep(1.5)\\n    \\n    return spelling_changes\n",
        "\n",
        "def generate_spelling_report(spelling_changes):\n",
        "    \\\"\\\"\\\"ç”Ÿæˆæ‹¼å†™æ£€æŸ¥æŠ¥å‘Š\\\"\\\"\\\"\\n    if not spelling_changes:\\n        print(f\\\"\\\\nâœ… æ‰€æœ‰è¯æ±‡éƒ½å·²ç»æ˜¯è‹±å›½æ‹¼å†™ï¼\\\")\\n        return\\n    \\n    print(f\\\"\\\\nğŸ“‹ æ‹¼å†™æ£€æŸ¥æŠ¥å‘Š\\\")\\n    print(f\\\"=\\\" * 50)\\n    print(f\\\"å‘ç°éœ€è¦è½¬æ¢çš„è¯æ±‡æ•°: {len(spelling_changes)}\\\")\\n    \\n    # æŒ‰ç±»åˆ«åˆ†ç»„\\n    by_category = {}\\n    by_type = {'local_conversion': 0, 'llm_conversion': 0}\\n    \\n    for change in spelling_changes:\\n        category = change['category']\\n        if category not in by_category:\\n            by_category[category] = []\\n        by_category[category].append(change)\\n        \\n        change_type = change.get('type', 'unknown')\\n        if change_type in by_type:\\n            by_type[change_type] += 1\\n    \\n    print(f\\\"\\\\nğŸ“Š è½¬æ¢ç±»å‹ç»Ÿè®¡:\\\")\\n    print(f\\\"  - æœ¬åœ°è¯å…¸è½¬æ¢: {by_type['local_conversion']}\\\")\\n    print(f\\\"  - LLMè¯†åˆ«è½¬æ¢: {by_type['llm_conversion']}\\\")\\n    \\n    print(f\\\"\\\\nğŸ”„ æ‹¼å†™è½¬æ¢è¯¦æƒ…:\\\")\\n    for category, changes in by_category.items():\\n        print(f\\\"\\\\n  ã€{category.upper()}ã€‘:\\\")\\n        for change in changes:\\n            print(f\\\"    {change['original']} â†’ {change['corrected']} ({change['pos']})\\\")\\n\\ndef update_dataset_spelling(dataset, spelling_changes):\\n    \\\"\\\"\\\"æ›´æ–°æ•°æ®é›†ä¸­çš„æ‹¼å†™\\\"\\\"\\\"\\n    if not spelling_changes:\\n        return dataset, 0\\n    \\n    # åˆ›å»ºè½¬æ¢æ˜ å°„\\n    conversion_map = {change['original']: change['corrected'] for change in spelling_changes}\\n    \\n    updated_count = 0\\n    updated_dataset = []\\n    \\n    for entry in dataset:\\n        updated_entry = entry.copy()\\n        \\n        # æ£€æŸ¥target word\\n        if entry['target'] in conversion_map:\\n            updated_entry['target'] = conversion_map[entry['target']]\\n            updated_count += 1\\n        \\n        # æ£€æŸ¥taboo words\\n        updated_taboo = []\\n        for taboo_word in entry['taboo']:\\n            if taboo_word in conversion_map:\\n                updated_taboo.append(conversion_map[taboo_word])\\n            else:\\n                updated_taboo.append(taboo_word)\\n        updated_entry['taboo'] = updated_taboo\\n        \\n        updated_dataset.append(updated_entry)\\n    \\n    return updated_dataset, updated_count\n",
        "\n",
        "# æ‰§è¡Œè‹±å›½æ‹¼å†™æ£€æŸ¥\n",
        "print(\\\"=== è‹±å›½æ‹¼å†™æ£€æŸ¥å¼€å§‹ ===\\\")\\nprint(\\\"ğŸ” æ­£åœ¨æ£€æŸ¥æ•°æ®é›†ä¸­çš„æ‹¼å†™å¹¶è½¬æ¢ä¸ºè‹±å›½è‹±è¯­...\\\")\\n\\n# åˆ›å»ºè½¬æ¢å­—å…¸\\nconversion_dict = create_spelling_conversion_dict()\\nprint(f\\\"ğŸ“š å·²åŠ è½½ {len(conversion_dict)} ä¸ªæ‹¼å†™è½¬æ¢è§„åˆ™\\\")\\n\\n# åŠ è½½æ•°æ®é›†å’ŒAPIå¯†é’¥\\ndataset = load_generated_dataset()\\napi_key = load_api_key()\\n\\nif dataset:\\n    print(f\\\"âœ… æ•°æ®é›†å·²åŠ è½½ï¼Œå…± {len(dataset)} ä¸ªè¯æ±‡\\\")\\n    \\n    # æå–target words\\n    target_words_info = extract_target_words(dataset)\\n    print(f\\\"âœ… å·²æå–æ‰€æœ‰target words\\\")\\n    \\n    # æ‰¹é‡æ£€æŸ¥æ‹¼å†™\\n    print(f\\\"ğŸ¤– å¼€å§‹è¿›è¡Œæ‹¼å†™æ£€æŸ¥...\\\")\\n    spelling_changes = check_spelling_batch(target_words_info, api_key, conversion_dict)\\n    \\n    # ç”Ÿæˆæ‹¼å†™æŠ¥å‘Š\\n    generate_spelling_report(spelling_changes)\\n    \\n    # æ›´æ–°æ•°æ®é›†\\n    if spelling_changes:\\n        updated_dataset, updated_count = update_dataset_spelling(dataset, spelling_changes)\\n        \\n        # ä¿å­˜æ›´æ–°åçš„æ•°æ®é›†\\n        output_filename = \\\"BRITISH_SPELLING_DATASET.json\\\"\\n        with open(output_filename, 'w', encoding='utf-8') as f:\\n            json.dump(updated_dataset, f, indent=2, ensure_ascii=False)\\n        \\n        print(f\\\"\\\\nâœ… å·²æ›´æ–° {updated_count} ä¸ªtarget wordsçš„æ‹¼å†™\\\")\\n        print(f\\\"ğŸ“„ æ›´æ–°åçš„æ•°æ®é›†å·²ä¿å­˜åˆ°: {output_filename}\\\")\\n        \\n        # ä¿å­˜æ‹¼å†™è½¬æ¢æŠ¥å‘Š\\n        with open('spelling_conversion_report.json', 'w', encoding='utf-8') as f:\\n            json.dump(spelling_changes, f, indent=2, ensure_ascii=False)\\n        print(f\\\"ğŸ“„ æ‹¼å†™è½¬æ¢æŠ¥å‘Šå·²ä¿å­˜åˆ°: spelling_conversion_report.json\\\")\\n    else:\\n        print(f\\\"\\\\nâœ… æ•°æ®é›†æ‹¼å†™æ£€æŸ¥å®Œæˆï¼Œæ— éœ€ä¿®æ”¹\\\")\\n        \\nelse:\\n    print(\\\"âŒ æ— æ³•åŠ è½½æ•°æ®é›†\\\")\\n\\nprint(\\\"\\\\n=== è‹±å›½æ‹¼å†™æ£€æŸ¥å®Œæˆ ===\\\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== è‹±å›½æ‹¼å†™æ£€æŸ¥å¼€å§‹ ===\n",
            "ğŸ” æ­£åœ¨ä½¿ç”¨LLMæ£€æŸ¥æ•°æ®é›†ä¸­çš„æ‹¼å†™å¹¶è½¬æ¢ä¸ºè‹±å›½è‹±è¯­...\n",
            "âœ… æ•°æ®é›†å·²åŠ è½½ï¼Œå…± 300 ä¸ªè¯æ±‡\n",
            "âœ… å·²æå–æ‰€æœ‰target words\n",
            "ğŸ¤– å¼€å§‹ä½¿ç”¨LLMè¿›è¡Œæ‹¼å†™æ£€æŸ¥...\n",
            "ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ 1/15 çš„æ‹¼å†™ (20 ä¸ªè¯æ±‡)...\n",
            "ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ 2/15 çš„æ‹¼å†™ (20 ä¸ªè¯æ±‡)...\n",
            "ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ 3/15 çš„æ‹¼å†™ (20 ä¸ªè¯æ±‡)...\n",
            "ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ 4/15 çš„æ‹¼å†™ (20 ä¸ªè¯æ±‡)...\n",
            "ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ 5/15 çš„æ‹¼å†™ (20 ä¸ªè¯æ±‡)...\n",
            "ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ 6/15 çš„æ‹¼å†™ (20 ä¸ªè¯æ±‡)...\n",
            "ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ 7/15 çš„æ‹¼å†™ (20 ä¸ªè¯æ±‡)...\n",
            "ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ 8/15 çš„æ‹¼å†™ (20 ä¸ªè¯æ±‡)...\n",
            "ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ 9/15 çš„æ‹¼å†™ (20 ä¸ªè¯æ±‡)...\n",
            "ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ 10/15 çš„æ‹¼å†™ (20 ä¸ªè¯æ±‡)...\n",
            "ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ 11/15 çš„æ‹¼å†™ (20 ä¸ªè¯æ±‡)...\n",
            "ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ 12/15 çš„æ‹¼å†™ (20 ä¸ªè¯æ±‡)...\n",
            "ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ 13/15 çš„æ‹¼å†™ (20 ä¸ªè¯æ±‡)...\n",
            "ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ 14/15 çš„æ‹¼å†™ (20 ä¸ªè¯æ±‡)...\n",
            "ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ 15/15 çš„æ‹¼å†™ (20 ä¸ªè¯æ±‡)...\n",
            "\n",
            "âœ… æ‰€æœ‰è¯æ±‡éƒ½å·²ç»æ˜¯è‹±å›½æ‹¼å†™ï¼\n",
            "\n",
            "âœ… æ•°æ®é›†æ‹¼å†™æ£€æŸ¥å®Œæˆï¼Œæ— éœ€ä¿®æ”¹\n",
            "\n",
            "=== è‹±å›½æ‹¼å†™æ£€æŸ¥å®Œæˆ ===\n"
          ]
        }
      ],
      "source": [
        "# è‹±å›½æ‹¼å†™æ£€æŸ¥åŠŸèƒ½ - ä½¿ç”¨LLMç›´æ¥æ£€æŸ¥\n",
        "import json\n",
        "import time\n",
        "\n",
        "def check_spelling_with_llm(words_info, api_key, batch_size=20):\n",
        "    \"\"\"ä½¿ç”¨LLMæ‰¹é‡æ£€æŸ¥æ‹¼å†™å¹¶è¿›è¡Œè½¬æ¢\"\"\"\n",
        "    if not api_key:\n",
        "        print(\"âŒ APIå¯†é’¥ä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œæ‹¼å†™æ£€æŸ¥\")\n",
        "        return []\n",
        "        \n",
        "    spelling_changes = []\n",
        "    total_batches = (len(words_info) + batch_size - 1) // batch_size\n",
        "    \n",
        "    for batch_idx in range(0, len(words_info), batch_size):\n",
        "        batch = words_info[batch_idx:batch_idx + batch_size]\n",
        "        batch_num = batch_idx // batch_size + 1\n",
        "        \n",
        "        print(f\"ğŸ” æ­£åœ¨æ£€æŸ¥æ‰¹æ¬¡ {batch_num}/{total_batches} çš„æ‹¼å†™ ({len(batch)} ä¸ªè¯æ±‡)...\")\n",
        "        \n",
        "        # æ„å»ºæ‰¹é‡æ£€æŸ¥çš„prompt\n",
        "        word_list = []\n",
        "        for i, word_info in enumerate(batch):\n",
        "            word_list.append(f\"{i+1}. {word_info['word']}\")\n",
        "        words_text = \"\\n\".join(word_list)\n",
        "        \n",
        "        prompt = f\"\"\"è¯·æ£€æŸ¥ä»¥ä¸‹è‹±æ–‡è¯æ±‡çš„æ‹¼å†™æ˜¯å¦ä¸ºè‹±å›½è‹±è¯­æ‹¼å†™ã€‚å¦‚æœæ˜¯ç¾å›½è‹±è¯­æ‹¼å†™ï¼Œè¯·è½¬æ¢ä¸ºè‹±å›½è‹±è¯­æ‹¼å†™ã€‚\n",
        "\n",
        "å¸¸è§è½¬æ¢è§„åˆ™ï¼š\n",
        "- -ize â†’ -ise (organize â†’ organise)\n",
        "- -or â†’ -our (color â†’ colour)  \n",
        "- -er â†’ -re (center â†’ centre)\n",
        "- -ense â†’ -ence (defense â†’ defence)\n",
        "- å…¶ä»–å¸¸è§è½¬æ¢ (gray â†’ grey, aluminum â†’ aluminium)\n",
        "\n",
        "å¾…æ£€æŸ¥è¯æ±‡ï¼š\n",
        "{words_text}\n",
        "\n",
        "è¯·åªè¿”å›éœ€è¦è½¬æ¢çš„è¯æ±‡ï¼Œæ ¼å¼ä¸ºï¼š\n",
        "ç¼–å·:åŸè¯â†’è‹±å›½æ‹¼å†™\n",
        "ä¾‹å¦‚ï¼š\n",
        "1:organizeâ†’organise\n",
        "3:colorâ†’colour\n",
        "\n",
        "å¦‚æœæ‰€æœ‰è¯æ±‡éƒ½æ˜¯è‹±å›½æ‹¼å†™ï¼Œè¯·è¿”å›\"æ— éœ€è½¬æ¢\"ã€‚\"\"\"\n",
        "\n",
        "        response = call_openrouter_api(prompt, api_key)\n",
        "        if response:\n",
        "            response = response.strip()\n",
        "            if response.lower() not in [\"æ— éœ€è½¬æ¢\", \"æ— \", \"none\", \"no changes needed\"]:\n",
        "                try:\n",
        "                    # è§£æLLMè¿”å›çš„æ‹¼å†™è½¬æ¢\n",
        "                    lines = response.split('\\n')\n",
        "                    for line in lines:\n",
        "                        line = line.strip()\n",
        "                        if ':' in line and 'â†’' in line:\n",
        "                            parts = line.split(':', 1)\n",
        "                            if len(parts) == 2 and parts[0].strip().isdigit():\n",
        "                                idx = int(parts[0].strip()) - 1\n",
        "                                conversion = parts[1].strip()\n",
        "                                if 'â†’' in conversion:\n",
        "                                    original, corrected = conversion.split('â†’', 1)\n",
        "                                    original = original.strip()\n",
        "                                    corrected = corrected.strip()\n",
        "                                    if 0 <= idx < len(batch):\n",
        "                                        spelling_changes.append({\n",
        "                                            'original': original,\n",
        "                                            'corrected': corrected,\n",
        "                                            'category': batch[idx]['category'],\n",
        "                                            'pos': batch[idx]['pos'],\n",
        "                                            'type': 'llm_conversion'\n",
        "                                        })\n",
        "                except Exception as e:\n",
        "                    print(f\"  - âš ï¸ è§£æLLMæ‹¼å†™å›å¤æ—¶å‡ºé”™: {e}\")\n",
        "                    print(f\"  - åŸå§‹å›å¤: {response[:100]}...\")\n",
        "        \n",
        "        # æ˜¾ç¤ºæœ¬æ‰¹æ¬¡çš„è½¬æ¢ç»“æœ\n",
        "        batch_changes = [c for c in spelling_changes if any(w['word'] == c['original'] for w in batch)]\n",
        "        if batch_changes:\n",
        "            print(f\"  ğŸ”„ æœ¬æ‰¹æ¬¡å‘ç° {len(batch_changes)} ä¸ªæ‹¼å†™è½¬æ¢\")\n",
        "            for change in batch_changes[-3:]:  # æ˜¾ç¤ºæœ€å3ä¸ª\n",
        "                print(f\"    {change['original']} â†’ {change['corrected']}\")\n",
        "        \n",
        "        # é¿å…APIè°ƒç”¨è¿‡äºé¢‘ç¹\n",
        "        if batch_num < total_batches:\n",
        "            time.sleep(1.5)\n",
        "    \n",
        "    return spelling_changes\n",
        "\n",
        "def generate_spelling_report(spelling_changes):\n",
        "    \"\"\"ç”Ÿæˆæ‹¼å†™æ£€æŸ¥æŠ¥å‘Š\"\"\"\n",
        "    if not spelling_changes:\n",
        "        print(f\"\\nâœ… æ‰€æœ‰è¯æ±‡éƒ½å·²ç»æ˜¯è‹±å›½æ‹¼å†™ï¼\")\n",
        "        return\n",
        "    \n",
        "    print(f\"\\nğŸ“‹ æ‹¼å†™æ£€æŸ¥æŠ¥å‘Š\")\n",
        "    print(f\"=\" * 50)\n",
        "    print(f\"å‘ç°éœ€è¦è½¬æ¢çš„è¯æ±‡æ•°: {len(spelling_changes)}\")\n",
        "    \n",
        "    # æŒ‰ç±»åˆ«åˆ†ç»„\n",
        "    by_category = {}\n",
        "    \n",
        "    for change in spelling_changes:\n",
        "        category = change['category']\n",
        "        if category not in by_category:\n",
        "            by_category[category] = []\n",
        "        by_category[category].append(change)\n",
        "    \n",
        "    print(f\"\\nğŸ”„ æ‹¼å†™è½¬æ¢è¯¦æƒ…:\")\n",
        "    for category, changes in by_category.items():\n",
        "        print(f\"\\n  ã€{category.upper()}ã€‘:\")\n",
        "        for change in changes:\n",
        "            print(f\"    {change['original']} â†’ {change['corrected']} ({change['pos']})\")\n",
        "\n",
        "def update_dataset_spelling(dataset, spelling_changes):\n",
        "    \"\"\"æ›´æ–°æ•°æ®é›†ä¸­çš„æ‹¼å†™\"\"\"\n",
        "    if not spelling_changes:\n",
        "        return dataset, 0\n",
        "    \n",
        "    # åˆ›å»ºè½¬æ¢æ˜ å°„\n",
        "    conversion_map = {change['original']: change['corrected'] for change in spelling_changes}\n",
        "    \n",
        "    updated_count = 0\n",
        "    updated_dataset = []\n",
        "    \n",
        "    for entry in dataset:\n",
        "        updated_entry = entry.copy()\n",
        "        \n",
        "        # æ£€æŸ¥target word\n",
        "        if entry['target'] in conversion_map:\n",
        "            updated_entry['target'] = conversion_map[entry['target']]\n",
        "            updated_count += 1\n",
        "        \n",
        "        # æ£€æŸ¥taboo words\n",
        "        updated_taboo = []\n",
        "        for taboo_word in entry['taboo']:\n",
        "            if taboo_word in conversion_map:\n",
        "                updated_taboo.append(conversion_map[taboo_word])\n",
        "            else:\n",
        "                updated_taboo.append(taboo_word)\n",
        "        updated_entry['taboo'] = updated_taboo\n",
        "        \n",
        "        updated_dataset.append(updated_entry)\n",
        "    \n",
        "    return updated_dataset, updated_count\n",
        "\n",
        "# æ‰§è¡Œè‹±å›½æ‹¼å†™æ£€æŸ¥\n",
        "print(\"=== è‹±å›½æ‹¼å†™æ£€æŸ¥å¼€å§‹ ===\")\n",
        "print(\"ğŸ” æ­£åœ¨ä½¿ç”¨LLMæ£€æŸ¥æ•°æ®é›†ä¸­çš„æ‹¼å†™å¹¶è½¬æ¢ä¸ºè‹±å›½è‹±è¯­...\")\n",
        "\n",
        "# åŠ è½½æ•°æ®é›†å’ŒAPIå¯†é’¥\n",
        "dataset = load_generated_dataset()\n",
        "api_key = load_api_key()\n",
        "\n",
        "if dataset and api_key:\n",
        "    print(f\"âœ… æ•°æ®é›†å·²åŠ è½½ï¼Œå…± {len(dataset)} ä¸ªè¯æ±‡\")\n",
        "    \n",
        "    # æå–target words\n",
        "    target_words_info = extract_target_words(dataset)\n",
        "    print(f\"âœ… å·²æå–æ‰€æœ‰target words\")\n",
        "    \n",
        "    # ä½¿ç”¨LLMæ‰¹é‡æ£€æŸ¥æ‹¼å†™\n",
        "    print(f\"ğŸ¤– å¼€å§‹ä½¿ç”¨LLMè¿›è¡Œæ‹¼å†™æ£€æŸ¥...\")\n",
        "    spelling_changes = check_spelling_with_llm(target_words_info, api_key)\n",
        "    \n",
        "    # ç”Ÿæˆæ‹¼å†™æŠ¥å‘Š\n",
        "    generate_spelling_report(spelling_changes)\n",
        "    \n",
        "    # æ›´æ–°æ•°æ®é›†\n",
        "    if spelling_changes:\n",
        "        updated_dataset, updated_count = update_dataset_spelling(dataset, spelling_changes)\n",
        "        \n",
        "        # ä¿å­˜æ›´æ–°åçš„æ•°æ®é›†\n",
        "        output_filename = \"BRITISH_SPELLING_DATASET.json\"\n",
        "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(updated_dataset, f, indent=2, ensure_ascii=False)\n",
        "        \n",
        "        print(f\"\\nâœ… å·²æ›´æ–° {updated_count} ä¸ªtarget wordsçš„æ‹¼å†™\")\n",
        "        print(f\"ğŸ“„ æ›´æ–°åçš„æ•°æ®é›†å·²ä¿å­˜åˆ°: {output_filename}\")\n",
        "        \n",
        "        # ä¿å­˜æ‹¼å†™è½¬æ¢æŠ¥å‘Š\n",
        "        with open('spelling_conversion_report.json', 'w', encoding='utf-8') as f:\n",
        "            json.dump(spelling_changes, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"ğŸ“„ æ‹¼å†™è½¬æ¢æŠ¥å‘Šå·²ä¿å­˜åˆ°: spelling_conversion_report.json\")\n",
        "    else:\n",
        "        print(f\"\\nâœ… æ•°æ®é›†æ‹¼å†™æ£€æŸ¥å®Œæˆï¼Œæ— éœ€ä¿®æ”¹\")\n",
        "        \n",
        "elif not dataset:\n",
        "    print(\"âŒ æ— æ³•åŠ è½½æ•°æ®é›†\")\n",
        "elif not api_key:\n",
        "    print(\"âŒ æ— æ³•åŠ è½½APIå¯†é’¥ï¼Œæ‹¼å†™æ£€æŸ¥åŠŸèƒ½ä¸å¯ç”¨\")\n",
        "\n",
        "print(\"\\n=== è‹±å›½æ‹¼å†™æ£€æŸ¥å®Œæˆ ===\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== å¼€å§‹åŸºäºLLMè´¨é‡æ£€æŸ¥ç»“æœé‡æ–°ç”Ÿæˆæ•°æ®é›† ===\n",
            "ğŸ“‹ å‘ç° 1 ä¸ªéœ€è¦ç§»é™¤çš„è¯æ±‡\n",
            "ğŸ“Š åŸæ•°æ®é›†å…± 300 ä¸ªè¯æ±‡\n",
            "\\nğŸ—‘ï¸ æŒ‰ç±»åˆ«ç§»é™¤ç»Ÿè®¡:\n",
            "  - philosophy: 1 ä¸ª\n",
            "\\nâœ… è¿‡æ»¤åä¿ç•™ 299 ä¸ªè¯æ±‡\n",
            "\\nğŸ”„ å¼€å§‹è¡¥å……æ–°è¯æ±‡...\n",
            "\\nğŸ“ ä¸º philosophy è¡¥å…… 1 ä¸ªè¯æ±‡:\n",
            "  å¯ç”¨å€™é€‰è¯: 473 ä¸ª\n",
            "    âœ… abilities (taboo: 5)\n",
            "  ğŸ“Š æˆåŠŸè¡¥å……: 1/1\n",
            "\\nâœ… æœ€ç»ˆæ•°æ®é›†å·²ä¿å­˜: FINAL_DATASET.json\n",
            "ğŸ“Š æœ€ç»ˆç»Ÿè®¡:\n",
            "  - ä¿ç•™åŸè¯æ±‡: 299\n",
            "  - æ–°å¢è¯æ±‡: 1\n",
            "  - æ€»è®¡: 300\n",
            "\\nğŸ“ˆ ç±»åˆ«åˆ†å¸ƒ:\n",
            "  - general: 100\n",
            "  - chemistry: 50\n",
            "  - cs: 50\n",
            "  - finance: 50\n",
            "  - philosophy: 50\n",
            "\\nğŸ† æ•°æ®é›†é‡æ–°ç”Ÿæˆå®Œæˆï¼\n"
          ]
        }
      ],
      "source": [
        "# åŸºäºLLMè´¨é‡æ£€æŸ¥ç»“æœé‡æ–°ç”Ÿæˆæ•°æ®é›†\n",
        "print(\"=== å¼€å§‹åŸºäºLLMè´¨é‡æ£€æŸ¥ç»“æœé‡æ–°ç”Ÿæˆæ•°æ®é›† ===\")\n",
        "\n",
        "# åŠ è½½ä¸åˆé€‚è¯æ±‡æŠ¥å‘Š\n",
        "try:\n",
        "    with open('inappropriate_words_report.json', 'r', encoding='utf-8') as f:\n",
        "        inappropriate_words = json.load(f)\n",
        "    print(f\"ğŸ“‹ å‘ç° {len(inappropriate_words)} ä¸ªéœ€è¦ç§»é™¤çš„è¯æ±‡\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ æ— æ³•åŠ è½½ä¸åˆé€‚è¯æ±‡æŠ¥å‘Š: {e}\")\n",
        "    inappropriate_words = []\n",
        "\n",
        "if inappropriate_words:\n",
        "    # åŠ è½½åŸæ•°æ®é›†\n",
        "    original_dataset = load_generated_dataset()\n",
        "    if not original_dataset:\n",
        "        print(\"âŒ æ— æ³•åŠ è½½åŸæ•°æ®é›†\")\n",
        "    else:\n",
        "        print(f\"ğŸ“Š åŸæ•°æ®é›†å…± {len(original_dataset)} ä¸ªè¯æ±‡\")\n",
        "        \n",
        "        # åˆ›å»ºé»‘åå•\n",
        "        inappropriate_word_set = {item['word'] for item in inappropriate_words}\n",
        "        \n",
        "        # ç»Ÿè®¡éœ€è¦ç§»é™¤çš„è¯æ±‡æŒ‰ç±»åˆ«åˆ†å¸ƒ\n",
        "        removal_by_category = {}\n",
        "        for item in inappropriate_words:\n",
        "            category = item['category'] \n",
        "            removal_by_category[category] = removal_by_category.get(category, 0) + 1\n",
        "        \n",
        "        print(f\"\\\\nğŸ—‘ï¸ æŒ‰ç±»åˆ«ç§»é™¤ç»Ÿè®¡:\")\n",
        "        for category, count in removal_by_category.items():\n",
        "            print(f\"  - {category}: {count} ä¸ª\")\n",
        "        \n",
        "        # è¿‡æ»¤æ•°æ®é›†\n",
        "        filtered_dataset = [entry for entry in original_dataset if entry['target'] not in inappropriate_word_set]\n",
        "        print(f\"\\\\nâœ… è¿‡æ»¤åä¿ç•™ {len(filtered_dataset)} ä¸ªè¯æ±‡\")\n",
        "        \n",
        "        # ä»ç°æœ‰å€™é€‰æ± è¡¥å……æ–°è¯æ±‡\n",
        "        print(f\"\\\\nğŸ”„ å¼€å§‹è¡¥å……æ–°è¯æ±‡...\")\n",
        "        new_entries = []\n",
        "        \n",
        "        # åˆ›å»ºé»‘åå•ï¼ˆåŒ…å«å·²ç§»é™¤å’Œå·²å­˜åœ¨çš„è¯æ±‡ï¼‰\n",
        "        existing_words = {entry['target'] for entry in filtered_dataset}\n",
        "        blacklist = inappropriate_word_set | existing_words\n",
        "        \n",
        "        # ä¸ºæ¯ä¸ªç±»åˆ«è¡¥å……è¯æ±‡\n",
        "        for category, needed_count in removal_by_category.items():\n",
        "            print(f\"\\\\nğŸ“ ä¸º {category} è¡¥å…… {needed_count} ä¸ªè¯æ±‡:\")\n",
        "            \n",
        "            # é€‰æ‹©å€™é€‰æ± \n",
        "            if category == 'general':\n",
        "                candidates = [w for w in wn.words() \n",
        "                            if (w.isalpha() and '_' not in w and 3 < len(w) < 16 and \n",
        "                                w not in blacklist and wn.synsets(w))]\n",
        "            else:\n",
        "                # ä»ä¸“ä¸šè¯æ±‡æ± é€‰æ‹©\n",
        "                candidates = []\n",
        "                if category in professional_pools:\n",
        "                    candidates = [w for w in professional_pools[category] \n",
        "                                if w not in blacklist and wn.synsets(w)]\n",
        "            \n",
        "            print(f\"  å¯ç”¨å€™é€‰è¯: {len(candidates)} ä¸ª\")\n",
        "            random.shuffle(candidates)\n",
        "            \n",
        "            added = 0\n",
        "            for word in candidates[:needed_count * 3]:  # å°è¯•æ›´å¤šå€™é€‰è¯\n",
        "                if added >= needed_count:\n",
        "                    break\n",
        "                    \n",
        "                try:\n",
        "                    # ç”Ÿæˆå®Œæ•´è¯æ±‡æ¡ç›®\n",
        "                    senses = get_all_senses(word)\n",
        "                    if not senses:\n",
        "                        continue\n",
        "                        \n",
        "                    primary_synset = wn.synset(senses[0]['name'])\n",
        "                    pos_map = {'n': 'noun', 'v': 'verb', 'a': 'adj', 'r': 'adverb', 's': 'adj'}\n",
        "                    main_pos = pos_map.get(senses[0]['pos'], 'other')\n",
        "                    \n",
        "                    # ç”Ÿæˆtaboo words\n",
        "                    taboo_words = generate_enhanced_taboo_words(primary_synset, word, senses)\n",
        "                    \n",
        "                    # LLMè¡¥å……ï¼ˆå¦‚æœéœ€è¦ï¼‰\n",
        "                    if len(taboo_words) < 5 and OPENROUTER_API_KEY:\n",
        "                        llm_taboos = generate_llm_taboo_words(\n",
        "                            word, taboo_words, main_pos, \n",
        "                            primary_synset.definition(), OPENROUTER_API_KEY\n",
        "                        )\n",
        "                        if llm_taboos:\n",
        "                            taboo_words.extend(llm_taboos)\n",
        "                        taboo_words = taboo_words[:5]\n",
        "                    \n",
        "                    # åˆ›å»ºæ¡ç›®\n",
        "                    new_entry = {\n",
        "                        \"target\": word,\n",
        "                        \"part_of_speech\": main_pos,\n",
        "                        \"taboo\": taboo_words,\n",
        "                        \"category\": category,\n",
        "                        \"senses\": senses,\n",
        "                        \"metadata\": {\n",
        "                            \"sense_count\": len(senses),\n",
        "                            \"concreteness_score\": round(concreteness_lookup.get(word, 0), 2) if concreteness_lookup.get(word) else None,\n",
        "                            \"taboo_count\": len(taboo_words)\n",
        "                        }\n",
        "                    }\n",
        "                    \n",
        "                    new_entries.append(new_entry)\n",
        "                    blacklist.add(word)\n",
        "                    added += 1\n",
        "                    \n",
        "                    print(f\"    âœ… {word} (taboo: {len(taboo_words)})\")\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"    âš ï¸ è·³è¿‡ {word}: {e}\")\n",
        "                    continue\n",
        "            \n",
        "            print(f\"  ğŸ“Š æˆåŠŸè¡¥å……: {added}/{needed_count}\")\n",
        "        \n",
        "        # åˆå¹¶æ•°æ®é›†\n",
        "        final_dataset = filtered_dataset + new_entries\n",
        "        \n",
        "        # ä¿å­˜æœ€ç»ˆæ•°æ®é›†\n",
        "        with open('FINAL_DATASET.json', 'w', encoding='utf-8') as f:\n",
        "            json.dump(final_dataset, f, indent=2, ensure_ascii=False)\n",
        "        \n",
        "        print(f\"\\\\nâœ… æœ€ç»ˆæ•°æ®é›†å·²ä¿å­˜: FINAL_DATASET.json\")\n",
        "        print(f\"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:\")\n",
        "        print(f\"  - ä¿ç•™åŸè¯æ±‡: {len(filtered_dataset)}\")\n",
        "        print(f\"  - æ–°å¢è¯æ±‡: {len(new_entries)}\")\n",
        "        print(f\"  - æ€»è®¡: {len(final_dataset)}\")\n",
        "        \n",
        "        # ç±»åˆ«åˆ†å¸ƒ\n",
        "        final_df = pd.DataFrame(final_dataset)\n",
        "        print(f\"\\\\nğŸ“ˆ ç±»åˆ«åˆ†å¸ƒ:\")\n",
        "        for category, count in final_df['category'].value_counts().items():\n",
        "            print(f\"  - {category}: {count}\")\n",
        "        \n",
        "        print(f\"\\\\nğŸ† æ•°æ®é›†é‡æ–°ç”Ÿæˆå®Œæˆï¼\")\n",
        "else:\n",
        "    print(\"âœ… æ²¡æœ‰å‘ç°éœ€è¦ç§»é™¤çš„è¯æ±‡ï¼Œæ•°æ®é›†è´¨é‡å¾ˆå¥½ï¼\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "300ä¸ªè¯çš„è¯æ€§åˆ†å¸ƒ:\n",
            "noun: 256 (85.3%)\n",
            "adj: 32 (10.7%)\n",
            "verb: 12 (4.0%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 20010 (\\N{CJK UNIFIED IDEOGRAPH-4E2A}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/opt/homebrew/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 35789 (\\N{CJK UNIFIED IDEOGRAPH-8BCD}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/opt/homebrew/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 30340 (\\N{CJK UNIFIED IDEOGRAPH-7684}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/opt/homebrew/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 24615 (\\N{CJK UNIFIED IDEOGRAPH-6027}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/opt/homebrew/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 20998 (\\N{CJK UNIFIED IDEOGRAPH-5206}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/opt/homebrew/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 24067 (\\N{CJK UNIFIED IDEOGRAPH-5E03}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ç»Ÿè®¡å¹¶å¯è§†åŒ–300ä¸ªè¯çš„è¯æ€§åˆ†å¸ƒ\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# åŠ è½½æ•°æ®é›†\n",
        "try:\n",
        "    with open('ENHANCED_DATASET.json', 'r', encoding='utf-8') as f:\n",
        "        dataset = json.load(f)\n",
        "except FileNotFoundError:\n",
        "    with open('data/dataset.json', 'r', encoding='utf-8') as f:\n",
        "        dataset = json.load(f)\n",
        "\n",
        "df = pd.DataFrame(dataset)\n",
        "pos_counts = df['part_of_speech'].value_counts()\n",
        "pos_perc = df['part_of_speech'].value_counts(normalize=True) * 100\n",
        "\n",
        "print('300ä¸ªè¯çš„è¯æ€§åˆ†å¸ƒ:')\n",
        "for pos in pos_counts.index:\n",
        "    print(f\"{pos}: {pos_counts[pos]} ({pos_perc[pos]:.1f}%)\")\n",
        "\n",
        "# é¥¼å›¾å¯è§†åŒ–\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.pie(pos_counts, labels=[f\"{k} ({v}ä¸ª)\" for k,v in pos_counts.items()], autopct='%1.1f%%', startangle=90)\n",
        "plt.title('300ä¸ªè¯çš„è¯æ€§åˆ†å¸ƒ')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "quick80æ•°æ®é›†å·²ç”Ÿæˆï¼Œå…±80ä¸ªè¯ã€‚å·²ä¿å­˜ä¸ºquick80_dataset.json\n",
            "\n",
            "ã€nounã€‘\n",
            "  ä»300è¯æ•°æ®é›†æŠ½å–ï¼ˆ20ï¼‰ï¼š\n",
            "    behaviorism, carcharhinus, aphrodisiac, futures, stylus, strength, authenticity, liquidity, wasteland, bevy, debenture, substance, architecture, perfectionism, virus, psittacidae, unemployment, short, necrosis, parentage\n",
            "  ä»WordNetè¡¥å……ï¼ˆ0ï¼‰ï¼š\n",
            "    \n",
            "\n",
            "ã€adjã€‘\n",
            "  ä»300è¯æ•°æ®é›†æŠ½å–ï¼ˆ20ï¼‰ï¼š\n",
            "    biological, scandent, immutable, epideictical, friendless, trinuclear, many, environmental, exilic, cubiform, quaggy, livable, coital, convincible, biomedical, allometric, ungovernable, nomothetic, combinatory, stomatal\n",
            "  ä»WordNetè¡¥å……ï¼ˆ0ï¼‰ï¼š\n",
            "    \n",
            "\n",
            "ã€advã€‘\n",
            "  ä»300è¯æ•°æ®é›†æŠ½å–ï¼ˆ0ï¼‰ï¼š\n",
            "    \n",
            "  ä»WordNetè¡¥å……ï¼ˆ20ï¼‰ï¼š\n",
            "    obtrusively, permissively, nervily, past, chattily, forbiddingly, sneakingly, wickedly, tonight, concisely, p.m., insanely, capably, low, kinda, inquiringly, often, con, centennially, unmemorably\n",
            "\n",
            "ã€verbã€‘\n",
            "  ä»300è¯æ•°æ®é›†æŠ½å–ï¼ˆ12ï¼‰ï¼š\n",
            "    dishearten, depend, calumniate, promote, blinded, lignify, inoculate, atomize, unzipping, booting, backtracking, underwriting\n",
            "  ä»WordNetè¡¥å……ï¼ˆ8ï¼‰ï¼š\n",
            "    dispensed, bevel, mushroom, believing, caulk, ranking, farm, brooks\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "import pandas as pd\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "# ========== LLMç›¸å…³å‡½æ•° ==========\n",
        "def load_api_key():\n",
        "    try:\n",
        "        with open('api_keys.json', 'r') as f:\n",
        "            api_keys = json.load(f)\n",
        "        return api_keys.get('OPENROUTER_API_KEY', None)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def call_openrouter_api(prompt, api_key, max_retries=3):\n",
        "    import requests, time\n",
        "    if not api_key:\n",
        "        return None\n",
        "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": \"deepseek/deepseek-chat-v3-0324:free\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "        \"max_tokens\": 150,\n",
        "        \"temperature\": 0.7\n",
        "    }\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.post(url, headers=headers, json=data, timeout=30)\n",
        "            if response.status_code == 200:\n",
        "                result = response.json()\n",
        "                return result['choices'][0]['message']['content'].strip()\n",
        "            else:\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(2 ** attempt)\n",
        "        except Exception:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(2 ** attempt)\n",
        "    return None\n",
        "\n",
        "def generate_llm_taboo_words(target_word, existing_taboos, part_of_speech, definition, api_key):\n",
        "    needed_count = 5 - len(existing_taboos)\n",
        "    if needed_count <= 0 or not api_key:\n",
        "        return []\n",
        "    existing_str = \", \".join(existing_taboos) if existing_taboos else \"æ— \"\n",
        "    prompt = f\"\"\"ä¸ºTabooæ¸¸æˆç”Ÿæˆç¦ç”¨è¯ã€‚\n",
        "\n",
        "ç›®æ ‡è¯: {target_word}\n",
        "è¯æ€§: {part_of_speech}  \n",
        "å®šä¹‰: {definition}\n",
        "å·²æœ‰ç¦ç”¨è¯: {existing_str}\n",
        "\n",
        "è¯·ä¸º\"{target_word}\"ç”Ÿæˆ{needed_count}ä¸ªæ–°çš„ç¦ç”¨è¯ï¼Œè¦æ±‚ï¼š\n",
        "1. ä¸ç›®æ ‡è¯è¯­ä¹‰ç›¸å…³ä½†ä¸èƒ½ç›´æ¥è¯´å‡º\n",
        "2. ä¸èƒ½é‡å¤å·²æœ‰ç¦ç”¨è¯  \n",
        "3. ä¼˜å…ˆé€‰æ‹©åŒä¹‰è¯ã€ç›¸å…³æ¦‚å¿µã€ä¸Šä¸‹ä½è¯\n",
        "4. æ¯ä¸ªè¯ç”¨è‹±æ–‡ï¼Œå•ä¸ªè¯æ±‡ï¼Œå°å†™\n",
        "5. é¿å…è¿‡äºé€šç”¨çš„è¯\n",
        "\n",
        "åªè¾“å‡º{needed_count}ä¸ªè‹±æ–‡å•è¯ï¼Œç”¨é€—å·åˆ†éš”ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚\"\"\"\n",
        "    response = call_openrouter_api(prompt, api_key)\n",
        "    if response:\n",
        "        words = [w.strip().lower() for w in response.split(',')]\n",
        "        valid_words = []\n",
        "        for word in words:\n",
        "            if (word.isalpha() and len(word) > 2 and \n",
        "                word not in existing_taboos and \n",
        "                word != target_word.lower() and\n",
        "                len(valid_words) < needed_count):\n",
        "                valid_words.append(word)\n",
        "        return valid_words\n",
        "    return []\n",
        "\n",
        "# ========== åŠ è½½æ•°æ® ==========\n",
        "try:\n",
        "    with open('ENHANCED_DATASET.json', 'r', encoding='utf-8') as f:\n",
        "        dataset = json.load(f)\n",
        "except FileNotFoundError:\n",
        "    with open('data/dataset.json', 'r', encoding='utf-8') as f:\n",
        "        dataset = json.load(f)\n",
        "\n",
        "df = pd.DataFrame(dataset)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "api_key = load_api_key()\n",
        "\n",
        "# ç›®æ ‡è¯æ€§\n",
        "pos_targets = {\n",
        "    'noun': ['noun', 'n'],\n",
        "    'adj': ['adj', 'a', 's'],\n",
        "    'adv': ['adverb', 'r'],\n",
        "    'verb': ['verb', 'v']\n",
        "}\n",
        "quick80 = []\n",
        "used_targets = set()\n",
        "from_300 = {k: [] for k in pos_targets}\n",
        "from_wn = {k: [] for k in pos_targets}\n",
        "\n",
        "# å¢å¼ºç‰ˆ taboo word ç”Ÿæˆå™¨\n",
        "def generate_taboo_words(syn, target_word):\n",
        "    taboo_set = set()\n",
        "    target_word_lower = target_word.lower()\n",
        "    # åŒä¹‰è¯\n",
        "    for lemma in syn.lemmas():\n",
        "        synonym = lemma.name().lower().replace('_', ' ')\n",
        "        if synonym != target_word_lower and len(synonym.split()) <= 2:\n",
        "            main_word = synonym.split()[0]\n",
        "            if main_word.isalpha() and len(main_word) > 2:\n",
        "                taboo_set.add(main_word)\n",
        "            if len(synonym.split()) <= 2 and all(w.isalpha() for w in synonym.split()):\n",
        "                taboo_set.add(synonym.replace(' ', ''))\n",
        "    # åä¹‰è¯\n",
        "    if syn.lemmas():\n",
        "        for antonym_lemma in syn.lemmas()[0].antonyms():\n",
        "            antonym = antonym_lemma.name().lower().replace('_', ' ')\n",
        "            main_word = antonym.split()[0]\n",
        "            if main_word.isalpha() and len(main_word) > 2:\n",
        "                taboo_set.add(main_word)\n",
        "    # å®šä¹‰å…³é”®è¯\n",
        "    definition = syn.definition()\n",
        "    tokens = word_tokenize(definition)\n",
        "    tagged = pos_tag(tokens)\n",
        "    for word, tag in tagged:\n",
        "        word_lower = word.lower()\n",
        "        if (tag.startswith(('NN', 'VB', 'JJ')) and \n",
        "            word_lower.isalpha() and \n",
        "            word_lower not in stop_words and \n",
        "            len(word_lower) > 3 and \n",
        "            word_lower != target_word_lower):\n",
        "            taboo_set.add(word_lower)\n",
        "    # ä¸Šä¸‹ä½è¯\n",
        "    try:\n",
        "        for hypernym in syn.hypernyms():\n",
        "            for lemma in hypernym.lemmas():\n",
        "                hyper_word = lemma.name().lower().replace('_', ' ').split()[0]\n",
        "                if (hyper_word.isalpha() and len(hyper_word) > 3 and hyper_word != target_word_lower):\n",
        "                    taboo_set.add(hyper_word)\n",
        "        hyponyms = syn.hyponyms()[:2]\n",
        "        for hyponym in hyponyms:\n",
        "            for lemma in hyponym.lemmas():\n",
        "                hypo_word = lemma.name().lower().replace('_', ' ').split()[0]\n",
        "                if (hypo_word.isalpha() and len(hypo_word) > 3 and hypo_word != target_word_lower):\n",
        "                    taboo_set.add(hypo_word)\n",
        "    except:\n",
        "        pass\n",
        "    # è¿‡æ»¤å’Œæ’åº\n",
        "    taboo_list = list(taboo_set)\n",
        "    taboo_list.sort(key=lambda x: (abs(len(x) - 6), x))\n",
        "    final_taboos = []\n",
        "    seen = set()\n",
        "    for word in taboo_list:\n",
        "        if word not in seen and len(final_taboos) < 5:\n",
        "            final_taboos.append(word)\n",
        "            seen.add(word)\n",
        "    return final_taboos\n",
        "\n",
        "# åªå¯¹WordNetè¡¥å……çš„è¯ç”Ÿæˆtaboo\n",
        "def build_entry_from_wordnet(word, pos_tag):\n",
        "    synsets = wn.synsets(word)\n",
        "    if not synsets:\n",
        "        return None\n",
        "    # å–ç¬¬ä¸€ä¸ªåŒ¹é…è¯æ€§çš„synset\n",
        "    for s in synsets:\n",
        "        if pos_tag == 'noun' and s.pos() == 'n':\n",
        "            syn = s\n",
        "            break\n",
        "        elif pos_tag == 'adj' and s.pos() in ['a', 's']:\n",
        "            syn = s\n",
        "            break\n",
        "        elif pos_tag == 'adv' and s.pos() == 'r':\n",
        "            syn = s\n",
        "            break\n",
        "        elif pos_tag == 'verb' and s.pos() == 'v':\n",
        "            syn = s\n",
        "            break\n",
        "    else:\n",
        "        return None\n",
        "    pos_map = {'n': 'noun', 'v': 'verb', 'a': 'adj', 's': 'adj', 'r': 'adv'}\n",
        "    main_pos = pos_map.get(syn.pos(), 'other')\n",
        "    taboo = generate_taboo_words(syn, word)\n",
        "    # LLMè¡¥è¶³\n",
        "    if len(taboo) < 5 and api_key:\n",
        "        llm_taboos = generate_llm_taboo_words(\n",
        "            word, taboo, main_pos, syn.definition(), api_key\n",
        "        )\n",
        "        taboo.extend(llm_taboos)\n",
        "    taboo = taboo[:5]\n",
        "    entry = {\n",
        "        'target': word,\n",
        "        'part_of_speech': main_pos,\n",
        "        'taboo': taboo,\n",
        "        'category': 'general',\n",
        "        'senses': [{\n",
        "            'name': syn.name(),\n",
        "            'pos': syn.pos(),\n",
        "            'definition': syn.definition(),\n",
        "            'examples': syn.examples()\n",
        "        }],\n",
        "        'metadata': {\n",
        "            'sense_count': len(synsets),\n",
        "            'concreteness_score': None,\n",
        "            'taboo_count': len(taboo)\n",
        "        }\n",
        "    }\n",
        "    return entry\n",
        "\n",
        "# 1. å…ˆä»ç°æœ‰æ•°æ®é›†ä¸­æŠ½å–ï¼ˆç›´æ¥ç”¨åŸtabooå­—æ®µï¼‰\n",
        "for pos, pos_keys in pos_targets.items():\n",
        "    sub = df[df['part_of_speech'].isin(pos_keys)]\n",
        "    selected = sub.sample(n=min(20, len(sub)), random_state=42) if len(sub) >= 20 else sub\n",
        "    for _, row in selected.iterrows():\n",
        "        quick80.append(dict(row))\n",
        "        used_targets.add(row['target'])\n",
        "        from_300[pos].append(row['target'])\n",
        "\n",
        "# 2. å¦‚æœæŸè¯æ€§ä¸è¶³20ä¸ªï¼Œä»WordNetè¡¥è¶³ï¼ˆç”¨å¢å¼ºç®—æ³•+LLMè¡¥è¶³ï¼‰\n",
        "for pos in pos_targets:\n",
        "    needed = 20 - sum(1 for e in quick80 if e['part_of_speech'] in pos_targets[pos])\n",
        "    if needed > 0:\n",
        "        wn_words = [w for w in wn.words() if w not in used_targets and '_' not in w]\n",
        "        random.shuffle(wn_words)\n",
        "        count = 0\n",
        "        for w in wn_words:\n",
        "            entry = build_entry_from_wordnet(w, pos)\n",
        "            if entry:\n",
        "                quick80.append(entry)\n",
        "                used_targets.add(w)\n",
        "                from_wn[pos].append(w)\n",
        "                count += 1\n",
        "            if count >= needed:\n",
        "                break\n",
        "\n",
        "# 3. ä¿å­˜\n",
        "with open('quick80_dataset.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(quick80, f, indent=2, ensure_ascii=False)\n",
        "print(f\"quick80æ•°æ®é›†å·²ç”Ÿæˆï¼Œå…±{len(quick80)}ä¸ªè¯ã€‚å·²ä¿å­˜ä¸ºquick80_dataset.json\")\n",
        "\n",
        "# 4. è¾“å‡ºæ¯ä¸ªè¯æ€§æ¥æº\n",
        "for pos in pos_targets:\n",
        "    print(f\"\\nã€{pos}ã€‘\")\n",
        "    print(f\"  ä»300è¯æ•°æ®é›†æŠ½å–ï¼ˆ{len(from_300[pos])}ï¼‰ï¼š\")\n",
        "    print(\"   \", ', '.join(from_300[pos]))\n",
        "    print(f\"  ä»WordNetè¡¥å……ï¼ˆ{len(from_wn[pos])}ï¼‰ï¼š\")\n",
        "    print(\"   \", ', '.join(from_wn[pos]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== å¼€å§‹ç­›é€‰æ–°å¢æ ·æœ¬ ===\n",
            "âœ… æˆåŠŸåŠ è½½ 300 ä¸ªè¯ä½œä¸ºå‚ç…§åŸºå‡†ã€‚\n",
            "âœ… æˆåŠŸåŠ è½½ 80 ä¸ªè¯ç”¨äºç­›é€‰ã€‚\n",
            "ğŸ“Š ç­›é€‰å®Œæˆï¼å‘ç° 28 ä¸ªä» WordNet æ–°å¢çš„æ ·æœ¬ã€‚\n",
            "ğŸ† æˆåŠŸï¼æ–°æ•°æ®é›†å·²ä¿å­˜ä¸º: 'quick80_from_wordnet_only.json'\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def create_wordnet_only_dataset():\n",
        "    \"\"\"\n",
        "    è¿‡æ»¤ quick80_dataset.jsonï¼Œåªä¿ç•™ä» WordNet æ–°å¢çš„è¯æ±‡ã€‚\n",
        "    \"\"\"\n",
        "    # å®šä¹‰æ–‡ä»¶è·¯å¾„\n",
        "    enhanced_dataset_path = 'data/dataset.json'\n",
        "    quick80_dataset_path = 'quick80_dataset.json'\n",
        "    output_path = 'quick80_from_wordnet_only.json'\n",
        "\n",
        "    # æ£€æŸ¥æ‰€éœ€æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
        "    if not os.path.exists(enhanced_dataset_path):\n",
        "        print(f\"âŒ é”™è¯¯: æºæ•°æ®é›† '{enhanced_dataset_path}' æœªæ‰¾åˆ°ã€‚æ— æ³•è¿›è¡Œæ¯”è¾ƒã€‚\")\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(quick80_dataset_path):\n",
        "        print(f\"âŒ é”™è¯¯: æ•°æ®é›† '{quick80_dataset_path}' æœªæ‰¾åˆ°ã€‚æ²¡æœ‰å¯ä¾›ç­›é€‰çš„æ•°æ®ã€‚\")\n",
        "        return\n",
        "\n",
        "    print(\"=== å¼€å§‹ç­›é€‰æ–°å¢æ ·æœ¬ ===\")\n",
        "\n",
        "    # 1. åŠ è½½300è¯æ•°æ®é›†ï¼Œå¹¶æå–æ‰€æœ‰ç›®æ ‡è¯ä½œä¸ºâ€œæ—§è¯â€åˆ—è¡¨\n",
        "    try:\n",
        "        with open(enhanced_dataset_path, 'r', encoding='utf-8') as f:\n",
        "            enhanced_dataset = json.load(f)\n",
        "        words_from_300 = {entry['target'] for entry in enhanced_dataset}\n",
        "        print(f\"âœ… æˆåŠŸåŠ è½½ {len(words_from_300)} ä¸ªè¯ä½œä¸ºå‚ç…§åŸºå‡†ã€‚\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ åŠ è½½ '{enhanced_dataset_path}' æ—¶å‡ºé”™: {e}\")\n",
        "        return\n",
        "\n",
        "    # 2. åŠ è½½ quick80 æ•°æ®é›†\n",
        "    try:\n",
        "        with open(quick80_dataset_path, 'r', encoding='utf-8') as f:\n",
        "            quick80_dataset = json.load(f)\n",
        "        print(f\"âœ… æˆåŠŸåŠ è½½ {len(quick80_dataset)} ä¸ªè¯ç”¨äºç­›é€‰ã€‚\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ åŠ è½½ '{quick80_dataset_path}' æ—¶å‡ºé”™: {e}\")\n",
        "        return\n",
        "\n",
        "    # 3. ç­›é€‰å‡ºæ‰€æœ‰â€œæ–°è¯â€\n",
        "    new_samples_from_wordnet = []\n",
        "    for entry in quick80_dataset:\n",
        "        if entry.get('target') not in words_from_300:\n",
        "            new_samples_from_wordnet.append(entry)\n",
        "    \n",
        "    print(f\"ğŸ“Š ç­›é€‰å®Œæˆï¼å‘ç° {len(new_samples_from_wordnet)} ä¸ªä» WordNet æ–°å¢çš„æ ·æœ¬ã€‚\")\n",
        "\n",
        "    # 4. ä¿å­˜æ–°æ•°æ®é›†\n",
        "    try:\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(new_samples_from_wordnet, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"ğŸ† æˆåŠŸï¼æ–°æ•°æ®é›†å·²ä¿å­˜ä¸º: '{output_path}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ä¿å­˜æ–°æ•°æ®é›†æ—¶å‡ºé”™: {e}\")\n",
        "\n",
        "# --- ç›´æ¥è¿è¡Œå‡½æ•° ---\n",
        "create_wordnet_only_dataset()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
