{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 20:09:45,592 - INFO - 所有Python库已安装。\n",
      "2025-06-11 20:09:45,594 - WARNING - Hugging Face 认证失败: The `notebook_login` function can only be used in a notebook (Jupyter or Colab) and you need the `ipywidgets` module: `pip install ipywidgets`.\n",
      "2025-06-11 20:09:45,595 - INFO - 您可以稍后手动登录或跳过需要认证的模型。\n",
      "2025-06-11 20:09:45,610 - INFO - 成功加载数据集，包含 10000 条记录从 ./data/wordnet_dataset.json\n",
      "2025-06-11 20:09:45,630 - INFO - 开始实验: taboo_experiment\n",
      "2025-06-11 20:09:45,630 - INFO - 参数: {'experiment_name': 'taboo_experiment', 'temperature': 0.7, 'max_turns': 5, 'hint_len': 15, 'domain': 'general', 'max_reprompt_attempts': 3, 'max_new_tokens': 50, 'do_sample': True, 'top_p': 0.8, 'top_k': 40, 'num_games_per_pair': 3, 'sample_dataset': True, 'output_dir': './results'}\n",
      "2025-06-11 20:09:45,630 - INFO - 输出目录: results/taboo_experiment_20250611_200945\n",
      "2025-06-11 20:09:45,632 - INFO - 处理模型对 1/1: microsoft/DialoGPT-medium -> microsoft/DialoGPT-medium\n",
      "2025-06-11 20:09:45,632 - INFO - 加载 HuggingFace 模型: microsoft/DialoGPT-medium\n",
      "2025-06-11 20:09:45,632 - INFO - 未检测到 GPU。将在CPU上运行。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集已加载: 10000 条记录\n",
      "第一个例子:\n",
      "目标词: Guaiacum\n",
      "禁忌词: ['guaiac', 'thing', 'object', 'item', 'entity']\n",
      "类别: general\n",
      "定义: small genus of evergreen resinous trees or shrubs of warm and tropical America\n",
      "Hugging Face 本地模型列表已定义。\n",
      "实验中可用模型: ['microsoft/DialoGPT-medium']\n",
      "实验配置已加载:\n",
      "温度: 0.3\n",
      "最大回合数: 5\n",
      "最大提示长度: 20 tokens\n",
      "领域: general\n",
      "最大重试提示次数: 3\n",
      "游戏验证函数已加载成功\n",
      "TabooGame 类已加载成功\n",
      "游戏方法实现已完成\n",
      "实验参数已设置，运行以下命令开始实验:\n",
      "result_dir, stats = run_experiment_local()\n",
      "\n",
      "当前参数:\n",
      "  experiment_name: taboo_experiment\n",
      "  temperature: 0.7\n",
      "  max_turns: 5\n",
      "  hint_len: 15\n",
      "  domain: general\n",
      "  max_reprompt_attempts: 3\n",
      "  max_new_tokens: 50\n",
      "  do_sample: True\n",
      "  top_p: 0.8\n",
      "  top_k: 40\n",
      "  num_games_per_pair: 3\n",
      "  sample_dataset: True\n",
      "  output_dir: ./results\n",
      "\n",
      "修改参数示例:\n",
      "EXPERIMENT_PARAMS['num_games_per_pair'] = 10\n",
      "EXPERIMENT_PARAMS['temperature'] = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2025-06-11 20:10:22,347 - INFO - 模型加载成功，使用的设备: mps:0\n",
      "2025-06-11 20:10:22,349 - INFO - 开始游戏 - 目标词: molecule, Hinter: microsoft/DialoGPT-medium, Guesser: microsoft/DialoGPT-medium\n",
      "2025-06-11 20:10:22,349 - INFO - 开始游戏 - 目标词: acerate, Hinter: microsoft/DialoGPT-medium, Guesser: microsoft/DialoGPT-medium\n",
      "2025-06-11 20:10:22,350 - INFO - 正在玩回合 1\n",
      "2025-06-11 20:10:22,351 - INFO - 正在玩回合 1\n",
      "2025-06-11 20:10:22,351 - INFO - 正在玩回合 1\n",
      "2025-06-11 20:10:22,352 - INFO - 正在玩回合 1\n",
      "2025-06-11 20:10:24,436 - WARNING - 提示格式无效，尝试 1: You\n",
      "2025-06-11 20:10:24,816 - WARNING - 提示格式无效，尝试 2: Thanks\n",
      "2025-06-11 20:10:25,050 - WARNING - 提示格式无效，尝试 1: I'mature '\n",
      "2025-06-11 20:10:25,375 - WARNING - 提示格式无效，尝试 3: Your clues\n",
      "2025-06-11 20:10:25,376 - INFO - 游戏失败，回合 1 格式问题\n",
      "2025-06-11 20:10:25,376 - INFO - 开始游戏 - 目标词: disservice, Hinter: microsoft/DialoGPT-medium, Guesser: microsoft/DialoGPT-medium\n",
      "2025-06-11 20:10:25,376 - INFO -   已完成游戏 1/3: acerate\n",
      "2025-06-11 20:10:25,377 - INFO - 正在玩回合 1\n",
      "2025-06-11 20:10:25,377 - INFO - 正在玩回合 1\n",
      "2025-06-11 20:10:25,662 - WARNING - 提示格式无效，尝试 1: I\n",
      "2025-06-11 20:10:25,662 - WARNING - 提示格式无效，尝试 2: You've been\n",
      "2025-06-11 20:10:26,092 - WARNING - 提示格式无效，尝试 2: You\n",
      "2025-06-11 20:10:26,398 - WARNING - 提示格式无效，尝试 3: I'mt\n",
      "2025-06-11 20:10:26,399 - INFO - 游戏失败，回合 1 格式问题\n",
      "2025-06-11 20:10:26,399 - INFO -   已完成游戏 2/3: molecule\n",
      "2025-06-11 20:10:26,450 - WARNING - 提示格式无效，尝试 3: We're\n",
      "2025-06-11 20:10:26,450 - INFO - 游戏失败，回合 1 格式问题\n",
      "2025-06-11 20:10:26,451 - INFO -   已完成游戏 3/3: disservice\n",
      "2025-06-11 20:10:26,451 - INFO - 保存实验结果到CSV文件...\n",
      "2025-06-11 20:10:26,458 - INFO - 游戏汇总保存到: results/taboo_experiment_20250611_200945/taboo_experiment_game_summary.csv\n",
      "2025-06-11 20:10:26,460 - INFO - 对话数据保存到: results/taboo_experiment_20250611_200945/taboo_experiment_conversations.csv\n",
      "2025-06-11 20:10:26,460 - INFO - 实验完成!\n",
      "2025-06-11 20:10:26,460 - INFO - 总计: 3 局游戏, 0 局成功 (0.00% 成功率)\n",
      "2025-06-11 20:10:26,460 - INFO - 对话记录: 6 条\n",
      "2025-06-11 20:10:26,461 - INFO - 结果保存在: results/taboo_experiment_20250611_200945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comprehensive experiment results analysis...\n",
      "\n",
      "游戏汇总分析:\n",
      "                                   run_id               hinter_model  \\\n",
      "0  DialoGPT_DialoGPT_20250611_201022_6437  microsoft/DialoGPT-medium   \n",
      "1  DialoGPT_DialoGPT_20250611_201022_2410  microsoft/DialoGPT-medium   \n",
      "2  DialoGPT_DialoGPT_20250611_201025_6918  microsoft/DialoGPT-medium   \n",
      "\n",
      "               guesser_model  temperature   domain target_word  \\\n",
      "0  microsoft/DialoGPT-medium          0.7  general     acerate   \n",
      "1  microsoft/DialoGPT-medium          0.7  general    molecule   \n",
      "2  microsoft/DialoGPT-medium          0.7  general  disservice   \n",
      "\n",
      "                                      taboo_words  success  turn_count  \\\n",
      "0       acerose|acicular|simple|unsubdivided|word    False           1   \n",
      "1  coenzyme|supermolecule|macromolecule|unit|EDTA    False           1   \n",
      "2              service|badly|help|intended|injury    False           1   \n",
      "\n",
      "   total_tokens  error  \n",
      "0             0    NaN  \n",
      "1             0    NaN  \n",
      "2             0    NaN  \n",
      "\n",
      "对话分析:\n",
      "                                   run_id  turn_id     role  \\\n",
      "0  DialoGPT_DialoGPT_20250611_201022_6437        1   hinter   \n",
      "1  DialoGPT_DialoGPT_20250611_201022_6437        1  guesser   \n",
      "2  DialoGPT_DialoGPT_20250611_201022_2410        1   hinter   \n",
      "3  DialoGPT_DialoGPT_20250611_201022_2410        1  guesser   \n",
      "4  DialoGPT_DialoGPT_20250611_201025_6918        1   hinter   \n",
      "\n",
      "                       model  \\\n",
      "0  microsoft/DialoGPT-medium   \n",
      "1  microsoft/DialoGPT-medium   \n",
      "2  microsoft/DialoGPT-medium   \n",
      "3  microsoft/DialoGPT-medium   \n",
      "4  microsoft/DialoGPT-medium   \n",
      "\n",
      "                                              prompt  output  content  tokens  \\\n",
      "0  System: You are playing a Taboo word game as t...     NaN      NaN       0   \n",
      "1                                                NaN     NaN      NaN       0   \n",
      "2  System: You are playing a Taboo word game as t...     NaN      NaN       0   \n",
      "3                                                NaN     NaN      NaN       0   \n",
      "4  System: You are playing a Taboo word game as t...     NaN      NaN       0   \n",
      "\n",
      "   format_ok  violate_taboo target_word  \n",
      "0      False          False     acerate  \n",
      "1      False          False     acerate  \n",
      "2      False          False    molecule  \n",
      "3      False          False    molecule  \n",
      "4      False          False  disservice  \n",
      "正在创建综合饼图分析...\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'labels' must be of length 'x', not 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1336\u001b[39m\n\u001b[32m   1333\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m summary_df\n\u001b[32m   1335\u001b[39m \u001b[38;5;66;03m# Run pie chart analysis\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m pie_summary = \u001b[43mcreate_pie_chart_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1179\u001b[39m, in \u001b[36mcreate_pie_chart_analysis\u001b[39m\u001b[34m(result_dir_path)\u001b[39m\n\u001b[32m   1177\u001b[39m success_counts = summary_df[\u001b[33m'\u001b[39m\u001b[33msuccess\u001b[39m\u001b[33m'\u001b[39m].value_counts()\n\u001b[32m   1178\u001b[39m success_labels = [\u001b[33m'\u001b[39m\u001b[33m成功游戏\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m失败游戏\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m-> \u001b[39m\u001b[32m1179\u001b[39m \u001b[43mplt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpie\u001b[49m\u001b[43m(\u001b[49m\u001b[43msuccess_counts\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuccess_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautopct\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m%1.1f\u001b[39;49;00m\u001b[38;5;132;43;01m%%\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m       \u001b[49m\u001b[43mcolors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolors_success\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstartangle\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m90\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1181\u001b[39m plt.title(\u001b[33m'\u001b[39m\u001b[33m整体游戏结果分布\u001b[39m\u001b[33m'\u001b[39m, fontsize=\u001b[32m16\u001b[39m, fontweight=\u001b[33m'\u001b[39m\u001b[33mbold\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   1183\u001b[39m \u001b[38;5;66;03m# 2. 作为Hinter的模型性能 - 获胜分布\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/matplotlib/_api/deprecation.py:453\u001b[39m, in \u001b[36mmake_keyword_only.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > name_idx:\n\u001b[32m    448\u001b[39m     warn_deprecated(\n\u001b[32m    449\u001b[39m         since, message=\u001b[33m\"\u001b[39m\u001b[33mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    450\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[33m; the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    451\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mparameter will become keyword-only in \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    452\u001b[39m         name=name, obj_type=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m453\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/matplotlib/pyplot.py:3795\u001b[39m, in \u001b[36mpie\u001b[39m\u001b[34m(x, explode, labels, colors, autopct, pctdistance, shadow, labeldistance, startangle, radius, counterclock, wedgeprops, textprops, center, frame, rotatelabels, normalize, hatch, data)\u001b[39m\n\u001b[32m   3772\u001b[39m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes.pie)\n\u001b[32m   3773\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpie\u001b[39m(\n\u001b[32m   3774\u001b[39m     x: ArrayLike,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3793\u001b[39m     data=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   3794\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Wedge], \u001b[38;5;28mlist\u001b[39m[Text]] | \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Wedge], \u001b[38;5;28mlist\u001b[39m[Text], \u001b[38;5;28mlist\u001b[39m[Text]]:\n\u001b[32m-> \u001b[39m\u001b[32m3795\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpie\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3796\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3797\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexplode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexplode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3798\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3799\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3800\u001b[39m \u001b[43m        \u001b[49m\u001b[43mautopct\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautopct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3801\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpctdistance\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpctdistance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3802\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshadow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshadow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3803\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabeldistance\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabeldistance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3804\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstartangle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstartangle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3805\u001b[39m \u001b[43m        \u001b[49m\u001b[43mradius\u001b[49m\u001b[43m=\u001b[49m\u001b[43mradius\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcounterclock\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcounterclock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3807\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwedgeprops\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwedgeprops\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3808\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtextprops\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtextprops\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3809\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcenter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3810\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3811\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrotatelabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrotatelabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3812\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3814\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3815\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/matplotlib/_api/deprecation.py:453\u001b[39m, in \u001b[36mmake_keyword_only.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > name_idx:\n\u001b[32m    448\u001b[39m     warn_deprecated(\n\u001b[32m    449\u001b[39m         since, message=\u001b[33m\"\u001b[39m\u001b[33mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    450\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[33m; the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    451\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mparameter will become keyword-only in \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    452\u001b[39m         name=name, obj_type=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m453\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/matplotlib/__init__.py:1521\u001b[39m, in \u001b[36m_preprocess_data.<locals>.inner\u001b[39m\u001b[34m(ax, data, *args, **kwargs)\u001b[39m\n\u001b[32m   1518\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m   1519\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(ax, *args, data=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m   1520\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1521\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1523\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1524\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1526\u001b[39m     bound = new_sig.bind(ax, *args, **kwargs)\n\u001b[32m   1527\u001b[39m     auto_label = (bound.arguments.get(label_namer)\n\u001b[32m   1528\u001b[39m                   \u001b[38;5;129;01mor\u001b[39;00m bound.kwargs.get(label_namer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/matplotlib/axes/_axes.py:3373\u001b[39m, in \u001b[36mAxes.pie\u001b[39m\u001b[34m(self, x, explode, labels, colors, autopct, pctdistance, shadow, labeldistance, startangle, radius, counterclock, wedgeprops, textprops, center, frame, rotatelabels, normalize, hatch)\u001b[39m\n\u001b[32m   3371\u001b[39m     explode = [\u001b[32m0\u001b[39m] * \u001b[38;5;28mlen\u001b[39m(x)\n\u001b[32m   3372\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) != \u001b[38;5;28mlen\u001b[39m(labels):\n\u001b[32m-> \u001b[39m\u001b[32m3373\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m\u001b[33m must be of length \u001b[39m\u001b[33m'\u001b[39m\u001b[33mx\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(labels)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   3374\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) != \u001b[38;5;28mlen\u001b[39m(explode):\n\u001b[32m   3375\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mexplode\u001b[39m\u001b[33m'\u001b[39m\u001b[33m must be of length \u001b[39m\u001b[33m'\u001b[39m\u001b[33mx\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(explode)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: 'labels' must be of length 'x', not 2"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgYAAAH5CAYAAAD6E/bxAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAGo5JREFUeJzt3XuMFfX5+PGHi4CmgloKCEWpWm+1goIgIjE2VBINlj+aUjVCiZdarbGQVsALeKlivYWkokTUatJasEasEYJVlBgrDREk0VYwirrUyK0WFlEXhPPNzO+3T1lckIXdZcHXK5nAmZ3ZM2dcmffOzOecVpVKpRIAABHRem9vAADQcggDACAJAwAgCQMAIAkDACAJAwAgCQMAIAkDACAJAwAgCQMAYPfD4OWXX45hw4ZF9+7do1WrVvH0009/5Trz58+PU089Ndq3bx/HHHNMPProow19WgCgJYbBxo0bo3fv3jF16tRdWv69996L8847L84+++xYsmRJ/OpXv4pLL700nnvuud3ZXgCgCbXakw9RKs4YzJo1K4YPH77DZcaNGxezZ8+ON998M+f99Kc/jXXr1sXcuXN396kBgCbQNprYggULYsiQIXXmDR06tDxzsCM1NTXlVGvr1q3x8ccfxze/+c0yRgCAiOJ3+w0bNpSX91u3br1vhMHKlSuja9eudeYVj6urq+Ozzz6LAw888EvrTJ48OW6++eam3jQA2C+sWLEivv3tb+8bYbA7JkyYEGPHjs3H69evjyOOOKJ84R07dtyr2wYALUXxS3bPnj3j4IMPbrTv2eRh0K1bt1i1alWdecXj4gBf39mCQjF6oZi2V6wjDACgrsa8zN7k72MwcODAmDdvXp15zz//fDkfAGhZGhwGn3zySTnssJhqhyMWf6+qqsrLACNHjszlr7jiili+fHlce+21sXTp0rj//vvjiSeeiDFjxjTm6wAA9kYYvPbaa3HKKaeUU6G4F6D4+8SJE8vHH330UUZC4Tvf+U45XLE4S1C8/8E999wTDz30UDkyAQDYj97HoDlvrujUqVN5E6J7DACg6Y6PPisBAEjCAABIwgAASMIAAEjCAABIwgAASMIAAEjCAABIwgAASMIAAEjCAABIwgAASMIAAEjCAABIwgAASMIAAEjCAABIwgAASMIAAEjCAABIwgAASMIAAEjCAABIwgAASMIAAEjCAABIwgAASMIAAEjCAABIwgAASMIAAEjCAABIwgAASMIAAEjCAABIwgAASMIAAEjCAABIwgAASMIAAEjCAABIwgAASMIAAEjCAABIwgAASMIAAEjCAABIwgAASMIAAEjCAABIwgAASMIAAEjCAABIwgAASMIAAEjCAABIwgAASMIAAEjCAABIwgAASMIAAEjCAABIwgAASMIAAEjCAABIwgAASMIAAEjCAABIwgAASMIAAEjCAABIwgAASMIAAEjCAABIwgAASMIAAEjCAABIwgAASMIAAEjCAADYszCYOnVq9OrVKzp06BADBgyIhQsX7nT5KVOmxHHHHRcHHnhg9OzZM8aMGROff/757jw1ANCSwmDmzJkxduzYmDRpUixevDh69+4dQ4cOjdWrV9e7/OOPPx7jx48vl3/rrbfi4YcfLr/Hdddd1xjbDwDszTC4995747LLLovRo0fHiSeeGNOmTYuDDjooHnnkkXqXf/XVV2PQoEFx4YUXlmcZzjnnnLjgggu+8iwDANDCw2DTpk2xaNGiGDJkyP++QevW5eMFCxbUu84ZZ5xRrlMbAsuXL485c+bEueeeu6fbDgA0srYNWXjt2rWxZcuW6Nq1a535xeOlS5fWu05xpqBY78wzz4xKpRJffPFFXHHFFTu9lFBTU1NOtaqrqxuymQBASx2VMH/+/Lj99tvj/vvvL+9JeOqpp2L27Nlx66237nCdyZMnR6dOnXIqblgEAJpeq0rxa3wDLiUU9xM8+eSTMXz48Jw/atSoWLduXfz1r3/90jqDBw+O008/Pe66666c98c//jEuv/zy+OSTT8pLEbtyxqCIg/Xr10fHjh0b+hoBYL9UXV1d/gLdmMfHBp0xaNeuXfTt2zfmzZuX87Zu3Vo+HjhwYL3rfPrpp186+Ldp06b8c0dN0r59+/IFbjsBAC3sHoNCMVSxOEPQr1+/6N+/f/keBRs3bixHKRRGjhwZPXr0KC8HFIYNG1aOZDjllFPK9zx455134sYbbyzn1wYCALCPhsGIESNizZo1MXHixFi5cmX06dMn5s6dmzckVlVV1TlDcMMNN0SrVq3KPz/88MP41re+VUbBbbfd1rivBABo3nsM9qdrKACwr6ve2/cYAAD7N2EAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAAOxZGEydOjV69eoVHTp0iAEDBsTChQt3uvy6deviqquuisMPPzzat28fxx57bMyZM2d3nhoAaEJtG7rCzJkzY+zYsTFt2rQyCqZMmRJDhw6NZcuWRZcuXb60/KZNm+KHP/xh+bUnn3wyevToER988EEccsghjfUaAIBG0qpSqVQaskIRA6eddlrcd9995eOtW7dGz5494+qrr47x48d/afkiIO66665YunRpHHDAAbu1kdXV1dGpU6dYv359dOzYcbe+BwDsb6qb4PjYoEsJxW//ixYtiiFDhvzvG7RuXT5esGBBves888wzMXDgwPJSQteuXeOkk06K22+/PbZs2bLnWw8A7L1LCWvXri0P6MUBflvF4+KMQH2WL18eL774Ylx00UXlfQXvvPNOXHnllbF58+aYNGlSvevU1NSU07ZFBADsB6MSiksNxf0FDz74YPTt2zdGjBgR119/fXmJYUcmT55cnhqpnYpLFQBACwuDzp07R5s2bWLVqlV15hePu3XrVu86xUiEYhRCsV6tE044IVauXFlemqjPhAkTyusltdOKFSsaspkAQHOEQbt27crf+ufNm1fnjEDxuLiPoD6DBg0qLx8Uy9V6++23y2Aovl99iiGNxU0U204AQAu8lFAMVZw+fXo89thj8dZbb8UvfvGL2LhxY4wePbr8+siRI8vf+GsVX//444/jmmuuKYNg9uzZ5c2Hxc2IAMA+/j4GxT0Ca9asiYkTJ5aXA/r06RNz587NGxKrqqrKkQq1ivsDnnvuuRgzZkycfPLJ5fsYFJEwbty4xn0lAEDzv4/B3uB9DACgBb6PAQCwfxMGAEASBgBAEgYAQBIGAEASBgBAEgYAQBIGAEASBgBAEgYAQBIGAEASBgBAEgYAQBIGAEASBgBAEgYAQBIGAEASBgBAEgYAQBIGAEASBgBAEgYAQBIGAEASBgBAEgYAQBIGAEASBgBAEgYAQBIGAEASBgBAEgYAQBIGAEASBgBAEgYAQBIGAEASBgBAEgYAQBIGAEASBgBAEgYAQBIGAEASBgBAEgYAQBIGAEASBgBAEgYAQBIGAEASBgBAEgYAQBIGAEASBgBAEgYAQBIGAEASBgBAEgYAQBIGAEASBgBAEgYAQBIGAEASBgBAEgYAQBIGAEASBgBAEgYAQBIGAEASBgBAEgYAQBIGAEASBgBAEgYAQBIGAEASBgBAEgYAQBIGAEASBgBAEgYAQBIGAEASBgBAEgYAQBIGAEASBgDAnoXB1KlTo1evXtGhQ4cYMGBALFy4cJfWmzFjRrRq1SqGDx++O08LALS0MJg5c2aMHTs2Jk2aFIsXL47evXvH0KFDY/Xq1Ttd7/33349f//rXMXjw4D3ZXgCgJYXBvffeG5dddlmMHj06TjzxxJg2bVocdNBB8cgjj+xwnS1btsRFF10UN998cxx11FF7us0AQEsIg02bNsWiRYtiyJAh//sGrVuXjxcsWLDD9W655Zbo0qVLXHLJJbv0PDU1NVFdXV1nAgBaWBisXbu2/O2/a9eudeYXj1euXFnvOq+88ko8/PDDMX369F1+nsmTJ0enTp1y6tmzZ0M2EwBoiaMSNmzYEBdffHEZBZ07d97l9SZMmBDr16/PacWKFU25mQDA/9c2GqA4uLdp0yZWrVpVZ37xuFu3bl9a/t133y1vOhw2bFjO27p16/974rZtY9myZXH00Ud/ab327duXEwDQgs8YtGvXLvr27Rvz5s2rc6AvHg8cOPBLyx9//PHxxhtvxJIlS3I6//zz4+yzzy7/7hIBAOzDZwwKxVDFUaNGRb9+/aJ///4xZcqU2LhxYzlKoTBy5Mjo0aNHeZ9A8T4HJ510Up31DznkkPLP7ecDAPtgGIwYMSLWrFkTEydOLG847NOnT8ydOzdvSKyqqipHKgAA+55WlUqlEi1cMVyxGJ1Q3IjYsWPHvb05ALDfHh/9ag8AJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAAOxZGEydOjV69eoVHTp0iAEDBsTChQt3uOz06dNj8ODBceihh5bTkCFDdro8ALAPhcHMmTNj7NixMWnSpFi8eHH07t07hg4dGqtXr653+fnz58cFF1wQL730UixYsCB69uwZ55xzTnz44YeNsf0AQCNqValUKg1ZoThDcNppp8V9991XPt66dWt5sL/66qtj/PjxX7n+li1byjMHxfojR47cpeesrq6OTp06xfr166Njx44N2VwA2G9VN8HxsUFnDDZt2hSLFi0qLwfkN2jdunxcnA3YFZ9++mls3rw5DjvssB0uU1NTU77YbScAoOk1KAzWrl1b/sbftWvXOvOLxytXrtyl7zFu3Ljo3r17nbjY3uTJk8sCqp2KMxIAwH42KuGOO+6IGTNmxKxZs8obF3dkwoQJ5WmR2mnFihXNuZkA8LXVtiELd+7cOdq0aROrVq2qM7943K1bt52ue/fdd5dh8MILL8TJJ5+802Xbt29fTgBACz5j0K5du+jbt2/Mmzcv5xU3HxaPBw4cuMP17rzzzrj11ltj7ty50a9fvz3bYgCgZZwxKBRDFUeNGlUe4Pv37x9TpkyJjRs3xujRo8uvFyMNevToUd4nUPjd734XEydOjMcff7x874PaexG+8Y1vlBMAsA+HwYgRI2LNmjXlwb44yPfp06c8E1B7Q2JVVVU5UqHWAw88UI5m+PGPf1zn+xTvg3DTTTc1xmsAAPbW+xjsDd7HAABa4PsYAAD7N2EAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAACRhAAAkYQAAJGEAAOxZGEydOjV69eoVHTp0iAEDBsTChQt3uvxf/vKXOP7448vlv//978ecOXN252kBgJYWBjNnzoyxY8fGpEmTYvHixdG7d+8YOnRorF69ut7lX3311bjgggvikksuiddffz2GDx9eTm+++WZjbD8A0IhaVSqVSkNWKM4QnHbaaXHfffeVj7du3Ro9e/aMq6++OsaPH/+l5UeMGBEbN26MZ599Nuedfvrp0adPn5g2bdouPWd1dXV06tQp1q9fHx07dmzI5gLAfqu6CY6PbRuy8KZNm2LRokUxYcKEnNe6desYMmRILFiwoN51ivnFGYZtFWcYnn766R0+T01NTTnVKl5w7Q4AAKLOcbGBv+M3XhisXbs2tmzZEl27dq0zv3i8dOnSetdZuXJlvcsX83dk8uTJcfPNN39pfnFmAgCo6z//+U955qDZw6C5FGcktj3LsG7dujjyyCOjqqqq0V44O67PIsBWrFjhsk0Ts6+bj33dfOzr5lWcUT/iiCPisMMOa7Tv2aAw6Ny5c7Rp0yZWrVpVZ37xuFu3bvWuU8xvyPKF9u3bl9P2iijwg9Y8iv1sXzcP+7r52NfNx75uXsVl/Ub7Xg1ZuF27dtG3b9+YN29ezituPiweDxw4sN51ivnbLl94/vnnd7g8ALD3NPhSQnGKf9SoUdGvX7/o379/TJkypRx1MHr06PLrI0eOjB49epT3CRSuueaaOOuss+Kee+6J8847L2bMmBGvvfZaPPjgg43/agCA5g2DYvjhmjVrYuLEieUNhMWww7lz5+YNhsV9ANue0jjjjDPi8ccfjxtuuCGuu+66+O53v1uOSDjppJN2+TmLywrF+ybUd3mBxmVfNx/7uvnY183Hvt7393eD38cAANh/+awEACAJAwAgCQMAIAkDAKDlhYGPcm6Z+3r69OkxePDgOPTQQ8up+FyMr/pvw+7/XNcqhvW2atWq/CRSmmZfF++oetVVV8Xhhx9e3tF97LHH+nekifZ1Maz9uOOOiwMPPLB8V8QxY8bE559/3mzbu696+eWXY9iwYdG9e/fy34OdfcZQrfnz58epp55a/kwfc8wx8eijjzb8iSstwIwZMyrt2rWrPPLII5V//vOflcsuu6xyyCGHVFatWlXv8n//+98rbdq0qdx5552Vf/3rX5UbbrihcsABB1TeeOONZt/2fU1D9/WFF15YmTp1auX111+vvPXWW5Wf/exnlU6dOlX+/e9/N/u27+/7utZ7771X6dGjR2Xw4MGVH/3oR822vV+nfV1TU1Pp169f5dxzz6288sor5T6fP39+ZcmSJc2+7fv7vv7Tn/5Uad++fflnsZ+fe+65yuGHH14ZM2ZMs2/7vmbOnDmV66+/vvLUU08Vowcrs2bN2unyy5cvrxx00EGVsWPHlsfG3//+9+Wxcu7cuQ163hYRBv37969cddVV+XjLli2V7t27VyZPnlzv8j/5yU8q5513Xp15AwYMqPz85z9v8m3d1zV0X2/viy++qBx88MGVxx57rAm38uu7r4v9e8YZZ1QeeuihyqhRo4RBE+3rBx54oHLUUUdVNm3a1Ixb+fXc18WyP/jBD+rMKw5cgwYNavJt3Z/ELoTBtddeW/ne975XZ96IESMqQ4cObdBz7fVLCbUf5Vycom7IRzlvu3ztRznvaHl2f19v79NPP43Nmzc36gd27I92d1/fcsst0aVLl7jkkkuaaUu/nvv6mWeeKd+WvbiUULw5W/GGa7fffnv56bE07r4u3uSuWKf2csPy5cvLSzbnnntus23318WCRjo27vVPV2yuj3Jm9/b19saNG1de79r+h48939evvPJKPPzww7FkyZJm2sqv774uDk4vvvhiXHTRReVB6p133okrr7yyjN7iXeRovH194YUXluudeeaZxRnq+OKLL+KKK64o3wmXxrWjY2PxiZefffZZeY/HrtjrZwzYd9xxxx3lTXGzZs0qbzqi8WzYsCEuvvji8mbP4lNMaVrFh78VZ2aKz2wpPhiueKv366+/PqZNm7a3N22/U9wMV5yNuf/++2Px4sXx1FNPxezZs+PWW2/d25tGSz1j0Fwf5czu7etad999dxkGL7zwQpx88slNvKVfv3397rvvxvvvv1/egbztwavQtm3bWLZsWRx99NHNsOVfj5/rYiTCAQccUK5X64QTTih/4ypOlxefJEvj7Osbb7yxjN5LL720fFyMIis+eO/yyy8vY6wxPy74667bDo6Nxcdf7+rZgsJe/y/io5xb9r4u3HnnnWXdFx+WVXyqJo2/r4uht2+88UZ5GaF2Ov/88+Pss88u/14M8aLxfq4HDRpUXj6oja/C22+/XQaDKGjcfV3cl7T9wb82yHxUT+NqtGNjpYUMfymGszz66KPlEIvLL7+8HP6ycuXK8usXX3xxZfz48XWGK7Zt27Zy9913l0PoJk2aZLhiE+3rO+64oxya9OSTT1Y++uijnDZs2LAXX8X+ua+3Z1RC0+3rqqqqcnTNL3/5y8qyZcsqzz77bKVLly6V3/72t3vxVeyf+7r497nY13/+85/L4XR/+9vfKkcffXQ5uoydK/6dLYaKF1NxuL733nvLv3/wwQfl14v9XOzv7Ycr/uY3vymPjcVQ8312uGKhGG95xBFHlAehYjjMP/7xj/zaWWedVf4jua0nnniicuyxx5bLF8MzZs+evRe2et/UkH195JFHlj+Q20/F/+w0/s/1toRB0+7rV199tRzmXBzkiqGLt912WzlclMbd15s3b67cdNNNZQx06NCh0rNnz8qVV15Z+e9//7uXtn7f8dJLL9X772/t/i3+LPb39uv06dOn/G9T/Fz/4Q9/aPDz+thlAKDl3GMAALQcwgAASMIAAEjCAABIwgAASMIAAEjCAABIwgAASMIAAEjCAABIwgAASMIAAIha/wdA9T4GdqkbEQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 2000x2500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Taboo Game Benchmark Script - HuggingFace Version for Colab\n",
    "# 此代码应复制并粘贴到 Colab Notebook 的一个【单个代码单元格】中运行。\n",
    "# ==============================================================================\n",
    "\n",
    "# ==============================================================================\n",
    "# Section 1: 导入库和 Colab 环境设置\n",
    "# ==============================================================================\n",
    "\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from threading import Lock\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Colab specific setup: Install Python libraries ---\n",
    "# This block installs/re-installs required libraries with precise versioning\n",
    "# based on common Colab environments (PyTorch 2.5.1 + CUDA 12.4).\n",
    "# It's crucial for managing dependency conflicts that often occur in Colab.\n",
    "try:\n",
    "    import torch\n",
    "    from transformers import (\n",
    "        AutoTokenizer, AutoModelForCausalLM,\n",
    "        pipeline, set_seed # Removed BitsAndBytesConfig\n",
    "    )\n",
    "    from huggingface_hub import login\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    # Removed bitsandbytes import\n",
    "    logger.info(\"所有Python库已安装。\")\n",
    "except ImportError:\n",
    "    logger.info(\"正在安装所需Python库 (transformers, accelerate, pandas, numpy, huggingface_hub)... \")\n",
    "\n",
    "    # Precise versioning based on typical Colab environment (PyTorch 2.5.1 + CUDA 12.4)\n",
    "    TORCH_VER = \"2.5.1\"\n",
    "    CUDA_VER = \"cu124\" # Suffix for PyTorch wheels\n",
    "\n",
    "    # transformers 库的版本选择：通常，最新的transformers版本会兼容最新的PyTorch版本。\n",
    "    TRANSFORMERS_VER = \"4.41.2\" # Compatible with PyTorch 2.5.1\n",
    "    \n",
    "    # bitsandbytes is removed, so no BITSANDBYTES_VER needed for installation\n",
    "\n",
    "    # First, uninstall potentially conflicting existing versions to ensure a clean slate\n",
    "    logger.info(\"卸载可能冲突的现有库...\")\n",
    "    # Removed bitsandbytes from uninstall list\n",
    "    !pip uninstall -y torch torchvision torchaudio transformers accelerate pandas numpy huggingface_hub --quiet\n",
    "\n",
    "    # Install PyTorch and Torchvision precisely\n",
    "    logger.info(f\"精确安装 torch=={TORCH_VER}+{CUDA_VER}, torchvision and torchaudio...\")\n",
    "    !pip install -q torch=={TORCH_VER}+{CUDA_VER} torchvision==0.20.1+{CUDA_VER} torchaudio=={TORCH_VER}+{CUDA_VER} --index-url https://download.pytorch.org/whl/{CUDA_VER}\n",
    "\n",
    "    # Then install other dependencies, ensuring they use the newly installed PyTorch\n",
    "    # Removed bitsandbytes from install list\n",
    "    logger.info(f\"安装 transformers=={TRANSFORMERS_VER}, accelerate 等...\")\n",
    "    !pip install -q --no-build-isolation transformers=={TRANSFORMERS_VER} accelerate pandas numpy huggingface_hub\n",
    "\n",
    "    # Re-import essential libraries (excluding bitsandbytes from this top-level check)\n",
    "    try:\n",
    "        import torch\n",
    "        from transformers import (\n",
    "            AutoTokenizer, AutoModelForCausalLM,\n",
    "            pipeline, set_seed # Removed BitsAndBytesConfig\n",
    "        )\n",
    "        from huggingface_hub import login\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        logger.info(\"安装完成。\")\n",
    "    except ImportError as e:\n",
    "        logger.error(f\"安装后仍然无法导入核心库: {e}. 请检查Colab运行时类型是否为GPU，或尝试重置运行时并调整安装版本。\")\n",
    "        raise # If core libraries still can't be imported, terminate script\n",
    "\n",
    "# --- Hugging Face Authentication for Colab ---\n",
    "# IMPORTANT: Replace 'hf_YOUR_HUGGING_FACE_TOKEN_HERE' with your actual Hugging Face Access Token.\n",
    "# You can generate one from Hugging Face website: Settings -> Access Tokens.\n",
    "# For gated models (like Qwen/Qwen3-8B), ensure you have\n",
    "# requested and been granted access on the model page first.\n",
    "# You can also use huggingface_hub.login() without passing the token if you prefer to type it in interactively.\n",
    "# 为了安全，建议使用交互式登录而不是硬编码 token\n",
    "try:\n",
    "    login() # Interactive login - 会提示您输入 token\n",
    "    logger.info(\"Hugging Face 认证成功。\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Hugging Face 认证失败: {e}\")\n",
    "    logger.info(\"您可以稍后手动登录或跳过需要认证的模型。\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# Section 2: HuggingFaceModelClient 定义\n",
    "# ==============================================================================\n",
    "\n",
    "class HuggingFaceModelClient:\n",
    "    \"\"\"\n",
    "    Hugging Face local model client supporting multiple open source large language models.\n",
    "    This client loads models locally using transformers library.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str, quantization_config=None): # quantization_config will always be None now\n",
    "        self.model_name = model_name\n",
    "        self.quantization_config = quantization_config # This will effectively be None\n",
    "\n",
    "        logger.info(f\"加载 HuggingFace 模型: {model_name}\")\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        if self.device == \"cuda\":\n",
    "            logger.info(f\"使用 GPU 设备: {torch.cuda.get_device_name(0)}\")\n",
    "        else:\n",
    "            logger.info(\"未检测到 GPU。将在CPU上运行。\")\n",
    "\n",
    "\n",
    "        # Load tokenizer\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True,\n",
    "                padding_side=\"left\"\n",
    "            )\n",
    "            if not hasattr(self.tokenizer, 'chat_template') or self.tokenizer.chat_template is None:\n",
    "                logger.warning(f\"分词器 {model_name} 没有chat_template。将回退到通用提示格式。\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"加载分词器失败 {model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Ensure pad token exists\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        # Load model\n",
    "        try:\n",
    "            model_kwargs = {\n",
    "                \"torch_dtype\": torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "                \"device_map\": \"auto\", # 让 accelerate 自动映射到可用设备\n",
    "                \"trust_remote_code\": True,\n",
    "                \"low_cpu_mem_usage\": True # 对大型模型有益\n",
    "            }\n",
    "\n",
    "            # Quantization code removed: self.quantization_config will be None\n",
    "\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                **model_kwargs\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"加载模型失败 {model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "        # 创建 pipeline\n",
    "        self.generator = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "        )\n",
    "\n",
    "        logger.info(f\"模型加载成功，使用的设备: {self.model.device}\")\n",
    "\n",
    "    def generate_response(self, messages: List[Dict[str, str]], **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        根据消息生成响应。\n",
    "        确保输出格式与原始 OpenRouter API 响应类似，以便下游逻辑保持不变。\n",
    "        \"\"\"\n",
    "        # 使用模型的chat template构建提示（如果可用）\n",
    "        if hasattr(self.tokenizer, 'chat_template') and self.tokenizer.chat_template is not None:\n",
    "             prompt_formatted = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        else:\n",
    "            # 对于没有chat template的模型（如基本gpt2），回退到通用格式\n",
    "            prompt_formatted = self._build_generic_prompt(messages)\n",
    "            \n",
    "        # 设置生成参数\n",
    "        generation_kwargs = {\n",
    "            \"max_new_tokens\": kwargs.get(\"max_new_tokens\", EXPERIMENT_CONFIG[\"max_new_tokens\"]),\n",
    "            \"temperature\": kwargs.get(\"temperature\", EXPERIMENT_CONFIG[\"temperature\"]),\n",
    "            \"do_sample\": kwargs.get(\"do_sample\", EXPERIMENT_CONFIG[\"do_sample\"]),\n",
    "            \"top_p\": kwargs.get(\"top_p\", EXPERIMENT_CONFIG[\"top_p\"]),\n",
    "            \"top_k\": kwargs.get(\"top_k\", EXPERIMENT_CONFIG[\"top_k\"]),\n",
    "            \"pad_token_id\": self.tokenizer.eos_token_id,\n",
    "            \"return_full_text\": False, # 只返回生成部分\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # 生成响应\n",
    "            outputs = self.generator(prompt_formatted, **generation_kwargs)\n",
    "            \n",
    "            # 提取和清理响应文本\n",
    "            response_text = outputs[0][\"generated_text\"]\n",
    "            \n",
    "            # 针对Qwen等模型的清理（例如，Qwen可能在输出中重复prompt）\n",
    "            if self.tokenizer.chat_template is not None and (\"Qwen\" in self.model_name or \"Mistral\" in self.model_name or \"Llama\" in self.model_name):\n",
    "                # 寻找'assistant\\n'或'Assistant:\\n'后的内容\n",
    "                if \"assistant\\n\" in response_text:\n",
    "                    response_text = response_text.split(\"assistant\\n\", 1)[-1]\n",
    "                elif \"Assistant:\\n\" in response_text:\n",
    "                    response_text = response_text.split(\"Assistant:\\n\", 1)[-1]\n",
    "                response_text = response_text.strip()\n",
    "            \n",
    "            clean_response = self._clean_response(response_text) # 应用通用清理规则\n",
    "            \n",
    "            # 模拟 OpenRouter API 的输出格式\n",
    "            # 计算生成文本的token数\n",
    "            completion_tokens = self.count_tokens(clean_response)\n",
    "            # 计算整个提示的token数（包括系统和用户消息）\n",
    "            prompt_tokens = self.count_tokens(prompt_formatted)\n",
    "            \n",
    "            return {\n",
    "                'choices': [{'message': {'content': clean_response}}],\n",
    "                'usage': {\n",
    "                    'prompt_tokens': prompt_tokens,\n",
    "                    'completion_tokens': completion_tokens,\n",
    "                    'total_tokens': prompt_tokens + completion_tokens\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"生成响应时出错: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _build_generic_prompt(self, messages: List[Dict[str, str]]) -> str:\n",
    "        \"\"\"\n",
    "        为没有chat template的模型构建通用提示格式。\n",
    "        \"\"\"\n",
    "        prompt_parts = []\n",
    "        for message in messages:\n",
    "            role = message[\"role\"]\n",
    "            content = message[\"content\"]\n",
    "            if role == \"system\":\n",
    "                prompt_parts.append(f\"System: {content}\\n\\n\")\n",
    "            elif role == \"user\":\n",
    "                prompt_parts.append(f\"User: {content}\\n\\n\")\n",
    "            elif role == \"assistant\":\n",
    "                prompt_parts.append(f\"Assistant: {content}\\n\\n\")\n",
    "        prompt_parts.append(\"Assistant:\") # 明确请求助手的回复\n",
    "        return \"\".join(prompt_parts)\n",
    "\n",
    "    def _clean_response(self, response: str) -> str:\n",
    "        \"\"\"\n",
    "        清理模型输出以确保单行线索/猜测格式。\n",
    "        去除可能的角色前缀和多余的空白字符。\n",
    "        \"\"\"\n",
    "        # Remove common role prefixes\n",
    "        response = response.replace(\"Assistant:\", \"\").strip()\n",
    "        response = response.replace(\"User:\", \"\").strip()\n",
    "        response = response.replace(\"System:\", \"\").strip()\n",
    "        response = response.replace(\"Human:\", \"\").strip()\n",
    "        response = response.replace(\"AI:\", \"\").strip()\n",
    "        \n",
    "        # Remove special tokens for various models\n",
    "        response = response.replace(\"<|im_start|>user\\n\", \"\").strip() # For Qwen specific tags\n",
    "        response = response.replace(\"<|im_start|>assistant\\n\", \"\").strip() # For Qwen specific tags\n",
    "        response = response.replace(\"<|im_end|>\", \"\").strip() # For Qwen specific tags\n",
    "        response = response.replace(\"<|endoftext|>\", \"\").strip() # For DialoGPT and GPT-2\n",
    "        \n",
    "        # Remove any leading/trailing whitespace and newlines\n",
    "        response = response.strip()\n",
    "        \n",
    "        # 仅保留第一行（避免模型生成过多内容）\n",
    "        lines = response.split('\\n')\n",
    "        if lines:\n",
    "            response = lines[0].strip()\n",
    "\n",
    "        return response\n",
    "\n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"使用HuggingFace分词器准确计算token数量\"\"\"\n",
    "        try:\n",
    "            tokens = self.tokenizer.encode(text, add_special_tokens=False)\n",
    "            return len(tokens)\n",
    "        except Exception:\n",
    "            # 如果分词失败，回退到基于单词的计数\n",
    "            return len(text.split())\n",
    "\n",
    "    def get_tokenizer(self):\n",
    "        \"\"\"获取HuggingFace分词器\"\"\"\n",
    "        return self.tokenizer\n",
    "\n",
    "\n",
    "def validate_model_name(model_name: str) -> bool:\n",
    "    \"\"\"验证模型名称是否符合 HuggingFace 格式 (HuggingFaceModelClient 的辅助函数) \"\"\"\n",
    "    # 对于 gpt2 这种没有'/'的模型，只发出警告，不阻止运行\n",
    "    if \"/\" not in model_name and model_name not in [\"gpt2\", \"distilbert-base-uncased\"]:\n",
    "        logger.warning(f\"模型名称 {model_name} 可能不是一个有效的 HuggingFace 模型名称 (缺少组织名/模型名格式)。\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Removed create_quantization_config function\n",
    "# This function is no longer needed as quantization is fully removed from the script logic.\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# Section 3: 数据集加载\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. Load Dataset\n",
    "def load_dataset(dataset_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    从JSON文件加载词汇数据集。\n",
    "\n",
    "    Args:\n",
    "        dataset_path: 数据集JSON文件的路径。\n",
    "\n",
    "    Returns:\n",
    "        包含目标词、禁忌词和元数据的列表。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Adjusted path for Colab environment\n",
    "        with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "            dataset = json.load(f)\n",
    "        logger.info(f\"成功加载数据集，包含 {len(dataset)} 条记录从 {dataset_path}\")\n",
    "        return dataset\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"数据集文件未找到: {dataset_path}。请上传你的数据集文件到Colab，例如到 /content/data/。\")\n",
    "        logger.error(\"返回空数据集。游戏将无法运行。\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        logger.error(f\"加载数据集失败 {dataset_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "# Update DATASET_PATH for local environment\n",
    "DATASET_PATH = \"./data/wordnet_dataset.json\" # 本地运行时的数据集路径\n",
    "dataset = load_dataset(DATASET_PATH)\n",
    "\n",
    "# Display dataset info\n",
    "print(f\"数据集已加载: {len(dataset)} 条记录\")\n",
    "if dataset:\n",
    "    print(f\"第一个例子:\")\n",
    "    print(f\"目标词: {dataset[0]['target']}\")\n",
    "    print(f\"禁忌词: {dataset[0]['taboo']}\")\n",
    "    print(f\"类别: {dataset[0]['category']}\")\n",
    "    print(f\"定义: {dataset[0]['definition']}\")\n",
    "else:\n",
    "    print(\"数据集为空，无法显示例子。\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Section 4: 模型设置\n",
    "# ==============================================================================\n",
    "\n",
    "# 2. Setup Models for Experiment\n",
    "# Removed OpenRouter API setup and replaced with local HuggingFace model names\n",
    "\n",
    "# 定义实验中使用的本地 HuggingFace 模型\n",
    "# 推荐几个适合 M1 Mac 且比 GPT-2 更强的模型选择：\n",
    "\n",
    "# 当前使用的模型\n",
    "MODELS = [\n",
    "    \"microsoft/DialoGPT-medium\",  # DialoGPT-medium (对话模型，约1B参数，适合M1 Mac)\n",
    "]\n",
    "\n",
    "# 其他备选模型（可以替换上面的模型）：\n",
    "# \"microsoft/DialoGPT-small\"     # 更小但仍比GPT-2强\n",
    "# \"EleutherAI/gpt-neo-1.3B\"     # GPT-Neo 1.3B，指令跟随能力较好\n",
    "# \"facebook/opt-1.3b\"           # Meta的OPT模型\n",
    "# \"distilgpt2\"                  # DistilGPT-2，比GPT-2稍强且更快\n",
    "\n",
    "print(\"Hugging Face 本地模型列表已定义。\")\n",
    "print(f\"实验中可用模型: {MODELS}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Section 5: 实验配置和提示模板\n",
    "# ==============================================================================\n",
    "\n",
    "# 3. Experiment Configuration (No change needed here)\n",
    "# Experiment parameters from design document\n",
    "EXPERIMENT_CONFIG = {\n",
    "    \"temperature\": 0.3,\n",
    "    \"max_turns\": 5,\n",
    "    \"hint_len\": 20,  # Maximum tokens for hints\n",
    "    \"domain\": \"general\",\n",
    "    \"max_reprompt_attempts\": 3,\n",
    "    \"max_new_tokens\": 100, # Added for text-generation pipeline\n",
    "    \"do_sample\": True, # Added for text-generation pipeline\n",
    "    \"top_p\": 0.9, # Added for text-generation pipeline\n",
    "    \"top_k\": 50, # Added for text-generation pipeline\n",
    "    # Removed enable_quantization and quantization_type\n",
    "}\n",
    "\n",
    "# Prompt templates from experiment design\n",
    "HINTER_SYSTEM_PROMPT = \"\"\"You are playing a Taboo word game as the Hinter. Your job is to give clues to help someone guess the target word.\n",
    "\n",
    "RULES:\n",
    "1. NEVER use any of the taboo words in your clue\n",
    "2. Your response must start with \"Clue: \" followed by your hint\n",
    "3. Keep your clue under {hint_len} words\n",
    "4. Give only ONE line as your response\n",
    "\n",
    "Example:\n",
    "Target: apple\n",
    "Taboo: fruit, red, tree\n",
    "Your response: Clue: Round food that keeps doctors away\"\"\"\n",
    "\n",
    "HINTER_USER_TEMPLATE = \"\"\"### Game context\n",
    "Target word: **{target_word}**\n",
    "Taboo list (must not appear in your clue): {taboo_words}\n",
    "\n",
    "Previously given clues:\n",
    "{history_clues}\n",
    "\n",
    "Provide your next clue now.\"\"\"\n",
    "\n",
    "GUESSER_SYSTEM_PROMPT = \"\"\"You are playing a Taboo word game as the Guesser. Someone will give you clues and you need to guess the word.\n",
    "\n",
    "RULES:\n",
    "1. Read the clue carefully\n",
    "2. Your response must start with \"Guess: \" followed by ONE word only\n",
    "3. Give only ONE line as your response, no explanations\n",
    "\n",
    "Example:\n",
    "Clue: Round food that keeps doctors away\n",
    "Your response: Guess: apple\"\"\"\n",
    "\n",
    "GUESSER_USER_TEMPLATE = \"\"\"### Game context\n",
    "Latest clue from Hinter:\n",
    "{latest_clue}\n",
    "\n",
    "Your previous guesses (for context): {history_guesses}\n",
    "\n",
    "Provide your next guess now.\"\"\"\n",
    "\n",
    "print(\"实验配置已加载:\")\n",
    "print(f\"温度: {EXPERIMENT_CONFIG['temperature']}\")\n",
    "print(f\"最大回合数: {EXPERIMENT_CONFIG['max_turns']}\")\n",
    "print(f\"最大提示长度: {EXPERIMENT_CONFIG['hint_len']} tokens\")\n",
    "print(f\"领域: {EXPERIMENT_CONFIG['domain']}\")\n",
    "print(f\"最大重试提示次数: {EXPERIMENT_CONFIG['max_reprompt_attempts']}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Section 6: 游戏逻辑和验证函数\n",
    "# ==============================================================================\n",
    "\n",
    "# 4. Game Logic and Validation Functions\n",
    "\n",
    "def validate_hint_format(hint_output: str) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    验证 hinter 输出是否符合所需格式。\n",
    "\n",
    "    Args:\n",
    "        hint_output: hinter 模型的原始输出。\n",
    "\n",
    "    Returns:\n",
    "        元组 (是否有效, 清理后的提示)。\n",
    "    \"\"\"\n",
    "    lines = hint_output.strip().split('\\n')\n",
    "    if not lines:\n",
    "        return False, \"\"\n",
    "\n",
    "    first_line = lines[0].strip()\n",
    "    if not first_line.startswith(\"Clue: \"):\n",
    "        return False, \"\"\n",
    "\n",
    "    hint = first_line[6:].strip()  # 移除 \"Clue: \" 前缀\n",
    "    return True, hint\n",
    "\n",
    "def validate_guess_format(guess_output: str) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    验证 guesser 输出是否符合所需格式。\n",
    "\n",
    "    Args:\n",
    "        guess_output: guesser 模型的原始输出。\n",
    "\n",
    "    Returns:\n",
    "        元组 (是否有效, 清理后的猜测)。\n",
    "    \"\"\"\n",
    "    lines = guess_output.strip().split('\\n')\n",
    "    if not lines:\n",
    "        return False, \"\"\n",
    "\n",
    "    first_line = lines[0].strip()\n",
    "    if not first_line.startswith(\"Guess: \"):\n",
    "        return False, \"\"\n",
    "\n",
    "    guess = first_line[7:].strip()  # 移除 \"Guess: \" 前缀\n",
    "    # 确保是单个单词\n",
    "    if len(guess.split()) != 1:\n",
    "        return False, \"\"\n",
    "\n",
    "    return True, guess\n",
    "\n",
    "def check_taboo_violation(hint: str, taboo_words: List[str]) -> bool:\n",
    "    \"\"\"\n",
    "    检查提示是否包含任何禁忌词或部分拼写。\n",
    "\n",
    "    Args:\n",
    "        hint: 要检查的提示文本。\n",
    "        taboo_words: 禁止词列表。\n",
    "\n",
    "    Returns:\n",
    "        如果发现违规则返回 True，否则返回 False。\n",
    "    \"\"\"\n",
    "    hint_lower = hint.lower()\n",
    "\n",
    "    for taboo in taboo_words:\n",
    "        taboo_lower = taboo.lower()\n",
    "        # Check exact word match\n",
    "        if re.search(r'\\b' + re.escape(taboo_lower) + r'\\b', hint_lower):\n",
    "            return True\n",
    "        # Check partial spelling (at least 3 characters)\n",
    "        if len(taboo_lower) >= 3 and taboo_lower in hint_lower:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "print(\"游戏验证函数已加载成功\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Section 7: 主游戏引擎和方法实现\n",
    "# ==============================================================================\n",
    "\n",
    "# 5. Main Game Engine\n",
    "class TabooGame:\n",
    "    \"\"\"\n",
    "    运行单个 Taboo 游戏实例的主游戏引擎。\n",
    "    \"\"\"\n",
    "    # Modified constructor to accept HuggingFaceModelClient instances\n",
    "    def __init__(self,\n",
    "                 hinter_client: HuggingFaceModelClient,\n",
    "                 guesser_client: HuggingFaceModelClient,\n",
    "                 config: Dict[str, Any]):\n",
    "        self.hinter_client = hinter_client # Store client instances\n",
    "        self.guesser_client = guesser_client # Store client instances\n",
    "        self.config = config\n",
    "\n",
    "        # Game state\n",
    "        self.turns = []\n",
    "        self.history_clues = []\n",
    "        self.history_guesses = []\n",
    "        self.success = False\n",
    "        self.total_tokens = 0\n",
    "\n",
    "    def play_single_game(self, game_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        用给定的目标词和禁忌词列表玩一个 Taboo 游戏。\n",
    "\n",
    "        Args:\n",
    "            game_data: 包含 'target', 'taboo' 等的字典。\n",
    "\n",
    "        Returns:\n",
    "            完整的游戏结果字典。\n",
    "        \"\"\"\n",
    "        target_word = game_data['target']\n",
    "        taboo_words = game_data['taboo']\n",
    "\n",
    "        logger.info(f\"开始游戏 - 目标词: {target_word}, Hinter: {self.hinter_client.model_name}, Guesser: {self.guesser_client.model_name}\")\n",
    "\n",
    "        # Initialize game state\n",
    "        self.turns = []\n",
    "        self.history_clues = []\n",
    "        self.history_guesses = []\n",
    "        self.success = False\n",
    "        self.total_tokens = 0\n",
    "\n",
    "        # Generate unique run ID\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        hinter_short = self.hinter_client.model_name.split('/')[-1].replace(':free', '')[:8]\n",
    "        guesser_short = self.guesser_client.model_name.split('/')[-1].replace(':free', '')[:8]\n",
    "        run_id = f\"{hinter_short}_{guesser_short}_{timestamp}_{random.randint(1000, 9999)}\" # Add random to avoid collision\n",
    "\n",
    "        # Play the game turn by turn\n",
    "        for turn_id in range(1, self.config['max_turns'] + 1):\n",
    "            logger.info(f\"正在玩回合 {turn_id}\")\n",
    "\n",
    "            turn_result = self._play_turn(turn_id, target_word, taboo_words)\n",
    "            self.turns.append(turn_result)\n",
    "\n",
    "            # Check if game ended\n",
    "            if turn_result.get('correct', False):\n",
    "                self.success = True\n",
    "                logger.info(f\"游戏胜利！回合数 {turn_result['turn_id']} 猜对: {turn_result['guess']}\")\n",
    "                break\n",
    "            elif not turn_result.get('hint_format_ok', True) or not turn_result.get('guesser_format_ok', True):\n",
    "                logger.info(f\"游戏失败，回合 {turn_result['turn_id']} 格式问题\")\n",
    "                break\n",
    "            elif turn_result.get('hint_violate', False):\n",
    "                logger.info(f\"游戏失败，回合 {turn_result['turn_id']} 禁忌词违规\")\n",
    "                break\n",
    "\n",
    "        # Calculate total tokens\n",
    "        self.total_tokens = sum(\n",
    "            turn.get('hint_tokens', 0) + turn.get('guess_tokens', 0)\n",
    "            for turn in self.turns\n",
    "        )\n",
    "\n",
    "        # Compile final result\n",
    "        result = {\n",
    "            \"run_id\": run_id,\n",
    "            \"hinter_model\": self.hinter_client.model_name, # Use client's model_name\n",
    "            \"guesser_model\": self.guesser_client.model_name, # Use client's model_name\n",
    "            \"temperature\": self.config['temperature'],\n",
    "            \"domain\": self.config['domain'],\n",
    "            \"target_word\": target_word,\n",
    "            \"taboo_words\": taboo_words,\n",
    "            \"success\": self.success,\n",
    "            \"turn_count\": len(self.turns),\n",
    "            \"total_tokens\": self.total_tokens,\n",
    "            \"turns\": self.turns\n",
    "        }\n",
    "\n",
    "        return result\n",
    "\n",
    "print(\"TabooGame 类已加载成功\")\n",
    "\n",
    "# 6. Game Methods Implementation\n",
    "# These functions are defined globally and then attached to TabooGame\n",
    "# Make sure to pass necessary client objects to generate_response method\n",
    "\n",
    "def _play_turn(self, turn_id: int, target_word: str, taboo_words: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    进行游戏的一个回合。\n",
    "\n",
    "    Args:\n",
    "        turn_id: 当前回合数（从1开始）。\n",
    "        target_word: 要猜测的目标词。\n",
    "        taboo_words: 禁止词列表。\n",
    "\n",
    "    Returns:\n",
    "        回合结果字典。\n",
    "    \"\"\"\n",
    "    logger.info(f\"正在玩回合 {turn_id}\")\n",
    "\n",
    "    # 生成提示\n",
    "    hint_result = self._get_hint(target_word, taboo_words)\n",
    "\n",
    "    # 如果提示生成失败，则结束回合\n",
    "    if not hint_result['hint_format_ok']:\n",
    "        return {\n",
    "            \"turn_id\": turn_id,\n",
    "            **hint_result,\n",
    "            \"guesser_prompt\": \"\",\n",
    "            \"guesser_output\": \"\",\n",
    "            \"guess\": \"\",\n",
    "            \"guess_tokens\": 0,\n",
    "            \"guesser_format_ok\": False,\n",
    "            \"correct\": False\n",
    "        }\n",
    "\n",
    "    # 添加提示到历史记录\n",
    "    self.history_clues.append(hint_result['hint'])\n",
    "\n",
    "    # 生成猜测\n",
    "    guess_result = self._get_guess(hint_result['hint'])\n",
    "\n",
    "    # 添加猜测到历史记录\n",
    "    if guess_result['guesser_format_ok']:\n",
    "        self.history_guesses.append(guess_result['guess'])\n",
    "\n",
    "    # 检查猜测是否正确\n",
    "    correct = (guess_result.get('guess', '').lower() == target_word.lower())\n",
    "\n",
    "    # 合并结果\n",
    "    turn_result = {\n",
    "        \"turn_id\": turn_id,\n",
    "        **hint_result,\n",
    "        **guess_result,\n",
    "        \"correct\": correct\n",
    "    }\n",
    "\n",
    "    self.total_tokens += hint_result.get('hint_tokens', 0) + guess_result.get('guess_tokens', 0)\n",
    "\n",
    "    return turn_result\n",
    "\n",
    "def _get_hint(self, target_word: str, taboo_words: List[str], domain: str = \"general\") -> Dict[str, Any]: # Add domain parameter\n",
    "    \"\"\"\n",
    "    通过 hinter 模型生成提示，带验证和重试逻辑。\n",
    "\n",
    "    Args:\n",
    "        target_word: 要提示的目标词。\n",
    "        taboo_words: 禁止词列表。\n",
    "        domain: 游戏领域。\n",
    "\n",
    "    Returns:\n",
    "        包含提示结果和元数据的字典。\n",
    "    \"\"\"\n",
    "    # 准备历史提示字符串\n",
    "    history_clues_str = \"\\n\".join([f\"- {clue}\" for clue in self.history_clues]) if self.history_clues else \"None\"\n",
    "\n",
    "    # 构建提示\n",
    "    system_prompt = HINTER_SYSTEM_PROMPT.format(hint_len=self.config['hint_len'], domain=domain) # Pass domain to prompt\n",
    "    user_prompt = HINTER_USER_TEMPLATE.format(\n",
    "        target_word=target_word,\n",
    "        taboo_words=taboo_words,\n",
    "        history_clues=history_clues_str\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    # 仅在格式错误时尝试多次（根据实验设计）\n",
    "    for attempt in range(self.config['max_reprompt_attempts']):\n",
    "        try:\n",
    "            # Call HuggingFaceModelClient.generate_response\n",
    "            response = self.hinter_client.generate_response(\n",
    "                messages,\n",
    "                max_new_tokens=self.config['max_new_tokens'],\n",
    "                temperature=self.config['temperature'],\n",
    "                do_sample=self.config['do_sample'],\n",
    "                top_p=self.config['top_p'],\n",
    "                top_k=self.config['top_k']\n",
    "            )\n",
    "\n",
    "            # Extract content and tokens from HF client's response format\n",
    "            hint_output = response['choices'][0]['message']['content']\n",
    "            hint_tokens = response['usage']['completion_tokens'] # Assuming usage field exists\n",
    "\n",
    "            # 验证格式\n",
    "            format_ok, hint = validate_hint_format(hint_output)\n",
    "            if not format_ok:\n",
    "                logger.warning(f\"提示格式无效，尝试 {attempt+1}: {hint_output}\")\n",
    "                continue  # 针对格式问题重试\n",
    "\n",
    "            # 检查 token 数量（这也是一个格式问题）\n",
    "            # Use hinter_client's accurate token counting\n",
    "            if self.hinter_client.count_tokens(hint) > self.config['hint_len']:\n",
    "                logger.warning(f\"提示过长，尝试 {attempt+1}: {self.hinter_client.count_tokens(hint)} tokens\")\n",
    "                continue  # 针对长度问题重试\n",
    "\n",
    "            # 检查禁忌词违规（不重试）\n",
    "            hint_violate = check_taboo_violation(hint, taboo_words)\n",
    "\n",
    "            # 返回结果（即使有禁忌词违规 - 不为违规重试）\n",
    "            return {\n",
    "                \"hinter_prompt\": f\"System: {system_prompt}\\n\\nUser: {user_prompt}\",\n",
    "                \"hinter_output\": hint_output,\n",
    "                \"hint\": hint,\n",
    "                \"hint_tokens\": hint_tokens,\n",
    "                \"hint_violate\": hint_violate,\n",
    "                \"hint_format_ok\": True\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"提示生成错误，尝试 {attempt+1}: {e}\")\n",
    "\n",
    "    # 所有尝试都失败了\n",
    "    return {\n",
    "        \"hinter_prompt\": f\"System: {system_prompt}\\n\\nUser: {user_prompt}\",\n",
    "        \"hinter_output\": \"\",\n",
    "        \"hint\": \"\",\n",
    "        \"hint_tokens\": 0,\n",
    "        \"hint_violate\": False,\n",
    "        \"hint_format_ok\": False\n",
    "    }\n",
    "\n",
    "def _get_guess(self, latest_hint: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    通过 guesser 模型生成猜测，带验证和重试逻辑。\n",
    "\n",
    "    Args:\n",
    "        latest_hint: Hinter 发出的最新提示。\n",
    "\n",
    "    Returns:\n",
    "        包含猜测结果和元数据的字典。\n",
    "    \"\"\"\n",
    "    # 准备历史猜测字符串\n",
    "    history_guesses_str = \", \".join(self.history_guesses) if self.history_guesses else \"None\"\n",
    "\n",
    "    # 构建提示\n",
    "    user_prompt = GUESSER_USER_TEMPLATE.format(\n",
    "        latest_clue=latest_hint,\n",
    "        history_guesses=history_guesses_str\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": GUESSER_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    # 仅在格式错误时尝试多次\n",
    "    for attempt in range(self.config['max_reprompt_attempts']):\n",
    "        try:\n",
    "            # Call HuggingFaceModelClient.generate_response\n",
    "            response = self.guesser_client.generate_response(\n",
    "                messages,\n",
    "                max_new_tokens=self.config['max_new_tokens'],\n",
    "                temperature=self.config['temperature'],\n",
    "                do_sample=self.config['do_sample'],\n",
    "                top_p=self.config['top_p'],\n",
    "                top_k=self.config['top_k']\n",
    "            )\n",
    "\n",
    "            # Extract content and tokens from HF client's response format\n",
    "            guess_output = response['choices'][0]['message']['content']\n",
    "            guess_tokens = response['usage']['completion_tokens']\n",
    "\n",
    "            # 验证格式\n",
    "            format_ok, guess = validate_guess_format(guess_output)\n",
    "            if format_ok:\n",
    "                return {\n",
    "                    \"guesser_prompt\": f\"System: {GUESSER_SYSTEM_PROMPT}\\\\n\\\\nUser: {user_prompt}\",\n",
    "                    \"guesser_output\": guess_output,\n",
    "                    \"guess\": guess,\n",
    "                    \"guess_tokens\": guess_tokens,\n",
    "                    \"guesser_format_ok\": True\n",
    "                }\n",
    "            else:\n",
    "                logger.warning(f\"猜测格式无效，尝试 {attempt+1}: {guess_output}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"猜测生成错误，尝试 {attempt+1}: {e}\")\n",
    "\n",
    "    # 所有尝试都失败了\n",
    "    return {\n",
    "        \"guesser_prompt\": f\"System: {GUESSER_SYSTEM_PROMPT}\\\\n\\\\nUser: {user_prompt}\",\n",
    "        \"guesser_output\": \"\",\n",
    "        \"guess\": \"\",\n",
    "        \"guess_tokens\": 0,\n",
    "        \"guesser_format_ok\": False\n",
    "    }\n",
    "\n",
    "# Attach methods to TabooGame class\n",
    "TabooGame._play_turn = _play_turn\n",
    "TabooGame._get_hint = _get_hint\n",
    "TabooGame._get_guess = _get_guess\n",
    "\n",
    "print(\"游戏方法实现已完成\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Section 8: 运行实验\n",
    "# ==============================================================================\n",
    "\n",
    "# 7. Run Experiment - Parameters adjustable, output saved as CSV\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np # Ensure numpy is imported\n",
    "\n",
    "# ============ Experiment Parameters Setting ============\n",
    "EXPERIMENT_PARAMS = {\n",
    "    \"experiment_name\": \"taboo_experiment\",\n",
    "    \"temperature\": 0.7,  # 提高温度以获得更有创意的回答\n",
    "    \"max_turns\": 5,\n",
    "    \"hint_len\": 15,  # 稍微减少提示长度\n",
    "    \"domain\": \"general\",\n",
    "    \"max_reprompt_attempts\": 3,\n",
    "    \"max_new_tokens\": 50, # 减少生成长度，更容易控制格式\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.8, # 调整采样参数\n",
    "    \"top_k\": 40,\n",
    "    \"num_games_per_pair\": 3,  # 减少游戏数量用于测试\n",
    "    \"sample_dataset\": True,\n",
    "    \"output_dir\": \"./results\"\n",
    "}\n",
    "\n",
    "# IMPORTANT: Defined models to use for this experiment\n",
    "LOCAL_HF_MODELS = [\n",
    "    \"Qwen/Qwen3-8B\", # Qwen-8B (可能需要访问权限，且内存占用大)\n",
    "    \"gpt2\",          # GPT-2 (公开模型，内存占用小)\n",
    "]\n",
    "\n",
    "# Cache loaded HuggingFaceModelClient instances\n",
    "model_client_cache = {}\n",
    "model_client_cache_lock = Lock() # For thread-safe model loading\n",
    "\n",
    "def get_hf_model_client(model_name: str) -> HuggingFaceModelClient:\n",
    "    \"\"\"\n",
    "    Get or create a HuggingFaceModelClient instance, with caching and quantization handling.\n",
    "    \"\"\"\n",
    "    with model_client_cache_lock:\n",
    "        if model_name not in model_client_cache:\n",
    "            quantization_config = None # Always None since quantization is disabled\n",
    "            \n",
    "            model_client_cache[model_name] = HuggingFaceModelClient(\n",
    "                model_name,\n",
    "                quantization_config=quantization_config\n",
    "            )\n",
    "        return model_client_cache[model_name]\n",
    "\n",
    "\n",
    "def process_result(result):\n",
    "    \"\"\"Process and store a single game result\"\"\"\n",
    "    with results_lock:\n",
    "        all_results.append(result)\n",
    "        \n",
    "        # Extract conversation data from turns\n",
    "        for turn in result.get('turns', []):\n",
    "            # Add hinter conversation\n",
    "            conversation_data.append({\n",
    "                'run_id': result['run_id'],\n",
    "                'turn_id': turn['turn_id'],\n",
    "                'role': 'hinter',\n",
    "                'model': result['hinter_model'],\n",
    "                'prompt': turn.get('hinter_prompt', ''),\n",
    "                'output': turn.get('hinter_output', ''),\n",
    "                'content': turn.get('hint', ''),\n",
    "                'tokens': turn.get('hint_tokens', 0),\n",
    "                'format_ok': turn.get('hint_format_ok', False),\n",
    "                'violate_taboo': turn.get('hint_violate', False),\n",
    "                'target_word': result['target_word']\n",
    "            })\n",
    "            \n",
    "            # Add guesser conversation\n",
    "            conversation_data.append({\n",
    "                'run_id': result['run_id'],\n",
    "                'turn_id': turn['turn_id'],\n",
    "                'role': 'guesser',\n",
    "                'model': result['guesser_model'],\n",
    "                'prompt': turn.get('guesser_prompt', ''),\n",
    "                'output': turn.get('guesser_output', ''),\n",
    "                'content': turn.get('guess', ''),\n",
    "                'tokens': turn.get('guess_tokens', 0),\n",
    "                'format_ok': turn.get('guesser_format_ok', False),\n",
    "                'violate_taboo': False,  # Guesser不会违反禁忌词\n",
    "                'target_word': result['target_word']\n",
    "            })\n",
    "\n",
    "def run_single_game_wrapper(args_tuple):\n",
    "    \"\"\"Wrapper function for running a single game in parallel\"\"\"\n",
    "    game_data, hinter_client, guesser_client, config = args_tuple\n",
    "\n",
    "    try:\n",
    "        # Create game instance\n",
    "        game = TabooGame(hinter_client, guesser_client, config)\n",
    "\n",
    "        # Run game\n",
    "        result = game.play_single_game(game_data) # Removed dataset_name as it's not used by play_single_game\n",
    "        result[\"dataset_name\"] = EXPERIMENT_PARAMS[\"domain\"] # Add domain back from config for consistency\n",
    "\n",
    "        return result, None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"游戏执行错误: {e}\")\n",
    "        return None, str(e)\n",
    "\n",
    "\n",
    "def run_experiment_local(): # Renamed function to avoid conflict, local implementation\n",
    "    \"\"\"\n",
    "    运行实验并将结果保存为CSV文件，包含原始对话数据。\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = Path(EXPERIMENT_PARAMS[\"output_dir\"]) / f\"{EXPERIMENT_PARAMS['experiment_name']}_{timestamp}\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 实验配置\n",
    "    config = {\n",
    "        \"temperature\": EXPERIMENT_PARAMS[\"temperature\"],\n",
    "        \"max_turns\": EXPERIMENT_PARAMS[\"max_turns\"],\n",
    "        \"hint_len\": EXPERIMENT_PARAMS[\"hint_len\"],\n",
    "        \"domain\": EXPERIMENT_PARAMS[\"domain\"],\n",
    "        \"max_reprompt_attempts\": EXPERIMENT_PARAMS[\"max_reprompt_attempts\"],\n",
    "        \"max_new_tokens\": EXPERIMENT_PARAMS.get(\"max_new_tokens\", 100), # Ensure generation params are passed\n",
    "        \"do_sample\": EXPERIMENT_PARAMS.get(\"do_sample\", True),\n",
    "        \"top_p\": EXPERIMENT_PARAMS.get(\"top_p\", 0.9),\n",
    "        \"top_k\": EXPERIMENT_PARAMS.get(\"top_k\", 50),\n",
    "    }\n",
    "\n",
    "    logger.info(f\"开始实验: {EXPERIMENT_PARAMS['experiment_name']}\")\n",
    "    logger.info(f\"参数: {EXPERIMENT_PARAMS}\")\n",
    "    logger.info(f\"输出目录: {output_dir}\")\n",
    "\n",
    "    # 保存实验配置\n",
    "    config_file = output_dir / \"experiment_config.json\"\n",
    "    with open(config_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(EXPERIMENT_PARAMS, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # 存储所有结果 - 将这些变量移到函数开始，使 process_result 可以访问\n",
    "    global all_results, conversation_data, results_lock\n",
    "    all_results = []\n",
    "    conversation_data = []\n",
    "    results_lock = Lock()  # For thread-safe access to shared data\n",
    "\n",
    "    # Iterate through MODELS (global list)\n",
    "    total_pairs = len(MODELS) * len(MODELS) # Use MODELS global list for iteration\n",
    "    pair_count = 0\n",
    "\n",
    "    # 遍历所有模型对\n",
    "    for hinter_model_name in MODELS: # Use MODELS global list\n",
    "        for guesser_model_name in MODELS: # Use MODELS global list\n",
    "            pair_count += 1\n",
    "            logger.info(f\"处理模型对 {pair_count}/{total_pairs}: {hinter_model_name} -> {guesser_model_name}\")\n",
    "\n",
    "            # Get HuggingFaceModelClient instances\n",
    "            try:\n",
    "                hinter_client = get_hf_model_client(hinter_model_name)\n",
    "                guesser_client = get_hf_model_client(guesser_model_name)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"因模型加载失败跳过此组合 ({hinter_model_name} 或 {guesser_model_name}): {e}\")\n",
    "                continue\n",
    "\n",
    "            # 选择游戏样本\n",
    "            if EXPERIMENT_PARAMS[\"sample_dataset\"]:\n",
    "                game_samples = random.sample(dataset, min(EXPERIMENT_PARAMS[\"num_games_per_pair\"], len(dataset)))\n",
    "            else:\n",
    "                game_samples = dataset[:EXPERIMENT_PARAMS[\"num_games_per_pair\"]]\n",
    "\n",
    "            if not game_samples:\n",
    "                logger.warning(f\"数据集 {EXPERIMENT_PARAMS['domain']} 为空，未找到游戏样本。跳过此组合。\")\n",
    "                continue\n",
    "\n",
    "            # 为并行执行准备参数\n",
    "            game_args = [\n",
    "                (game_data, hinter_client, guesser_client, config) # Pass client instances directly\n",
    "                for game_data in game_samples\n",
    "            ]\n",
    "\n",
    "            # 并行运行游戏\n",
    "            # For Colab with a single GPU, max_workers=1 or 2 is often sufficient to avoid memory issues.\n",
    "            # If multiple GPUs are available and models are small enough, you can set max_workers higher.\n",
    "            max_workers_for_pool = min(os.cpu_count(), EXPERIMENT_PARAMS.get('max_workers', 2))\n",
    "            if torch.cuda.is_available() and max_workers_for_pool > torch.cuda.device_count() * 2:\n",
    "                logger.warning(\"检测到GPU，但并行工作者数量可能过高，可能导致内存问题。请考虑减少 max_workers 参数。\")\n",
    "\n",
    "            with ThreadPoolExecutor(max_workers=max_workers_for_pool) as executor: # Use adjusted workers\n",
    "                # 提交所有游戏进行并行执行\n",
    "                future_to_game = {\n",
    "                    executor.submit(run_single_game_wrapper, args): i\n",
    "                    for i, args in enumerate(game_args)\n",
    "                }\n",
    "\n",
    "                # 处理已完成的游戏\n",
    "                completed = 0\n",
    "                for future in as_completed(future_to_game):\n",
    "                    completed += 1\n",
    "                    game_idx = future_to_game[future]\n",
    "\n",
    "                    try:\n",
    "                        result, error = future.result()\n",
    "                        if result is not None:\n",
    "                            logger.info(f\"  已完成游戏 {completed}/{len(game_samples)}: {result['target_word']}\")\n",
    "                            process_result(result)\n",
    "                        elif error:\n",
    "                            logger.error(f\"  游戏 {game_idx+1} 失败: {error}\")\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"  游戏 {game_idx+1} 执行错误: {e}\")\n",
    "\n",
    "    # 保存结果\n",
    "    logger.info(\"保存实验结果到CSV文件...\")\n",
    "\n",
    "    if all_results: # 仅当有结果时才保存\n",
    "        # 1. 保存游戏汇总结果\n",
    "        summary_rows = []\n",
    "        for result in all_results:\n",
    "            summary_rows.append({\n",
    "                'run_id': result.get('run_id', ''),\n",
    "                'hinter_model': result.get('hinter_model', ''),\n",
    "                'guesser_model': result.get('guesser_model', ''),\n",
    "                'temperature': result.get('temperature', ''),\n",
    "                'domain': result.get('domain', ''),\n",
    "                'target_word': result.get('target_word', ''),\n",
    "                'taboo_words': '|'.join(result.get('taboo_words', [])),\n",
    "                'success': result.get('success', False),\n",
    "                'turn_count': result.get('turn_count', 0),\n",
    "                'total_tokens': result.get('total_tokens', 0),\n",
    "                'error': result.get('error', '')\n",
    "            })\n",
    "\n",
    "        summary_df = pd.DataFrame(summary_rows)\n",
    "        summary_file = output_dir / f\"{EXPERIMENT_PARAMS['experiment_name']}_game_summary.csv\"\n",
    "        summary_df.to_csv(summary_file, index=False, encoding='utf-8')\n",
    "        logger.info(f\"游戏汇总保存到: {summary_file}\")\n",
    "\n",
    "        # 2. 保存原始对话数据\n",
    "        if conversation_data:\n",
    "            conversation_df = pd.DataFrame(conversation_data)\n",
    "            conversation_file = output_dir / f\"{EXPERIMENT_PARAMS['experiment_name']}_conversations.csv\"\n",
    "            conversation_df.to_csv(conversation_file, index=False, encoding='utf-8')\n",
    "            logger.info(f\"对话数据保存到: {conversation_file}\")\n",
    "\n",
    "        # 3. 保存统计摘要\n",
    "        total_games = len(all_results)\n",
    "        successful_games = sum(1 for r in all_results if r.get('success', False))\n",
    "        success_rate = successful_games / total_games if total_games > 0 else 0\n",
    "\n",
    "        stats = {\n",
    "            'total_games': total_games,\n",
    "            'successful_games': successful_games,\n",
    "            'success_rate': success_rate,\n",
    "            'total_conversations': len(conversation_data),\n",
    "            'total_tokens': sum(r.get('total_tokens', 0) for r in all_results)\n",
    "        }\n",
    "\n",
    "        stats_file = output_dir / f\"{EXPERIMENT_PARAMS['experiment_name']}_statistics.json\"\n",
    "        with open(stats_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(stats, f, indent=2)\n",
    "\n",
    "        logger.info(f\"实验完成!\")\n",
    "        logger.info(f\"总计: {total_games} 局游戏, {successful_games} 局成功 ({success_rate:.2%} 成功率)\")\n",
    "        logger.info(f\"对话记录: {len(conversation_data)} 条\")\n",
    "        logger.info(f\"结果保存在: {output_dir}\")\n",
    "\n",
    "        return str(output_dir), stats\n",
    "    else:\n",
    "        print(\"没有游戏运行或结果未收集。未生成输出文件。\")\n",
    "        return None, None # Return None for stats if no results\n",
    "\n",
    "print(\"实验参数已设置，运行以下命令开始实验:\")\n",
    "print(\"result_dir, stats = run_experiment_local()\")\n",
    "print()\n",
    "print(\"当前参数:\")\n",
    "for key, value in EXPERIMENT_PARAMS.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print()\n",
    "print(\"修改参数示例:\")\n",
    "print(\"EXPERIMENT_PARAMS['num_games_per_pair'] = 10\")\n",
    "print(\"EXPERIMENT_PARAMS['temperature'] = 0.5\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# Section 9: 运行和分析\n",
    "# ==============================================================================\n",
    "\n",
    "# Run comprehensive analysis\n",
    "result_dir, stats = run_experiment_local()\n",
    "\n",
    "if result_dir and stats: # Only analyze if experiment ran and results were collected\n",
    "    print(\"Starting comprehensive experiment results analysis...\")\n",
    "    \n",
    "    # Placeholder for actual analysis functions if they were in the original notebook.\n",
    "    # Assuming analyze_experiment_results and analyze_conversations were defined elsewhere.\n",
    "    # If they were intended to be in the same cell, you'd need to define them here.\n",
    "    # For now, just print a confirmation.\n",
    "    \n",
    "    # --- Example of analysis function (if you had them) ---\n",
    "    def analyze_experiment_results(result_dir_path):\n",
    "        summary_file = Path(result_dir_path) / \"taboo_experiment_game_summary.csv\"\n",
    "        if summary_file.exists():\n",
    "            df = pd.read_csv(summary_file)\n",
    "            print(f\"\\n游戏汇总分析:\\n{df.head()}\")\n",
    "            return df\n",
    "        else:\n",
    "            print(f\"警告: 游戏汇总文件未找到在 {summary_file}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def analyze_conversations(result_dir_path):\n",
    "        conversation_file = Path(result_dir_path) / \"taboo_experiment_conversations.csv\"\n",
    "        if conversation_file.exists():\n",
    "            df = pd.read_csv(conversation_file)\n",
    "            print(f\"\\n对话分析:\\n{df.head()}\")\n",
    "            return df\n",
    "        else:\n",
    "            print(f\"警告: 对话文件未找到在 {conversation_file}\")\n",
    "            return pd.DataFrame()\n",
    "    # --- End Example ---\n",
    "\n",
    "    summary_df = analyze_experiment_results(result_dir)\n",
    "    conv_df = analyze_conversations(result_dir)\n",
    "\n",
    "    # Pie Chart Analysis - Comprehensive Proportion Analysis\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    def create_pie_chart_analysis(result_dir_path):\n",
    "        \"\"\"创建综合饼图分析，显示数据比例\"\"\"\n",
    "        result_path = Path(result_dir_path)\n",
    "\n",
    "        # Load data\n",
    "        summary_file = result_path / \"taboo_experiment_game_summary.csv\"\n",
    "        conversation_file = result_path / \"taboo_experiment_conversations.csv\"\n",
    "\n",
    "        if not summary_file.exists():\n",
    "            print(\"游戏汇总文件未找到\")\n",
    "            return\n",
    "\n",
    "        summary_df = pd.read_csv(summary_file)\n",
    "        summary_df['hinter_short'] = summary_df['hinter_model'].apply(lambda x: x.split('/')[-1].replace(':free', ''))\n",
    "        summary_df['guesser_short'] = summary_df['guesser_model'].apply(lambda x: x.split('/')[-1].replace(':free', ''))\n",
    "\n",
    "        print(\"正在创建综合饼图分析...\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        # 创建带饼图的图形\n",
    "        fig = plt.figure(figsize=(20, 25))\n",
    "        colors_success = ['#2E8B57', '#CD5C5C']  # 绿色, 红色\n",
    "        colors_models = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']  # 鲜艳的颜色\n",
    "        colors_turns = ['#FFD93D', '#6BCF7F', '#4D96FF', '#9B59B6', '#E74C3C']\n",
    "\n",
    "        # 1. 整体成功与失败分布\n",
    "        plt.subplot(3, 3, 1)\n",
    "        success_counts = summary_df['success'].value_counts()\n",
    "        success_labels = ['成功游戏', '失败游戏']\n",
    "        plt.pie(success_counts.values, labels=success_labels, autopct='%1.1f%%',\n",
    "               colors=colors_success, startangle=90, explode=(0.05, 0))\n",
    "        plt.title('整体游戏结果分布', fontsize=16, fontweight='bold')\n",
    "\n",
    "        # 2. 作为Hinter的模型性能 - 获胜分布\n",
    "        plt.subplot(3, 3, 2)\n",
    "        hinter_wins = summary_df.groupby('hinter_short')['success'].sum().sort_values(ascending=False)\n",
    "        plt.pie(hinter_wins.values, labels=hinter_wins.index, autopct='%1.1f%%',\n",
    "               colors=colors_models, startangle=90)\n",
    "        plt.title('Hinter 模型获胜分布', fontsize=16, fontweight='bold')\n",
    "\n",
    "        # 3. 作为Guesser的模型性能 - 获胜分布\n",
    "        plt.subplot(3, 3, 3)\n",
    "        guesser_wins = summary_df.groupby('guesser_short')['success'].sum().sort_values(ascending=False)\n",
    "        plt.pie(guesser_wins.values, labels=guesser_wins.index, autopct='%1.1f%%',\n",
    "               colors=colors_models, startangle=90)\n",
    "        plt.title('Guesser 模型获胜分布', fontsize=16, fontweight='bold')\n",
    "\n",
    "        # 4. 成功游戏的回合数分布\n",
    "        plt.subplot(3, 3, 4)\n",
    "        successful_games_df = summary_df[summary_df['success'] == True] # Use a new df name to avoid conflict with `successful_games` variable in the text output part\n",
    "        if len(successful_games_df) > 0:\n",
    "            turn_counts = successful_games_df['turn_count'].value_counts().sort_index()\n",
    "            turn_labels = [f'{turn} 回合{\"s\" if turn > 1 else \"\"}' for turn in turn_counts.index] # Translate to Chinese\n",
    "            plt.pie(turn_counts.values, labels=turn_labels, autopct='%1.1f%%',\n",
    "                   colors=colors_turns[:len(turn_counts)], startangle=90)\n",
    "            plt.title('成功游戏回合数分布', fontsize=16, fontweight='bold')\n",
    "\n",
    "        # 5. Token 使用分布范围\n",
    "        plt.subplot(3, 3, 5)\n",
    "        token_bins = [0, 50, 100, 200, 500, float('inf')]\n",
    "        token_labels = ['≤50', '51-100', '101-200', '201-500', '>500']\n",
    "        summary_df['token_category'] = pd.cut(summary_df['total_tokens'], bins=token_bins, labels=token_labels, include_lowest=True)\n",
    "        token_dist = summary_df['token_category'].value_counts()\n",
    "        plt.pie(token_dist.values, labels=token_dist.index, autopct='%1.1f%%',\n",
    "               colors=plt.cm.Set3(np.linspace(0, 1, len(token_dist))), startangle=90)\n",
    "        plt.title('Token 使用范围分布', fontsize=16, fontweight='bold')\n",
    "\n",
    "        # 6. 模型参与度分布 (总游戏数)\n",
    "        plt.subplot(3, 3, 6)\n",
    "        hinter_games = summary_df.groupby('hinter_short').size()\n",
    "        guesser_games = summary_df.groupby('guesser_short').size()\n",
    "        total_participation = hinter_games.add(guesser_games, fill_value=0) # Use add with fill_value for robust sum\n",
    "        plt.pie(total_participation.values, labels=total_participation.index, autopct='%1.1f%%',\n",
    "               colors=colors_models, startangle=90)\n",
    "        plt.title('总游戏参与度分布', fontsize=16, fontweight='bold')\n",
    "\n",
    "        # 添加对话分析（如果可用）\n",
    "        if conversation_file.exists():\n",
    "            conv_df = pd.read_csv(conversation_file)\n",
    "            conv_df['model_short'] = conv_df['model'].apply(lambda x: x.split('/')[-1].replace(':free', ''))\n",
    "\n",
    "            # 7. Token 按角色分布\n",
    "            plt.subplot(3, 3, 7)\n",
    "            role_tokens = conv_df.groupby('role')['tokens'].sum()\n",
    "            role_labels = [f'{role.title()} ({tokens:,} tokens)' for role, tokens in role_tokens.items()]\n",
    "            plt.pie(role_tokens.values, labels=role_labels, autopct='%1.1f%%',\n",
    "                   colors=['#87CEEB', '#DDA0DD'], startangle=90)\n",
    "            plt.title('Token 按角色使用分布', fontsize=16, fontweight='bold')\n",
    "\n",
    "            # 8. 格式问题分布\n",
    "            plt.subplot(3, 3, 8)\n",
    "            format_issues_counts = conv_df.groupby('format_ok').size()\n",
    "            format_labels = ['格式正确', '格式问题']\n",
    "            if len(format_issues_counts) > 1:\n",
    "                plt.pie(format_issues_counts.values, labels=format_labels, autopct='%1.1f%%',\n",
    "                       colors=['#90EE90', '#FFB6C1'], startangle=90)\n",
    "            else:\n",
    "                # 如果所有格式都正确\n",
    "                plt.pie([100], labels=['所有格式都正确'], autopct='%1.1f%%',\n",
    "                       colors=['#90EE90'], startangle=90)\n",
    "            plt.title('格式合规性分布', fontsize=16, fontweight='bold')\n",
    "\n",
    "            # 9. 禁忌词违规分布 (仅Hinters)\n",
    "            plt.subplot(3, 3, 9)\n",
    "            hinter_data = conv_df[conv_df['role'] == 'hinter']\n",
    "            if len(hinter_data) > 0:\n",
    "                taboo_violations_counts = hinter_data.groupby('violate_taboo').size()\n",
    "                taboo_labels = ['无违规', '禁忌词违规']\n",
    "                if len(taboo_violations_counts) > 1:\n",
    "                    plt.pie(taboo_violations_counts.values, labels=taboo_labels, autopct='%1.1f%%',\n",
    "                           colors=['#98FB98', '#F08080'], startangle=90)\n",
    "                else:\n",
    "                    # 如果没有违规\n",
    "                    plt.pie([100], labels=['无禁忌词违规'], autopct='%1.1f%%',\n",
    "                           colors=['#98FB98'], startangle=90)\n",
    "                plt.title('禁忌词违规分布 (Hinters)', fontsize=16, fontweight='bold')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # 打印详细统计数据\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"饼图统计摘要\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        print(f\"\\n1. 总体成功率:\")\n",
    "        success_rate = summary_df['success'].mean()\n",
    "        total_games_overall = len(summary_df) # Use new variable name\n",
    "        successful_games_overall = summary_df['success'].sum() # Use new variable name\n",
    "        print(f\"   • 成功游戏: {successful_games_overall}/{total_games_overall} ({success_rate:.1%})\")\n",
    "        print(f\"   • 失败游戏: {total_games_overall - successful_games_overall}/{total_games_overall} ({1-success_rate:.1%})\")\n",
    "\n",
    "        print(f\"\\n2. 模型性能排名:\")\n",
    "        print(\"   作为 Hinter (按获胜次数):\")\n",
    "        hinter_wins_overall = summary_df.groupby('hinter_short')['success'].sum().sort_values(ascending=False)\n",
    "        for i, (model, wins) in enumerate(hinter_wins_overall.items(), 1):\n",
    "            total_hinter_games_model = summary_df[summary_df['hinter_short'] == model].shape[0]\n",
    "            win_rate_hinter = wins / total_hinter_games_model if total_hinter_games_model > 0 else 0\n",
    "            print(f\"   {i}. {model}: {wins}/{total_hinter_games_model} 获胜 ({win_rate_hinter:.1%})\")\n",
    "\n",
    "        print(\"   作为 Guesser (按获胜次数):\")\n",
    "        guesser_wins_overall = summary_df.groupby('guesser_short')['success'].sum().sort_values(ascending=False)\n",
    "        for i, (model, wins) in enumerate(guesser_wins_overall.items(), 1):\n",
    "            total_guesser_games_model = summary_df[summary_df['guesser_short'] == model].shape[0]\n",
    "            win_rate_guesser = wins / total_guesser_games_model if total_guesser_games_model > 0 else 0\n",
    "            print(f\"   {i}. {model}: {wins}/{total_guesser_games_model} 获胜 ({win_rate_guesser:.1%})\")\n",
    "\n",
    "        successful_games_for_text = summary_df[summary_df['success'] == True] # Use a new variable for text output\n",
    "        if len(successful_games_for_text) > 0:\n",
    "            print(f\"\\n3. 效率分析:\")\n",
    "            turn_analysis_text = successful_games_for_text['turn_count'].value_counts().sort_index()\n",
    "            print(\"   按回合数成功:\")\n",
    "            for turn, count in turn_analysis_text.items():\n",
    "                percentage = count / len(successful_games_for_text) * 100\n",
    "                print(f\"   • 回合 {turn}: {count} 游戏 ({percentage:.1f}%)\") # Corrected f-string for percentage\n",
    "            \n",
    "            avg_turns = successful_games_for_text['turn_count'].mean()\n",
    "            avg_tokens = successful_games_for_text['total_tokens'].mean()\n",
    "            print(f\"   • 平均成功回合数: {avg_turns:.1f}\")\n",
    "            print(f\"   • 每次成功游戏的平均Token数: {avg_tokens:.0f}\")\n",
    "\n",
    "        print(f\"\\n4. TOKEN 使用模式:\")\n",
    "        token_dist_overall = summary_df['token_category'].value_counts() # Use new variable for text output\n",
    "        for category, count in token_dist_overall.items():\n",
    "            percentage = count / len(summary_df) * 100\n",
    "            print(f\"   • {category} tokens: {count} 游戏 ({percentage:.1f}%)\") # Corrected f-string for percentage\n",
    "        \n",
    "        if conversation_file.exists():\n",
    "            conv_df_overall = pd.read_csv(conversation_file) # Use new variable for text output\n",
    "            conv_df_overall['model_short'] = conv_df_overall['model'].apply(lambda x: x.split('/')[-1].replace(':free', ''))\n",
    "            total_conversations_overall = len(conv_df_overall)\n",
    "            total_tokens_used_overall = conv_df_overall['tokens'].sum()\n",
    "            print(f\"\\n5. 对话分析:\")\n",
    "            print(f\"   • 总对话数: {total_conversations_overall:,}\")\n",
    "            print(f\"   • 总使用Token数: {total_tokens_used_overall:,}\")\n",
    "            \n",
    "            hinter_data_overall = conv_df_overall[conv_df_overall['role'] == 'hinter'] # Use new variable for text output\n",
    "            if len(hinter_data_overall) > 0:\n",
    "                violation_rate_overall = hinter_data_overall['violate_taboo'].mean()\n",
    "                format_rate_overall = conv_df_overall['format_ok'].mean()\n",
    "                print(f\"   • 禁忌词违规率: {violation_rate_overall:.1%}\")\n",
    "                print(f\"   • 格式合规率: {format_rate_overall:.1%}\")\n",
    "        \n",
    "        return summary_df\n",
    "\n",
    "    # Run pie chart analysis\n",
    "    pie_summary = create_pie_chart_analysis(result_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
